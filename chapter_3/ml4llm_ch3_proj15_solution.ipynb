{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[15] Analogy vectors</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec\n","\n","import pandas as pd\n","import seaborn as sns\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","from transformers import RobertaTokenizer, RobertaForMaskedLM"],"metadata":{"id":"R5SI-Iyy4dtm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300,\n","})"],"metadata":{"id":"9sMNotjcbD2c"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"slh8cyWJxASL"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# **Part 1: Explore the RoBERTa model**"],"metadata":{"id":"H6ApwsN67t6q"}},{"cell_type":"code","source":["tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n","model = RobertaForMaskedLM.from_pretrained('roberta-large')\n","model.eval()"],"metadata":{"id":"gfcYtjsB04Wh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# extract the embeddings matrix\n","embeddings = model.roberta.embeddings.word_embeddings.weight.detach().numpy()\n","embeddings.shape"],"metadata":{"id":"s6nEci8b0zoy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["words = [ 'list', 'computer', 'apple', 'spaceship' ]\n","\n","for w in words:\n","  print(f'\"{w}\" is indices {tokenizer.encode(w)}')\n","  print(f'\" {w}\" is indices {tokenizer.encode(\" \"+w)}\\n')"],"metadata":{"id":"SrPLKA0G4oy9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(20):\n","  print(f'Index {i:2} is \"{tokenizer.decode(i)}\"')"],"metadata":{"id":"kTqE3c_e6Orq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.encode([' king'],add_special_tokens=False))\n","print(tokenizer.encode(' king',add_special_tokens=False))"],"metadata":{"id":"MhJZgWkV9fvE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"o-7lIjsa5OCm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: Extract four embeddings and make a dataframe**"],"metadata":{"id":"4Hu2ctv35N_b"}},{"cell_type":"code","source":["# tokenize\n","words = [ ' king',' man',' woman',' queen' ]\n","tokens = [tokenizer.encode(w,add_special_tokens=False) for w in words]\n","\n","# print the token indices and corresponding tokens (words)\n","for w,tok in zip(words,tokens):\n","  print(f'\"{w}\" is encoded using token indices {tok}')"],"metadata":{"id":"QfzvoUgfza0a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# although we actually need a list of ints, not a list of lists of ints\n","tokens = [t[0] for t in tokens]\n","tokens"],"metadata":{"id":"ho1siU30D-Hp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# submatrix with embeddings\n","E = embeddings[tokens]\n","df = pd.DataFrame(E.T,columns=words)\n","\n","# summary of dataframe\n","df.describe()"],"metadata":{"id":"dctc6_abBuUL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FPuwHgqlnuwn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: Visualize using pairplots**"],"metadata":{"id":"FA4AmIARCw3_"}},{"cell_type":"code","source":["# visualize\n","sns.pairplot(df,kind='reg',\n","             plot_kws={'line_kws':{'color':'r'},\n","                       'scatter_kws':{'color':[.7,.7,.9],'s':10,'alpha':.5}},\n","             diag_kws={'color':[.9,.7,.7]}\n","            )\n","\n","plt.tight_layout()\n","plt.savefig('ch3_proj15_part3.png')\n","plt.show()"],"metadata":{"id":"tJ-ijiNE-V8p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"521U_yv_u3ZP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 4: Visualize cosine similarities**"],"metadata":{"id":"O2qStAStu3Tw"}},{"cell_type":"code","source":["# cosine similarities\n","csMat = cosine_similarity(E)\n","\n","# show the matrix\n","plt.imshow(csMat,vmin=csMat.min(),vmax=1,cmap='Reds')\n","plt.gca().set(xticks=range(4),yticks=range(4),\n","              xticklabels=words,yticklabels=words,\n","              title='All pairwise cosine similarities')\n","\n","# add text labels\n","for i in range(4):\n","  for j in range(4):\n","    plt.text(j,i,f'{csMat[i,j]:.2f}',\n","             ha='center',va='center',fontsize=18)\n","\n","plt.colorbar(pad=.02)\n","\n","plt.tight_layout()\n","plt.savefig('ch3_proj15_part4.png')\n","plt.show()"],"metadata":{"id":"nrjws_8IBbwU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"am_fw3dL1Z8p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 5: Arithmetic with embeddings vectors**"],"metadata":{"id":"1ncHf5JtC-s4"}},{"cell_type":"code","source":["# king - man + woman\n","analogyVector = df[' king'] - df[' man'] + df[' woman']\n","sim2all = cosine_similarity(analogyVector.values.reshape(1,-1),embeddings)\n","sim2all = np.squeeze(sim2all)\n","\n","fig = plt.figure(figsize=(12,3.5))\n","gs = gridspec.GridSpec(1,3,figure=fig)\n","ax1 = fig.add_subplot(gs[:-1])\n","ax2 = fig.add_subplot(gs[-1])\n","\n","ax1.scatter(range(len(sim2all)),sim2all,\n","            c=np.sqrt(abs(sim2all)),alpha=.7,s=(sim2all**2)*60,cmap='cool')\n","ax1.set(xlabel='Token index',ylabel='Cosine similarity',\n","        title='A) Cosine similarity with analogy vector')\n","\n","ax2.hist(sim2all,bins='fd',color=[.7,.7,.9],edgecolor='gray')\n","ax2.set(xlabel='Cosine similarity',ylabel='Count',\n","        xlim=[sim2all.min(),sim2all.max()],yscale='log',\n","        title='B) Distribution of similarities')\n","\n","plt.tight_layout()\n","plt.savefig('ch3_proj15_part5.png')\n","plt.show()"],"metadata":{"id":"racji2r_EVAK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print out the top 10 highest scores\n","top10 = sim2all.argsort()[-10:][::-1]\n","\n","print(' CosSim  |   R^2   |    word')\n","print('---------+---------+-------------')\n","for widx in top10:\n","  # correlation (square it to get shared variance)\n","  r = np.corrcoef(analogyVector,embeddings[widx])[0,1]\n","  print(f'  {sim2all[widx]:.3f}  |  {100*r**2:4.1f}%  |  \"{tokenizer.decode(widx)}\"')"],"metadata":{"id":"jl23eLSfGDME"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wwJYd67vC-qT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 6: An analogy-completing function**"],"metadata":{"id":"aVEHeCsQzfuX"}},{"cell_type":"code","source":["def analogyCalculator(word2start,word2subtract,word2add):\n","\n","  # 1) print the analogy\n","  print(f'\"{word2start}\" is to \"{word2subtract}\" as \"_____\" is to \"{word2add}\"\\n')\n","\n","  # 2) tokenize the words\n","  tokens = [tokenizer.encode(w,add_special_tokens=False) for w in [word2start,word2subtract,word2add]]\n","\n","  # 3) check that each word is one token\n","  if sum([len(l) for l in tokens]) != 3:\n","    raise ValueError(\"Warning: too many tokens.\")\n","\n","  # transform into single list\n","  tokens = [t[0] for t in tokens]\n","\n","  if '<unk>' in tokenizer.decode(tokens):\n","    raise ValueError(\"Unknown token: \",tokenizer.decode(tokens))\n","\n","  # 4) get the vectors\n","  v1 = embeddings[tokens[0]] # base word\n","  v2 = embeddings[tokens[1]] # to subtract\n","  v3 = embeddings[tokens[2]] # to add\n","\n","  # 5) analogy vector\n","  analogyVector = v1 - v2 + v3\n","\n","  # 6) cossim with all\n","  cossim2all = cosine_similarity(analogyVector.reshape(1,-1),embeddings)[0]\n","\n","  # 7) print out the top 10 highest scores\n","  top10 = cossim2all.argsort()[-10:][::-1]\n","  print('  CosSim  |   R^2   |    word')\n","  print('----------+---------+-------------')\n","  for widx in top10:\n","    # correlation (square it to get shared variance)\n","    r = np.corrcoef(analogyVector,embeddings[widx])[0,1]\n","    print(f'  {cossim2all[widx]:6.3f}  |  {100*r**2:4.1f}%  |  \"{tokenizer.decode(widx)}\"')\n"],"metadata":{"id":"SS7gjIDNzaxm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# try it\n","analogyCalculator(' king',' man',' woman')"],"metadata":{"id":"aceN0PkDzauj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# analogyCalculator(' tree',' leaf',' petals')\n","# analogyCalculator('tree','leaf','petals')\n","# analogyCalculator('leaf','tree','flower') # turn it around for better results?\n","# analogyCalculator(' husky',' dog',' bird')\n","# analogyCalculator('finger','hand','foot')\n","# analogyCalculator(' shoe',' foot',' hand')\n","analogyCalculator(' hand',' glove',' shoe')\n","# analogyCalculator('tomorrow','future','past')\n","# analogyCalculator('pants','legs','arms')"],"metadata":{"id":"z_diqnZjzar_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-Iirizdixzw7"},"execution_count":null,"outputs":[]}]}