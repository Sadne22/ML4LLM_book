{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[10] Sequential number cosine similarity</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","source":[],"metadata":{"id":"R5SI-Iyy4dtm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn.functional as F"],"metadata":{"id":"rn8Fcyf87EXx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300,\n","})"],"metadata":{"id":"dy4A-ah8kzZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FPFFnb6WNKjM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 1: Token counts of numbers**"],"metadata":{"id":"5hgVtrqEi6gj"}},{"cell_type":"code","source":["# GPT2 tokenizer and model\n","from transformers import AutoTokenizer,AutoModel\n","model = AutoModel.from_pretrained('gpt2')\n","tokenizer = AutoTokenizer.from_pretrained('gpt2')\n","\n","# the embeddings matrix\n","embeddings = model.wte.weight.detach()"],"metadata":{"id":"X-fyT7fnRjRq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the numbers to tokenize\n","numbers = np.arange(-1000,1001)\n","\n","# initializations\n","num_tokens = np.zeros(len(numbers),dtype=int)\n","token_idx = []\n","\n","# loop over tokens\n","for i in range(len(numbers)):\n","\n","  # tokenize\n","  t = tokenizer.encode(str(numbers[i]))\n","\n","  # store indices and length\n","  token_idx.append(t)\n","  num_tokens[i] = len(t)\n","\n","\n","plt.figure(figsize=(12,3))\n","plt.plot(numbers,num_tokens+np.random.normal(0,.03,len(numbers)),'ko',markerfacecolor='w',markersize=4,alpha=.5)\n","plt.gca().set(xlabel='Number',ylabel='Token count',yticks=np.unique(num_tokens),\n","              xlim=[numbers[0]-30,numbers[-1]+30])\n","\n","plt.tight_layout()\n","plt.savefig('ch3_proj10_part1a.png')\n","plt.show()"],"metadata":{"id":"6kI-U-cHjCeO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(12,3))\n","colord = 'bgr'\n","shape = 'os^'\n","\n","# loop over tokens\n","for i in range(len(numbers)):\n","\n","  # extract the token indices for this number\n","  t = token_idx[i]\n","\n","  # plot them\n","  for ti in range(len(t)):\n","    plt.plot(numbers[i],t[ti],'k',marker=shape[ti],markerfacecolor=colord[ti],\n","             markersize=6,alpha=.4,linewidth=.4)\n","\n","\n","# hacky legend solution (plot with alpha=0)\n","for i in range(3):\n","  plt.plot(0,0,'w',marker=shape[i],markerfacecolor=colord[i],\n","             markersize=10,alpha=0,linewidth=.5,label=['First','Second','Third'][i])\n","h = plt.legend()\n","for e in h.legend_handles: e.set_alpha(1)\n","\n","plt.gca().set(xlabel='Number',ylabel='Token index',ylim=[-1000,51000],xlim=[numbers[0]-30,numbers[-1]+30])\n","\n","plt.tight_layout()\n","plt.savefig('ch3_proj10_part1b.png')\n","plt.show()"],"metadata":{"id":"Wg7jxUlxb9gr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"G8MiaPzRkxg5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: Opposite sign, equal token indices**"],"metadata":{"id":"SRt7i2EtiE5w"}},{"cell_type":"code","source":["# initialize\n","matches_count = np.zeros(1000)\n","matches_vals = np.zeros(1000)\n","\n","for i in range(1000):\n","\n","  # confirm the digit pairing and token count\n","  n = f'({numbers[i]:>5},{numbers[-i-1]:>4})'\n","  t = f'({num_tokens[i]-1},{num_tokens[-i-1]})'\n","\n","  # see if they match\n","  matches_count[i] = (num_tokens[i]-1) == num_tokens[-i-1]\n","  matches_vals[i] = token_idx[i][1:] == token_idx[-i-1]\n","\n","  # for confirmation during code development\n","  # print(n,t)\n","\n","matches_count.sum(), matches_vals.sum()"],"metadata":{"id":"1vfjZEsskZEm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"C6dKIJtxi6db"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: Successive digit cosine similarity**"],"metadata":{"id":"KFig4ZZYRjXi"}},{"cell_type":"code","source":["# confirm that digits are single-token\n","for i in range(11):\n","  print(f'The token(s) for \"{i}\" are: {tokenizer.encode(str(i))}')"],"metadata":{"id":"6Z0_phfcQqt_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tokenize the digits\n","tokens = [tokenizer.encode(str(i))[0] for i in range(11)]\n","\n","# initialize cosine similarity\n","cossim = np.full(len(tokens),np.nan)\n","\n","# calculate cosine similarity for successive digit pairs\n","for ti in range(1,len(tokens)):\n","  v1 = embeddings[tokens[ti],:]\n","  v2 = embeddings[tokens[ti-1],:]\n","  cossim[ti] = torch.sum(v1*v2) / torch.sqrt( torch.sum(v1**2)*torch.sum(v2**2) )\n","\n","\n","# plot!\n","plt.figure(figsize=(12,4))\n","plt.bar(np.arange(len(cossim)),cossim,facecolor=[.7,.7,.9],edgecolor='k')\n","plt.gca().set(xticks=range(len(tokens)),xticklabels=[tokenizer.decode(t) for t in tokens],\n","              xlabel='Digit',ylabel='Cosine similarity',\n","              xlim=[-.5,len(tokens)-.5],ylim=[np.nanmin(cossim)-.05,np.nanmax(cossim)+.05])\n","\n","plt.title('Cosine similarities of sequential token embeddings',fontweight='bold')\n","\n","plt.tight_layout()\n","plt.savefig('ch3_proj10_part3.png')\n","plt.show()"],"metadata":{"id":"OAkHWmG_R1nz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oUBElc5-eW5L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 4: All to all number similarities**"],"metadata":{"id":"Zn0EEDq-eW2B"}},{"cell_type":"code","source":["# tokenize the numbers\n","nums = [str(i) for i in range(10)] + [str(i*10) for i in range(1,11)]\n","tokens = tokenizer.encode(nums)\n","\n","# confirm single tokens\n","for t in tokens:\n","  print(f'{t:5} is \"{tokenizer.decode(t)}\"')"],"metadata":{"id":"YGcOgd9zbwYJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# cosine similarity matrix\n","E = embeddings[tokens,:]\n","E_norm = F.normalize(E,p=2,dim=1)\n","csM = E_norm @ E_norm.T\n","\n","# visualize\n","plt.imshow(csM,vmin=.2,vmax=.9,cmap='magma')\n","plt.gca().set(xticks=range(0,len(nums),2),xticklabels=nums[::2],\n","              yticks=range(1,len(nums),2),yticklabels=nums[1::2],\n","              title='Cosine similarity in number pairs')\n","plt.colorbar(pad=.02)\n","\n","plt.tight_layout()\n","plt.savefig('ch3_proj10_part4.png')\n","plt.show()"],"metadata":{"id":"1QU_Jlb6dMXI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"32nfDKHVSSwH"},"execution_count":null,"outputs":[]}]}