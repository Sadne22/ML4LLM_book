{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[15] Analogy vectors</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec\n","\n","import pandas as pd\n","import seaborn as sns\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","from transformers import RobertaTokenizer, RobertaForMaskedLM"],"metadata":{"id":"R5SI-Iyy4dtm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300,\n","})"],"metadata":{"id":"9sMNotjcbD2c"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"slh8cyWJxASL"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# **Part 1: Explore the RoBERTa model**"],"metadata":{"id":"H6ApwsN67t6q"}},{"cell_type":"code","source":["tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n","model = RobertaForMaskedLM.from_pretrained('roberta-large')\n","model.eval()"],"metadata":{"id":"gfcYtjsB04Wh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# extract the embeddings matrix\n","embeddings = model.rob\n","embeddings.shape"],"metadata":{"id":"s6nEci8b0zoy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["words = [ 'list', 'computer', 'apple', 'spaceship' ]\n","\n","for w in words:\n","  print(f'\"{w}\" is indices {}')\n","  print(f'\" {w}\" is indices {}\\n')"],"metadata":{"id":"SrPLKA0G4oy9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(20):\n","  print"],"metadata":{"id":"kTqE3c_e6Orq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.encode\n","print(tokenizer.encode"],"metadata":{"id":"MhJZgWkV9fvE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"o-7lIjsa5OCm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: Extract four embeddings and make a dataframe**"],"metadata":{"id":"4Hu2ctv35N_b"}},{"cell_type":"code","source":["# tokenize\n","words = [ ' king',' man',' woman',' queen' ]\n","tokens =\n","\n","# print the token indices and corresponding tokens (words)\n","for w,tok in zip(words,tokens):\n","  print(f' is encoded using token indices ')"],"metadata":{"id":"QfzvoUgfza0a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# although we actually need a list of ints, not a list of lists of ints\n","tokens =\n","tokens"],"metadata":{"id":"ho1siU30D-Hp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# submatrix with embeddings\n","E =\n","df = pd.DataFrame(\n","\n","# summary of dataframe\n","df.describe()"],"metadata":{"id":"dctc6_abBuUL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FPuwHgqlnuwn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: Visualize using pairplots**"],"metadata":{"id":"FA4AmIARCw3_"}},{"cell_type":"code","source":["# visualize\n","sns.pairplot(df)\n","\n","plt.tight_layout()\n","plt.savefig('ch3_proj15_part3.png')\n","plt.show()"],"metadata":{"id":"tJ-ijiNE-V8p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"521U_yv_u3ZP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 4: Visualize cosine similarities**"],"metadata":{"id":"O2qStAStu3Tw"}},{"cell_type":"code","source":["# cosine similarities\n","csMat =\n","\n","# show the matrix\n","plt.imshow(csMat)\n","plt.gca().set(xticks=range(4),yticks=range(4),\n","              xticklabels=words,yticklabels=words,\n","              title='All pairwise cosine similarities')\n","\n","# add text labels\n","\n","plt.colorbar(pad=.02)\n","\n","plt.tight_layout()\n","plt.savefig('ch3_proj15_part4.png')\n","plt.show()"],"metadata":{"id":"nrjws_8IBbwU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"am_fw3dL1Z8p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 5: Arithmetic with embeddings vectors**"],"metadata":{"id":"1ncHf5JtC-s4"}},{"cell_type":"code","source":["# king - man + woman\n","analogyVector = df[' king'] - df[' man'] + df[' woman']\n","sim2all = cosine_similarity()\n","\n","fig = plt.figure(figsize=(12,3.5))\n","gs = gridspec.GridSpec(1,3,figure=fig)\n","ax1 = fig.add_subplot(gs[:-1])\n","ax2 = fig.add_subplot(gs[-1])\n","\n","ax1.scatter(sim2all)\n","ax1.set(xlabel='Token index',ylabel='Cosine similarity',\n","        title='A) Cosine similarity with analogy vector')\n","\n","ax2.hist()\n","ax2.set(xlabel='Cosine similarity',ylabel='Count',\n","        title='B) Distribution of similarities')\n","\n","plt.tight_layout()\n","plt.savefig('ch3_proj15_part5.png')\n","plt.show()"],"metadata":{"id":"racji2r_EVAK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print out the top 10 highest scores\n","top10 = sim2all.argsort()[-10:][::-1]\n","\n","print(' CosSim  |   R^2   |    word')\n","print('---------+---------+-------------')\n","for widx in top10:\n","  # correlation (square it to get shared variance)\n","  r = np.corrcoef(analogyVector,embeddings[widx])[0,1]\n","  print(f'')"],"metadata":{"id":"jl23eLSfGDME"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wwJYd67vC-qT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 6: An analogy-completing function**"],"metadata":{"id":"aVEHeCsQzfuX"}},{"cell_type":"code","source":["def analogyCalculator(word2start,word2subtract,word2add):\n","\n","  # 1) print the analogy\n","  print(f'\"{}\" is to \"{}\" as \"_____\" is to \"{}\"\\n')\n","\n","  # 2) tokenize the words\n","  tokens =\n","\n","  # 3) check that each word is one token\n","  if :\n","    raise ValueError(\"Warning: too many tokens.\")\n","\n","  # transform into single list\n","  tokens =\n","\n","  # check for unknown tokens (<unk>)\n","  if :\n","    raise ValueError(\"Unknown token: \",tokenizer.decode(tokens))\n","\n","  # 4) get the vectors\n","  v1 = embeddings # base word\n","  v2 = embeddings # to subtract\n","  v3 = embeddings # to add\n","\n","  # 5) analogy vector\n","  analogyVector =\n","\n","  # 6) cossim with all\n","  cossim2all =\n","\n","  # 7) print out the top 10 highest scores\n","  top10 = cossim2all\n","  print('  CosSim  |   R^2   |    word')\n","  print('----------+---------+-------------')\n","  for widx in top10:\n","    # correlation (square it to get shared variance)\n","    r = np.corrcoef(\n","    print(f'  {}  |  {}%  |  \"{}\"')\n"],"metadata":{"id":"SS7gjIDNzaxm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# try it\n","analogyCalculator(' king',' man',' woman')"],"metadata":{"id":"aceN0PkDzauj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# analogyCalculator(' tree',' leaf',' petals')\n","# analogyCalculator('tree','leaf','petals')\n","# analogyCalculator('leaf','tree','flower') # turn it around for better results?\n","# analogyCalculator(' husky',' dog',' bird')\n","# analogyCalculator('finger','hand','foot')\n","# analogyCalculator(' shoe',' foot',' hand')\n","# analogyCalculator(' hand',' glove',' shoe')\n","# analogyCalculator('tomorrow','future','past')\n","# analogyCalculator('pants','legs','arms')"],"metadata":{"id":"z_diqnZjzar_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-Iirizdixzw7"},"execution_count":null,"outputs":[]}]}