{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[12] RSA to compare GPT2 vs. BERT embeddings</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","import torch"],"metadata":{"id":"R5SI-Iyy4dtm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300,\n","})"],"metadata":{"id":"9sMNotjcbD2c"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oB3yKCAhaWhX"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# **Part 1: Embeddings from two LLMs**"],"metadata":{"id":"l3qQa_eTFiad"}},{"cell_type":"code","source":["# BERT tokenizer and model\n","from transformers import BertTokenizer, BertModel\n","\n","# load BERT tokenizer and model\n","tokenizerB = BertTokenizer.from_pretrained('bert-large-uncased')\n","modelB = BertModel.from_pretrained('bert-large-uncased')\n","\n","# the embeddings matrix\n","embeddingsB = modelB.embeddings.word_embeddings.weight.detach()"],"metadata":{"id":"IlLTVTpTBS75"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# GPT2 tokenizer and model\n","from transformers import AutoTokenizer,GPT2Model\n","tokenizerG = AutoTokenizer.from_pretrained('gpt2-medium')\n","modelG = GPT2Model.from_pretrained('gpt2-medium')\n","\n","# the embeddings matrix\n","embeddingsG = modelG.wte.weight.detach()"],"metadata":{"id":"gtAYkaX90FdV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"990BMSv4zxzd"},"outputs":[],"source":["print(f'BERT vocabulary size: {tokenizerB.vocab_size}')\n","print(f'GPT2 vocabulary size: {tokenizerG.vocab_size}')\n","print('')\n","\n","print(f'BERT embeddings shape: {list(embeddingsB.shape)}')\n","print(f'GPT2 embeddings shape: {list(embeddingsG.shape)}')"]},{"cell_type":"code","source":[],"metadata":{"id":"tq6T6x4z0p7U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: Comparing model embeddings**"],"metadata":{"id":"juJvf55P0p1W"}},{"cell_type":"code","source":["# list of words for RSA\n","words = [ 'space','spaceship','planet','moon'  ,'star' ,'galaxy',\n","          'chair','table'    ,'couch' ,'stool' ,'floor','desk',\n","          'apple','banana'   ,'pear'  ,'orange','peach','grape'\n","        ]\n","\n","# will be convenient later\n","num_words = len(words)\n","\n","# initialize lists of tokens\n","tokensB = []\n","tokensG = []\n","\n","# table header\n","print('     Word  | B-n | G-n | B-s | G-s')\n","print('-----------+-----+-----+-----+-----')\n","\n","for w in words:\n","\n","  # tokenize without preceding spaces\n","  tb_nospace = tokenizerB.encode(w,add_special_tokens=False)\n","  tg_nospace = tokenizerG.encode(w)\n","\n","  # tokenize with preceding spaces\n","  tb_withspace = tokenizerB.encode(f' {w}',add_special_tokens=False)\n","  tg_withspace = tokenizerG.encode(f' {w}')\n","\n","  # print a row of the table\n","  print(f'{w:>10} |  {len(tb_nospace)}  |  {len(tg_nospace)}  |  {len(tb_withspace)}  |  {len(tg_withspace)}')\n","\n","  # add to list\n","  tokensB.append(tb_nospace[0])\n","  tokensG.append(tg_withspace[0])"],"metadata":{"id":"2--Z6OhG1JXu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokensB,tokensG"],"metadata":{"id":"_pqZ_sqf42fi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# embeddings matrix for these words\n","sub_embedB = embeddingsB[tokensB,:]\n","sub_embedG = embeddingsG[tokensG,:]\n","\n","print(f'BERT embeddings shape: {list(sub_embedB.shape)}')\n","print(f'GPT2 embeddings shape: {list(sub_embedG.shape)}')"],"metadata":{"id":"BhGMzcjR13uk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(10,4))\n","\n","# scatter plot of an example word\n","axs[0].plot(sub_embedB[0,:],sub_embedG[0,:],'kh',\n","            markerfacecolor=[.7,.7,.9,.4])\n","axs[0].set(xlabel='BERT embeddings',ylabel='GPT-2 embeddings',title=f'A) Embeddings for \"{words[0]}\"')\n","\n","# correlations for all words\n","for i in range(num_words):\n","  r = np.corrcoef(sub_embedB[i,:],sub_embedG[i,:])[0,1]\n","  axs[1].plot(i,r,'kh',markerfacecolor=[.9,.7,.7],markersize=10)\n","\n","axs[1].axhline(0,linestyle='--',color='k',linewidth=.5,zorder=-1)\n","axs[1].set(xticks=range(num_words),xticklabels=words,ylim=[-.5,.5],\n","           title='B) Correlation for each word',ylabel='Correlation coefficient')\n","axs[1].tick_params(axis='x',labelrotation=90)\n","\n","plt.tight_layout()\n","plt.savefig('ch3_proj12_part2.png')\n","plt.show()"],"metadata":{"id":"XNQ8X9sA6GUX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KBiHPmR56GRr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: Representational similarity analysis**"],"metadata":{"id":"pr0Lgy-JFdry"}},{"cell_type":"code","source":["# cosine similarities\n","csG = torch.zeros((num_words,num_words))\n","csB = torch.zeros((num_words,num_words))\n","\n","for i in range(num_words):\n","  csB[i,:] = torch.cosine_similarity(sub_embedB[i],sub_embedB)\n","  csG[i,:] = torch.cosine_similarity(sub_embedG[i],sub_embedG)\n","\n","\n","# extract the upper-triangular elements\n","unique_B = csB[np.triu_indices_from(csB, k=1)]\n","unique_G = csG[np.triu_indices_from(csG, k=1)]\n","\n","# Pearson correlation\n","r = np.corrcoef(unique_B,unique_G)[0,1]\n","\n","print(f'Size of similarity matrices: {csB.shape}')\n","print(f'Number of non-redundant elements: {len(unique_B)}')"],"metadata":{"id":"mEJI5FdX1JUc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# and visualize\n","fig,axs = plt.subplots(1,3,figsize=(13,4))\n","\n","# color limits for images\n","vminmax = [.1,.5]\n","\n","# BERT: cosine similarity matrix\n","h = axs[0].imshow(csB,vmin=vminmax[0],vmax=vminmax[1],cmap='plasma')\n","axs[0].set(xticks=range(0,len(words),2),xticklabels=words[::2],\n","           yticks=range(1,len(words),2),yticklabels=words[1::2],\n","           title='A) Cossim matrix for BERT')\n","axs[0].tick_params(axis='x',labelrotation=90)\n","fig.colorbar(h,ax=axs[0],fraction=.046,pad=.02)\n","\n","\n","# GPT2: cosine similarity matrix\n","h = axs[1].imshow(csG,vmin=vminmax[0],vmax=vminmax[1],cmap='plasma')\n","axs[1].set(xticks=range(0,len(words),2),xticklabels=words[::2],\n","           yticks=range(1,len(words),2),yticklabels=words[1::2],\n","           title='B) Cossim matrix for GPT-2')\n","axs[1].tick_params(axis='x',labelrotation=90)\n","fig.colorbar(h,ax=axs[1],fraction=.046,pad=.02)\n","\n","\n","# scatter plot\n","axs[2].plot(unique_B,unique_G,'kh',markersize=8,markerfacecolor=[.7,.9,.7,.5])\n","axs[2].set(xlabel='BERT cosine similarities',ylabel='GPT-2 cosine similarities',\n","              title=f'C) Correlation (RSA score): r = {r:.2f}')\n","axs[2].grid(linestyle='--',color=[.9,.9,.9])\n","\n","\n","plt.tight_layout()\n","plt.savefig('ch3_proj12_part3.png')\n","plt.show()"],"metadata":{"id":"mlDv0c0MChf5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oN7EjxY41Icg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 4: Different embeddings sizes**"],"metadata":{"id":"g8-CxqB3dl38"}},{"cell_type":"code","source":[],"metadata":{"id":"8z39VkDWdx9S"},"execution_count":null,"outputs":[]}]}