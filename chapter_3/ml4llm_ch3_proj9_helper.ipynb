{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[9] Sequential word cosine similarity</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","source":[],"metadata":{"id":"R5SI-Iyy4dtm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn.metrics.pairwise import cosine_similarity"],"metadata":{"id":"rn8Fcyf87EXx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300,\n","})"],"metadata":{"id":"dy4A-ah8kzZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FPFFnb6WNKjM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 1: Cosine similarity of sequential tokens**"],"metadata":{"id":"rytwUuiPsDCb"}},{"cell_type":"code","source":["# load BERT tokenizer and model\n","from transformers import BertTokenizer, BertModel\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertModel.from_pretrained('bert-base-uncased')\n","\n","# the embeddings matrix\n","embeddings = model.em"],"metadata":{"id":"IlLTVTpTBS75"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HNSL-fPzzvOD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentence = \"my phone is in the kitchen near the cold ice cream\"\n","\n","# tokenize the sentence\n","tokens = tokenizer.encode(\n","\n","# initialize cosine similarity\n","cossim =\n","\n","# calculate cosine similarity for successive word pairs\n","for ti in range(1,len(tokens)):\n","  v1 = embeddings\n","  v2 = embeddings\n","  cossim[ti] =\n","\n","\n","# plot!\n","plt.figure(figsize=(12,4))\n","plt.bar(,,facecolor=[.7,.7,.9],edgecolor='k')\n","\n","\n","plt.title('Cosine similarities of sequential token embeddings',fontweight='bold')\n","\n","plt.tight_layout()\n","plt.savefig('ch3_proj9_part1.png')\n","plt.show()"],"metadata":{"id":"4TPJlmC-4t18"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nUyLPFK85IbP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: Are embeddings vectors context-dependent?**"],"metadata":{"id":"h37sa2TIT288"}},{"cell_type":"code","source":["sentences = [\n","    'The conductor waved his hands as the train departed and people sat down',\n","    'The conductor waved his hands as the orchestra began and people sat down'\n","]\n","\n","\n","plt.figure(figsize=(12,4))\n","\n","for i,sent in enumerate(sentences):\n","\n","  # tokenize the sentence\n","  tokens = tokenizer.encode(sent)[1:-1]\n","\n","  # initialize cosine similarity\n","  cossim = np.full(len(tokens),np.nan)\n","\n","  # cosine similarity for successive word pairs\n","  for ti in range(1,len(tokens)):\n","    cossim[ti] =\n","\n","\n","  # plot!\n","  plt.bar()\n","\n","\n","\n","# finish the plot\n","plt.axhline(0,linestyle='--',color='k',linewidth=.5,zorder=-3)\n","plt.legend()\n","plt.title('Cosine similarities of sequential token embeddings',fontweight='bold')\n","\n","plt.tight_layout()\n","plt.savefig('ch3_proj9_part2.png')\n","plt.show()"],"metadata":{"id":"8wKka_GW2WRH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"32nfDKHVSSwH"},"execution_count":null,"outputs":[]}]}