{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[12] Word similarity via distance and cosine</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import torch"],"metadata":{"id":"R5SI-Iyy4dtm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300,\n","})"],"metadata":{"id":"9sMNotjcbD2c"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oB3yKCAhaWhX"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# **Part 1: Cosine similarity vs. Euclidean distance**"],"metadata":{"id":"V_LjfGSI4CBq"}},{"cell_type":"code","source":["# simulation parameters\n","M = 768\n","k = 1000\n","\n","# initializations\n","cs = np.zeros((k,3))\n","dist = np.zeros((k,3))\n","\n","# loop over simulation iterations\n","for i in range(k):\n","\n","  # create the data\n","  x = np.random.normal(0,1,M)\n","  y = np.random.normal(0,1,M)\n","\n","  # case 1: normal random\n","  dist[i,0] = np.sqrt( np.sum( (x-y)**2 ) )\n","  cs[i,0] = np.dot(x,y) / (np.linalg.norm(x)*np.linalg.norm(y))\n","\n","  # case 2: different variances\n","  x /= 10\n","  dist[i,1] = np.sqrt( np.sum( (x-y)**2 ) )\n","  cs[i,1] = np.dot(x,y) / (np.linalg.norm(x)*np.linalg.norm(y))\n","\n","  # case 3: normalized\n","  x = x/np.linalg.norm(x)\n","  y = y/np.linalg.norm(y)\n","  dist[i,2] = np.sqrt( np.sum( (x-y)**2 ) )\n","  cs[i,2] = np.dot(x,y) # need the norm-scaling?\n","\n","\n","# the first plot\n","plt.plot(dist[:,0],cs[:,0],'rh',markerfacecolor=[.9,.7,.7,.5],label='Case 1: randn')\n","plt.plot(dist[:,1],cs[:,1],'go',markerfacecolor=[.7,.9,.7,.5],label='Case 2: Unequal var.')\n","plt.plot(dist[:,2],cs[:,2],'bs',markerfacecolor=[.7,.7,.9,.5],label='Case 3: Normed')\n","\n","plt.gca().set(xlabel='Euclidean distance',ylabel='Cosine similarity')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.savefig('ch3_proj13_part1a.png')\n","plt.show()"],"metadata":{"id":"8tEs3Oaau9kY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the second plot\n","_,axs = plt.subplots(1,3,figsize=(10,3))\n","\n","axs[0].plot(dist[:,0],cs[:,0],'rh',markerfacecolor=[.9,.7,.7,.7])\n","axs[0].set(xlabel='Euclidean distance',ylabel='Cosine similarity',title='A) Case 1: randn')\n","\n","axs[1].plot(dist[:,1],cs[:,1],'go',markerfacecolor=[.7,.9,.7,.7])\n","axs[1].set(xlabel='Euclidean distance',ylabel='Cosine similarity',title='B) Case 2: Unequal var.')\n","\n","axs[2].plot(dist[:,2],cs[:,2],'bs',markerfacecolor=[.7,.7,.9,.7])\n","axs[2].set(xlabel='Euclidean distance',ylabel='Cosine similarity',title='C) Case 3: Normed')\n","\n","plt.tight_layout()\n","plt.savefig('ch3_proj13_part1b.png')\n","plt.show()"],"metadata":{"id":"Uw2kIcZz3jHZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9IMawnr2zwQ-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: Relationship between similarity and distance**"],"metadata":{"id":"UJugBzLrzwMh"}},{"cell_type":"code","source":["# apply equation 1.13 to Case 3 (both vectors normed)\n","eq13 = cs[:,2] - (1-dist[:,2]**2/2)\n","\n","plt.figure(figsize=(8,4))\n","plt.plot(eq13,'k.')\n","plt.gca().set(xlabel='Simulation number',ylabel='Error')\n","\n","# note the y-axis, then use the same y-axis as in Part 1\n","# plt.ylim([-.1,.1])\n","\n","plt.tight_layout()\n","plt.savefig('ch3_proj13_part2a.png')\n","plt.show()"],"metadata":{"id":"w5Jmj1t8u9gh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# errors (difference between RHS and LHS)\n","e = np.zeros(k)\n","\n","# loop over simulation iterations\n","for i in range(k):\n","\n","  # create the data\n","  x = np.random.normal(0,1,M)\n","  y = np.random.normal(0,1,M)\n","\n","  # case 1: normal random\n","  d = np.sqrt( np.sum( (x-y)**2 ) )\n","  s = np.dot(x,y)/(np.linalg.norm(x)*np.linalg.norm(y))\n","\n","  xtx = x.T@x\n","  yty = y.T@y\n","\n","  # Equation 1.14\n","  den = np.sqrt(xtx) * np.sqrt(yty)\n","  rhs = xtx/den + yty/den - d**2/den\n","  e[i] = 2*s - rhs\n","\n","  # Equation 1.15 (comment out to run 1.14)\n","  e[i] = 2*x.T@y - (xtx + yty - d**2)\n","\n","\n","# and plot\n","plt.figure(figsize=(8,4))\n","plt.plot(e,'k.')\n","plt.gca().set(xlabel='Simulation number',ylabel='Error')\n","\n","plt.tight_layout()\n","plt.savefig('ch3_proj13_part2b.png')\n","plt.show()"],"metadata":{"id":"0wFi3jqKGP5_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SsLJGB9gzL3K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: Word synonyms via distance and similarity**"],"metadata":{"id":"IYa6IRn8NZQK"}},{"cell_type":"code","source":["from transformers import BertTokenizer, BertModel\n","\n","# load BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n","model = BertModel.from_pretrained('bert-large-uncased')\n","\n","# extract embeddings\n","embeddings = model.embeddings.word_embeddings.weight.detach()\n","\n","# vocab size\n","n_vocab = embeddings.shape[0]"],"metadata":{"id":"IlLTVTpTBS75"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pick a \"seed\" vector\n","seedword = 'beauty'\n","seedtoken = tokenizer.encode(seedword,add_special_tokens=False)\n","\n","print(f'The token \"{seedword}\" comprises these token indices: {seedtoken}')"],"metadata":{"id":"38dhFl7YNc8G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["seedvect = embeddings[seedtoken,:]\n","\n","# Euclidean distance to all other vectors\n","eucDist = torch.sqrt( torch.sum( (embeddings-seedvect)**2 ,axis=1) )\n","\n","# cosine similarity to all other vectors\n","cossim = torch.cosine_similarity(seedvect,embeddings)\n","\n","# remove trivial values\n","eucDist[torch.argmin(eucDist)] = torch.nan\n","cossim[torch.argmax(cossim)] = torch.nan\n","\n","# min-max scaling for coloring the scatter plot\n","eucDist_minmax = (eucDist-np.nanmin(eucDist)) / (np.nanmax(eucDist)-np.nanmin(eucDist))\n","cossim_minmax = (cossim-np.nanmin(cossim)) / (np.nanmax(cossim)-np.nanmin(cossim))"],"metadata":{"id":"ytq8TdhqPZ_g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualizations\n","_,axs = plt.subplots(1,3,figsize=(15,4))\n","\n","# plot the Euclidean distances\n","axs[0].scatter(range(n_vocab),eucDist,s=20,c=cossim_minmax,cmap=plt.cm.plasma,alpha=.4)\n","axs[0].set(xlim=[-20,n_vocab+20],xlabel='Token index',ylabel='Euclidean distance',\n","           title=f'Distance to \"{seedword}\",\\ncolored by cosine similarity')\n","\n","# plot the cosine similarities\n","axs[1].scatter(range(n_vocab),cossim,s=20,c=eucDist_minmax,cmap=plt.cm.magma,alpha=.4)\n","axs[1].set(xlim=[-20,n_vocab+20],xlabel='Token index',ylabel='Cosine similarity',\n","           title=f'Cosine similarity with \"{seedword}\",\\ncolored by distance')\n","\n","# and their relationship\n","axs[2].plot(eucDist,cossim,'ko',markerfacecolor=[.7,.7,.9,.2])\n","axs[2].set(xlabel='Euclidean distance',ylabel='Cosine similarity',\n","           title='Relation between\\nS$_C$ and Euclidean distance')\n","\n","plt.tight_layout()\n","plt.savefig('ch3_proj13_part3.png')\n","plt.show()"],"metadata":{"id":"1ujtyROLdKjN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"poyFTXWDcGBn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 4: Top-k closest and most similar**"],"metadata":{"id":"h6KjeZ-j4a0Q"}},{"cell_type":"code","source":["# now for the top-k closest tokens\n","k = 10\n","topKidx = torch.argsort(eucDist)[:k]\n","\n","print(f'Minimum distance {k} words to \"{seedword}\":')\n","for i in topKidx:\n","  print(f'  Distance of {eucDist[i]:.3f} to \"{tokenizer.decode(i)}\"')"],"metadata":{"id":"f7YhIKAmNZNP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# now for the top-k most similar tokens\n","topKidx = torch.argsort(cossim,descending=True)[:k]\n","\n","print(f'Most similar {k} words to \"{seedword}\":')\n","for i in topKidx:\n","  print(f'  Similarity of {cossim[i]:.3f} to \"{tokenizer.decode(i)}\"')"],"metadata":{"id":"fSOsvT4Nar7g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# FYI\n","cossim.sort()"],"metadata":{"id":"FdiNgAOrYaYC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uHh0aBdeW857"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 5: Normalized distance**"],"metadata":{"id":"9pCIFv_Ct06S"}},{"cell_type":"code","source":["### run this cell then repeat Parts 3 and 4\n","\n","# normalize the embeddings matrix\n","E_norm = torch.nn.functional.normalize(embeddings,p=2,dim=1)\n","\n","# Euclidean distance to all other vectors\n","eucDist = torch.sqrt( torch.sum( (E_norm-E_norm[seedtoken,:])**2 ,axis=1) )\n","eucDist[eucDist==0] = torch.nan\n","\n","# min-max scaling for coloring the scatter plot\n","eucDist_minmax = (eucDist-np.nanmin(eucDist)) / (np.nanmax(eucDist)-np.nanmin(eucDist))"],"metadata":{"id":"-3kBjd3xfY7t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6vHSrKbst71u"},"execution_count":null,"outputs":[]}]}