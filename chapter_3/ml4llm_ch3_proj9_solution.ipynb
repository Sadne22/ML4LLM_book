{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[9] Sequential word cosine similarity</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","source":[],"metadata":{"id":"R5SI-Iyy4dtm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn.metrics.pairwise import cosine_similarity"],"metadata":{"id":"rn8Fcyf87EXx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300,\n","})"],"metadata":{"id":"dy4A-ah8kzZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FPFFnb6WNKjM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 1: Cosine similarity of sequential tokens**"],"metadata":{"id":"rytwUuiPsDCb"}},{"cell_type":"code","source":["# load BERT tokenizer and model\n","from transformers import BertTokenizer, BertModel\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertModel.from_pretrained('bert-base-uncased')\n","\n","# the embeddings matrix\n","embeddings = model.embeddings.word_embeddings.weight.detach()"],"metadata":{"id":"IlLTVTpTBS75"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HNSL-fPzzvOD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentence = \"my phone is in the kitchen near the cold ice cream\"\n","\n","# tokenize the sentence\n","tokens = tokenizer.encode(sentence)[1:-1] # ignore the cls/sep tokens\n","\n","# initialize cosine similarity\n","cossim = np.full(len(tokens),np.nan)\n","\n","# calculate cosine similarity for successive word pairs\n","for ti in range(1,len(tokens)):\n","  v1 = embeddings[tokens[ti],:]\n","  v2 = embeddings[tokens[ti-1],:]\n","  c = cosine_similarity(v1.reshape(1,-1),v2.reshape(1,-1))\n","  cossim[ti] = c.item()\n","\n","\n","# plot!\n","plt.figure(figsize=(12,4))\n","plt.bar(np.arange(len(cossim)),cossim,facecolor=[.7,.7,.9],edgecolor='k')\n","plt.gca().set(xticks=range(len(tokens)),xticklabels=[tokenizer.decode(t) for t in tokens],\n","              xlim=[-.5,len(tokens)-.5],ylabel='Cosine similarity')\n","plt.gca().tick_params(axis='x',rotation=-45)\n","\n","plt.title('Cosine similarities of sequential token embeddings',fontweight='bold')\n","\n","plt.tight_layout()\n","plt.savefig('ch3_proj9_part1.png')\n","plt.show()"],"metadata":{"id":"4TPJlmC-4t18"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nUyLPFK85IbP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: Are embeddings vectors context-dependent?**"],"metadata":{"id":"h37sa2TIT288"}},{"cell_type":"code","source":["sentences = [\n","    'The conductor waved his hands as the train departed and people sat down',\n","    'The conductor waved his hands as the orchestra began and people sat down'\n","]\n","\n","\n","plt.figure(figsize=(12,4))\n","\n","for i,sent in enumerate(sentences):\n","\n","  # tokenize the sentence\n","  tokens = tokenizer.encode(sent)[1:-1]\n","\n","  # initialize cosine similarity\n","  cossim = np.full(len(tokens),np.nan)\n","\n","  # cosine similarity for successive word pairs\n","  for ti in range(1,len(tokens)):\n","    v1 = embeddings[tokens[ti],:]\n","    v2 = embeddings[tokens[ti-1],:]\n","    c = cosine_similarity(v1.reshape(1,-1),v2.reshape(1,-1))\n","    cossim[ti] = c.item()\n","\n","\n","  # plot!\n","  plt.bar(np.arange(0,len(cossim))+i/4-.1,cossim,width=.5,edgecolor='k',\n","          label=tokenizer.decode(tokens[7:9]))\n","\n","\n","\n","# finish the plot\n","plt.axhline(0,linestyle='--',color='k',linewidth=.5,zorder=-3)\n","xticklabs = [tokenizer.decode(t) for t in tokens]\n","xticklabs[7] = f'train/\\norchestra'\n","xticklabs[8] = 'departed/\\nbegan'\n","plt.gca().set(xticks=range(len(tokens)),xticklabels=xticklabs,\n","              xlim=[-.5,len(tokens)-.5],ylabel='Cosine similarity')\n","plt.tick_params(axis='x',rotation=-45)\n","plt.legend()\n","plt.title('Cosine similarities of sequential token embeddings',fontweight='bold')\n","\n","plt.tight_layout()\n","plt.savefig('ch3_proj9_part2.png')\n","plt.show()"],"metadata":{"id":"8wKka_GW2WRH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"32nfDKHVSSwH"},"execution_count":null,"outputs":[]}]}