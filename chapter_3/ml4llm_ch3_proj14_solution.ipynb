{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[14] Linear semantic axes</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","source":["import numpy as np\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec"],"metadata":{"id":"R5SI-Iyy4dtm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300,\n","})"],"metadata":{"id":"9sMNotjcbD2c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VFb6fkqJSHfR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 1: Extract and normalize an embeddings matrix**"],"metadata":{"id":"rxASHe6mVwUq"}},{"cell_type":"code","source":["from transformers import RobertaTokenizer, RobertaForMaskedLM\n","\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","model = RobertaForMaskedLM.from_pretrained('roberta-base')\n","\n","# extract embeddings matrix and convert to numpy\n","embeddings = model.roberta.embeddings.word_embeddings.weight.detach().numpy()"],"metadata":{"id":"v75Pc3Wl5CYf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# optional normalization\n","vectorNorms = np.linalg.norm(embeddings,axis=1,keepdims=True)\n","embeddings_norm = embeddings / vectorNorms\n","embeddings_norm.shape"],"metadata":{"id":"9csAsC_kW9bJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(12,3))\n","plt.plot(np.squeeze(vectorNorms),'o',markeredgewidth=.3,markerfacecolor=[.9,.7,.7,.3])\n","plt.gca().set(xlabel='Token index',ylabel='Embedding norm',\n","              xlim=[-50,len(vectorNorms)+50])\n","\n","plt.tight_layout()\n","plt.savefig('ch3_proj14_part1.png')\n","plt.show()"],"metadata":{"id":"GBojcAFNU9Id"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# confirm unit vector\n","np.linalg.norm(embeddings_norm[300]), np.linalg.norm(embeddings[300])"],"metadata":{"id":"yUONRRCoX7nU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nuM14ZFwUUmM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: Create a \"semantic axis\"**"],"metadata":{"id":"H6ApwsN67t6q"}},{"cell_type":"code","source":["# pick two words to define the axis\n","word4pos = tokenizer.encode(' future',add_special_tokens=False)\n","word4neg = tokenizer.encode(' past',add_special_tokens=False)\n","\n","# confirm they're single-token words\n","print(word4pos, word4neg)\n","\n","# but it's best to \"de-dimensionalize\" them for subsequent plotting\n","word4pos = word4pos[0]\n","word4neg = word4neg[0]"],"metadata":{"id":"BGpB_3A820wV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get the vectors for those words\n","v2add = embeddings[word4pos]\n","v2sub = embeddings[word4neg]\n","\n","# create the \"semantic axis\" with \"raw\" vectors\n","semantic_axis = v2add - v2sub\n","semantic_axis /= np.linalg.norm(semantic_axis) # post-subtraction normalization\n","\n","# now starting from the normed vectors\n","v2add = embeddings_norm[word4pos]\n","v2sub = embeddings_norm[word4neg]\n","semantic_axis_from_norm = v2add - v2sub\n","\n","# print the norms\n","print(f'Norm of non-normed subtraction: {np.linalg.norm(semantic_axis):.3f}')\n","print(f'Norm of pre-normed subtraction: {np.linalg.norm(semantic_axis_from_norm):.3f}')"],"metadata":{"id":"duwZikvgmWod"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualize\n","fig = plt.figure(figsize=(12,4))\n","gs = gridspec.GridSpec(1,3,figure=fig)\n","ax1 = fig.add_subplot(gs[:-1])\n","ax2 = fig.add_subplot(gs[-1])\n","\n","\n","ax1.plot(semantic_axis,'rs',markerfacecolor=[.9,.7,.7,.5],label='Normed after subtraction')\n","ax1.plot(semantic_axis_from_norm,'bo',markerfacecolor=[.7,.7,.9,.5],label='Subtraction of normed vectors')\n","ax1.legend()\n","ax1.set(xlabel='Embeddings dimension',ylabel='Embedding weight',\n","        xlim=[-10,len(semantic_axis)+10],title='A) Vector differences')\n","\n","ax2.plot(semantic_axis,semantic_axis_from_norm,'ks',markerfacecolor=[.7,.9,.7,.5])\n","ax2.set(xlabel='Difference of \"raw\" vectors',ylabel='Difference of normed vectors',\n","        title='B) Comparison of difference vectors')\n","\n","plt.tight_layout()\n","plt.savefig('ch3_proj14_part2.png')\n","plt.show()"],"metadata":{"id":"0iJ3XNq5mq1S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Norms of the two vectors:')\n","print(f' {np.linalg.norm(embeddings[word4pos]):.3f}')\n","print(f' {np.linalg.norm(embeddings[word4neg]):.3f}')"],"metadata":{"id":"o8dzHtU1d-mi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sE80SJZFSHVN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: Project all tokens onto the axis**"],"metadata":{"id":"_q6LZCQz8NoM"}},{"cell_type":"code","source":["# calculate dot products\n","dotprods = semantic_axis @ embeddings_norm.T\n","\n","fig = plt.figure(figsize=(12,3.5))\n","gs = gridspec.GridSpec(1,3,figure=fig)\n","ax1 = fig.add_subplot(gs[:-1])\n","ax2 = fig.add_subplot(gs[-1])\n","\n","# plot all the similarities\n","ax1.plot(dotprods,'k.',alpha=.3)\n","ax1.set(xlabel='Token index',ylabel='Projection',xlim=[-10,len(dotprods)+10],\n","        title='A) Projections onto semantic axis')\n","\n","ax2.hist(dotprods,bins='fd',color=[.7,.7,.9],edgecolor='gray')\n","ax2.set(xlabel='Projection',ylabel='Count',\n","        xlim=[dotprods.min(),dotprods.max()],yscale='log',\n","        title='B) Distribution of projection values')\n","\n","plt.tight_layout()\n","plt.savefig('ch3_proj14_part3.png')\n","plt.show()"],"metadata":{"id":"mEUFF3S38NCu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find top and bottom 10 highest scores\n","top10 = dotprods.argsort()[-10:][::-1]\n","bot10 = dotprods.argsort()[:10]\n","\n","\n","# print them out\n","print('\\n10 most positive-projected words:')\n","print('  Proj.  |   Word')\n","print('---------+------------')\n","for widx in top10:\n","  print(f'  {dotprods[widx]:.3f}  |  \"{tokenizer.decode(widx)}\"')\n","\n","print('\\n\\n10 most negative-projected words:')\n","print('  Proj.  |   Word')\n","print('---------+------------')\n","for widx in bot10:\n","  print(f' {dotprods[widx]:.3f}  |  \"{tokenizer.decode(widx)}\"')"],"metadata":{"id":"8YTOkLT7cEss"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EV4O7t7tR8KJ"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# **Part 4: Try other semantic axes**"],"metadata":{"id":"-qk9ksxPfX7Z"}},{"cell_type":"code","source":["# good/evil\n","# young/old\n","# big/small"],"metadata":{"id":"jFA2v8FffX4t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6KXOjwBifX15"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 5: Try other embeddings**"],"metadata":{"id":"VK0PGt-qfVP9"}},{"cell_type":"code","source":["# # load BERT tokenizer and model\n","# from transformers import BertTokenizer, BertModel\n","# tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n","# model = BertModel.from_pretrained('bert-large-uncased')\n","# embeddings = model.embeddings.word_embeddings.weight.detach().numpy()"],"metadata":{"id":"YIh_kfsyW6Vk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # GPT2 tokenizer and model\n","# from transformers import GPT2Tokenizer,GPT2Model\n","# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","# model = GPT2Model.from_pretrained('gpt2-large')\n","# embeddings = model.wte.weight.detach().numpy()"],"metadata":{"id":"4gqhC4z4fPjp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7q5ggW-TfVKU"},"execution_count":null,"outputs":[]}]}