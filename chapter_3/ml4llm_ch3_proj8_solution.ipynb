{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNGAxOTsbMs8fcoBZlbSdpx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[8] All to all cosine similarity</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","source":[],"metadata":{"id":"R5SI-Iyy4dtm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec\n","import torch\n","\n","# for monitoring for-loop progress\n","from tqdm import tqdm"],"metadata":{"id":"rn8Fcyf87EXx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300,\n","})"],"metadata":{"id":"dy4A-ah8kzZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"h2wzYW_wUJ1u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Background: Embeddings vectors**"],"metadata":{"id":"6FU_ktRiNLnJ"}},{"cell_type":"code","source":["# load BERT tokenizer and model\n","from transformers import BertTokenizer, BertModel\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertModel.from_pretrained('bert-base-uncased')"],"metadata":{"id":"IlLTVTpTBS75"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the embeddings matrix\n","embeddings = model.embeddings.word_embeddings.weight.detach()\n","\n","# check shape of embedding matrix (vocab size Ã— embedding dim)\n","print(f'Embedding matrix shape: {embeddings.shape}')"],"metadata":{"id":"Upgfe2mABWNx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# two words (should be single-token)\n","word1 = 'hello'\n","word2 = 'world'\n","\n","# get two token indices\n","token1 = tokenizer.encode(word1,add_special_tokens=False)\n","token2 = tokenizer.encode(word2,add_special_tokens=False)\n","\n","# their embeddings vectors\n","emb1 = embeddings[token1,:].squeeze()\n","emb2 = embeddings[token2,:].squeeze()\n","\n","# and plot\n","fig = plt.figure(figsize=(12,3))\n","gs = gridspec.GridSpec(1,4,figure=fig)\n","ax1 = fig.add_subplot(gs[:3])\n","ax2 = fig.add_subplot(gs[-1])\n","\n","ax1.plot(emb1,'ks',markerfacecolor=[.9,.7,.7,.5],markersize=5,label=word1)\n","ax1.plot(emb2,'ko',markerfacecolor=[.7,.9,.7,.5],markersize=5,label=word2)\n","\n","ax1.set(xlabel='Embeddings dimension',ylabel='Value',xlim=[-5,len(emb1)+5],title='Embeddings of two words')\n","ax1.legend()\n","\n","ax2.plot(emb1,emb2,'ko',markerfacecolor=[.7,.7,.9,.5])\n","ax2.set(xlabel=f'Embeddings of \"{word1}\"',ylabel=f'Embeddings of \"{word2}\"')\n","\n","plt.tight_layout()\n","plt.savefig('ch3_embeddingsA.png')\n","plt.show()"],"metadata":{"id":"TQUO30bNo-Oa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the whole matrix\n","plt.figure(figsize=(12,4))\n","plt.imshow(embeddings.T,vmin=-.05,vmax=.05,aspect='auto',cmap='bwr')\n","\n","plt.gca().set(xlabel='Token index',ylabel='Embeddings dimension',\n","              title='Embeddings matrix')\n","\n","plt.colorbar(pad=.01)\n","plt.tight_layout()\n","plt.savefig('ch3_embeddingsB.png')\n","plt.show()"],"metadata":{"id":"ooVdTiSOo-Je"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HT9lg4cEyVsm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Background: Cosine similarity**"],"metadata":{"id":"aPEgJHRyUJyt"}},{"cell_type":"code","source":["# generate some correlated data\n","x = np.random.randn(50)\n","y = x + np.random.randn(50)\n","\n","# manual cosine similarity\n","num = np.sum(x*y)\n","norm_x = np.sum(x**2)\n","norm_y = np.sum(y**2)\n","den = np.sqrt( norm_x*norm_y )\n","cs = num/den\n","\n","plt.figure(figsize=(5,4))\n","plt.plot(x,y,'kh',markerfacecolor=[.7,.9,.7,.7],markersize=12)\n","plt.gca().set(xlabel='x',ylabel='y',\n","              title=f'Cosine similarity = {cs:.2f}')\n","\n","plt.tight_layout()\n","plt.savefig('ch3_cossimA.png')\n","plt.show()"],"metadata":{"id":"rqsRdyilJP04"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a data matrix (random numbers with linear offsets)\n","n = 100 # observations\n","m = 15  # features\n","\n","# create the data (try commenting out the + np.linspace...)\n","X = np.random.randn(n,m) + np.linspace(-2,2,m)\n","\n","# normalize (note the vector_norm not matrix_norm!)\n","# also be careful of which axis to normalize depending on matrix dimension\n","X_norm = X / np.linalg.norm(X,axis=0,keepdims=True)\n","\n","# cosine similarity matrix (note the transpose on the first matrix)\n","csM = X_norm.T @ X_norm\n","\n","# correlation matrix\n","R = np.corrcoef(X.T)"],"metadata":{"id":"pLVITNCXtg7z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = plt.figure(figsize=(12,6.5))\n","gs = gridspec.GridSpec(2,3,figure=fig)\n","ax1 = fig.add_subplot(gs[0,:])\n","ax2 = fig.add_subplot(gs[1,0])\n","ax3 = fig.add_subplot(gs[1,1])\n","ax4 = fig.add_subplot(gs[1,2])\n","\n","\n","ax1.imshow(X.T,aspect='auto',cmap='plasma')\n","ax1.set(xlabel='Observation',ylabel='Feature',title='A) Data matrix')\n","\n","# and the cosine similarity matrix\n","h = ax2.imshow(csM,vmin=-1,vmax=1,cmap='RdBu_r')\n","fig.colorbar(h,ax=ax2,pad=.02)\n","ax2.set(xlabel='Feature',ylabel='Feature',xticks=range(0,m,2),yticks=range(1,m,2),\n","           title='B) Cosine similarity matrix')\n","\n","# and the cosine similarity matrix\n","h = ax3.imshow(R,vmin=-1,vmax=1,cmap='RdBu_r')\n","fig.colorbar(h,ax=ax3,pad=.02)\n","ax3.set(xlabel='Feature',ylabel='Feature',xticks=range(0,m,2),yticks=range(1,m,2),\n","           title='C) Correlation matrix')\n","\n","\n","unique_cs = csM[np.triu_indices(csM.shape[0],k=1)]\n","unique_rs = R[np.triu_indices(R.shape[0],k=1)]\n","\n","ax4.axhline(0,linestyle='--',color='k',linewidth=.5)\n","ax4.axvline(0,linestyle='--',color='k',linewidth=.5)\n","ax4.plot(unique_cs,unique_rs,'kh',markerfacecolor=[.7,.7,.9,.7])\n","ax4.set(xlabel='Cosine similarity',ylabel='Correlation',xlim=[-1,1],ylim=[-1,1],\n","        title='D) $S_C-r$ relationship')\n","\n","plt.tight_layout()\n","plt.savefig('ch3_cossimB.png')\n","plt.show()"],"metadata":{"id":"FPFFnb6WNKjM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7uL_x2mwd5fc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Now for the project**"],"metadata":{"id":"4LFHsg-zyVpp"}},{"cell_type":"code","source":[],"metadata":{"id":"EjCyTbjxybNz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 1: Similarity of embeddings pairs**"],"metadata":{"id":"bzazogb7yayJ"}},{"cell_type":"code","source":["# use this code for two random tokens\n","tokenpair = np.random.choice(np.arange(3000,6001),2)"],"metadata":{"id":"BVWx6WdRWnoK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# or pick two words\n","word1 = 'sunshine'\n","word2 = 'pineapple'\n","\n","# tokenize\n","# tokenpair = tokenizer.encode([word1,word2],add_special_tokens=False)"],"metadata":{"id":"hTZOn7-S-w2A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check that these are single-token words\n","for t in tokenpair:\n","  print(f'{t:5} is \"{tokenizer.decode(t)}\"')"],"metadata":{"id":"RTX23LBWhvWQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get their embedding vectors\n","v1 = embeddings[tokenpair[0]]\n","v2 = embeddings[tokenpair[1]]\n","\n","v1_token = tokenizer.decode(tokenpair[0])\n","v2_token = tokenizer.decode(tokenpair[1])\n","\n","print(f'Token pair: \"{v1_token}\" and \"{v2_token}\"')\n","print(f'Embedding shape: {v1.shape}')"],"metadata":{"id":"PFtJBrRs_cJK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(7,6))\n","\n","plt.plot(v1,v2,'kh',markerfacecolor=[.7,.9,.7,.7])\n","plt.gca().set(xlabel=f'Embeddings for \"{v1_token}\"',ylabel=f'Embeddings for \"{v2_token}\"')\n","\n","plt.tight_layout()\n","plt.savefig('ch3_proj8_part1.png')\n","plt.show()"],"metadata":{"id":"mbPFquHB3sB2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calculate cosine similarity manually\n","num = torch.sum(v1*v2)\n","norm_v1 = torch.sqrt( torch.sum(v1**2) )\n","norm_v2 = torch.linalg.norm(v2) # equivalent to previous line\n","den = norm_v1*norm_v2\n","\n","print(f'Shape of vectors: {v1.shape}')\n","manual_cs = num/den"],"metadata":{"id":"f9bH1H16daK7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# and now in torch\n","v1_t = v1.unsqueeze(dim=0)\n","v2_t = v2.view(1,-1) # both view() and unsqueeze() work in this case\n","\n","print(f'Shape of torch vectors: {v1_t.shape}')\n","torch_cs = torch.cosine_similarity(v1_t,v2_t)"],"metadata":{"id":"jASTkdEveVF-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print the results\n","print(f'Manual cosine similarity:  {manual_cs:.5f}')\n","print(f'Pytorch cosine similarity: {torch_cs.item():.5f}')"],"metadata":{"id":"WT_mXSuJWHdR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9v4KCU4fsDFc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: All-to-all cosine similarity**"],"metadata":{"id":"cyjIpvKCdX0T"}},{"cell_type":"code","source":["# normalize (note the vector_norm not matrix_norm!)\n","# important: compare dim=1 here to dim=0 earlier (also note that numpy calls it \"axis\" instead of \"dim\")\n","E_norm = embeddings / torch.linalg.norm(embeddings,dim=1,keepdim=True)\n","\n","# cosine similarity matrix (note which matrix is transposed)\n","csM = E_norm @ E_norm.T\n","\n","# check size\n","print(f'Cosine similarity matrix shape: {csM.shape} ({np.prod(csM.shape):,} total elements!)')"],"metadata":{"id":"wAT_EqZaNJhk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check resources (System RAM) before and after running this code block\n","\n","# reduce precision to save on RAM for subsequent analyses\n","csM = csM.to(torch.float16)\n","\n","# and delete unused large variable\n","del E_norm"],"metadata":{"id":"DPbknZDbjTOv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# select a subset of the matrix\n","skip = 3\n","csMsub = csM[::skip,::skip]\n","\n","# check size\n","print(f'Full matrix shape: {csM.shape} ({np.prod(csM.shape):>11,} total elements!)')\n","print(f'Submatrix shape  : {csMsub.shape} ({np.prod(csMsub.shape):>11,} total elements!)')"],"metadata":{"id":"6_tPIDs3MX7t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get the non-redundant values of that matrix\n","cs_nonredun = csMsub[np.triu_indices(csMsub.shape[0],k=1)]\n","\n","# show the histogram\n","fig,axs = plt.subplots(1,2,figsize=(12,3.5))\n","\n","h = axs[0].imshow(csMsub,vmin=0,vmax=.8,cmap='plasma')\n","axs[0].set(title='A) Token cosine similarities',xticks=[],yticks=[],\n","           xlabel='Tokens',ylabel='Tokens')\n","plt.colorbar(h,ax=axs[0],pad=.01)\n","\n","axs[1].hist(cs_nonredun,bins=100,density=True,color=[.9,.7,.9],edgecolor='k')\n","axs[1].set(xlabel='Cosine similarity',ylabel='Density',\n","           xlim=[cs_nonredun.min(),cs_nonredun.max()],\n","           title='B) Distribution of similarities')\n","\n","plt.tight_layout()\n","plt.savefig('ch3_proj8_part2.png')\n","plt.show()"],"metadata":{"id":"0fghs7TPPhaC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"x6OFTLa-WjYh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: BERT's unused tokens**"],"metadata":{"id":"df1CEL6AX0Lw"}},{"cell_type":"code","source":["for i in range(20):\n","  print(f'Token {i:2} is \"{tokenizer.decode(i)}\"')"],"metadata":{"id":"Q2Tvllhlqox0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find the \"unused\" tokens\n","unused_tokens = torch.zeros(tokenizer.vocab_size,dtype=bool)\n","for i in range(tokenizer.vocab_size):\n","  if '[unused' in tokenizer.decode(i):\n","    unused_tokens[i] = True\n","\n","titleinfo = f'{unused_tokens.sum()}/{tokenizer.vocab_size} ({100*unused_tokens.sum()/tokenizer.vocab_size:.2f}%) \"unused\" tokens'\n","\n","# visualize\n","plt.figure(figsize=(10,3))\n","plt.plot(range(tokenizer.vocab_size),torch.randn(tokenizer.vocab_size)/40+unused_tokens,\n","         'ko',markersize=2,markerfacecolor='w',alpha=.3)\n","plt.gca().set(xlabel='Token index',ylabel='Value',yticks=[0,1],yticklabels=['Used','Unused'],ylim=[-.5,1.5],\n","              xlim=[-15,len(unused_tokens)+15],title=titleinfo)\n","\n","plt.tight_layout()\n","plt.savefig('ch3_proj8_part3.png')\n","plt.show()"],"metadata":{"id":"IAQoEBFgWsQo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# extract a submatrix of just the unused tokens\n","csMsub = csM[unused_tokens,:][:,unused_tokens]\n","\n","# get the unique values of that matrix\n","cs_nonredun = csMsub[np.triu_indices(csMsub.shape[0],k=1)]\n","cs_unique = np.unique(cs_nonredun)\n","\n","print(f'There are {cs_nonredun.shape[0]:,} non-redundant values, {len(cs_unique)} of which are unique.')\n","print('\\nThe unique values are:\\n',cs_unique)"],"metadata":{"id":"WmnOaaMzYPFe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FE0jOauLn7JJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 4: GPT2 all to all, done differently**"],"metadata":{"id":"pve7b4UmAW3R"}},{"cell_type":"code","source":["# GPT2 tokenizer and model\n","from transformers import GPT2Model\n","model = GPT2Model.from_pretrained('gpt2')\n","\n","# the embeddings matrix\n","embeddings = model.wte.weight.detach()\n","\n","# downsample and reduce the precision before calculations\n","skip = 5\n","embeddings = embeddings[::skip,:].to(torch.float16)\n","embeddings.shape"],"metadata":{"id":"G-2k0tHkAW0S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# normalize in a different way\n","E_norm = torch.nn.functional.normalize(embeddings,p=2,dim=1)\n","\n","# initialize block size and CS matrix\n","N = embeddings.shape[0]\n","block = 1024\n","csM = torch.empty(N,N,dtype=E_norm.dtype)\n","\n","# loop over blocks\n","for i in tqdm(range(0,N,block)):\n","\n","  # find end index\n","  end_i = min(i+block,N)\n","\n","  # calculate just this block of cossim and put into matrix\n","  cs_part = E_norm[i:end_i] @ E_norm.T\n","  csM[i:end_i] = cs_part"],"metadata":{"id":"FrXxZjyTHeSx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get the non-redundant values of that matrix\n","row,col = np.triu_indices(csM.shape[0], k=1)\n","cs_nonredun = csM[row,col]\n","\n","# show the histogram\n","fig,axs = plt.subplots(1,2,figsize=(12,3.5))\n","\n","h = axs[0].imshow(csM,vmin=.1,vmax=.4,cmap='plasma')\n","axs[0].set(title='A) Cosine similarity matrix (GPT-2)',xticks=[],yticks=[],\n","           xlabel='Tokens',ylabel='Tokens')\n","plt.colorbar(h,ax=axs[0],pad=.01)\n","\n","axs[1].hist(cs_nonredun,bins=100,density=True,color=[.9,.7,.9],edgecolor='k')\n","axs[1].set(xlabel='Cosine similarity',ylabel='Density (log)',\n","           xlim=[cs_nonredun.min(),cs_nonredun.max()],yscale='log',\n","           title='B) Distribution of similarities in GPT-2')\n","\n","plt.tight_layout()\n","plt.savefig('ch3_proj8_part4.png')\n","plt.show()"],"metadata":{"id":"3LMLojviAWur"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Gm1-pMxyMPMm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 5: Compare correlation and similarity**"],"metadata":{"id":"voNkly0lMPJ7"}},{"cell_type":"code","source":["# mean-center and variance-normalize\n","E_norm = embeddings - embeddings.mean(dim=1,keepdim=True)\n","E_norm = torch.nn.functional.normalize(E_norm,p=2,dim=1)\n","\n","R = torch.empty(N,N,dtype=E_norm.dtype)\n","\n","# loop over blocks\n","for i in tqdm(range(0,N,block)):\n","\n","  # find end index\n","  end_i = min(i+block,N)\n","\n","  # calculate just this block of correlation and put into matrix\n","  R_part = E_norm[i:end_i] @ E_norm.T\n","  R[i:end_i] = R_part\n","\n","\n","# extract non-redundant matrix elements\n","R_nonredun = R[row,col]"],"metadata":{"id":"0OyC1KmQIrpu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calculate histograms\n","yAr,xAr = np.histogram(embeddings.mean(dim=1).numpy(),bins=80) # convert to numpy (not actually necessary here)\n","yL1,xL1 = np.histogram(abs(embeddings).mean(dim=1),bins=80)\n","\n","\n","fig,axs = plt.subplots(1,2,figsize=(10,4))\n","\n","# show the scatter plot\n","skip = 50000\n","# line of unity\n","axs[0].axline(np.full(2,R_nonredun[::skip].min().item()),slope=1,\n","              color='k',linestyle='--',linewidth=.3)\n","# scatter\n","axs[0].plot(R_nonredun[::skip],cs_nonredun[::skip],'ko',markerfacecolor=[.7,.7,.9,.3])\n","axs[0].set(xlabel='Correlation coefficient',ylabel='Cosine similarity',\n","           title='A) Correlation by similarity')\n","\n","# and the histograms\n","axs[1].plot(xAr[:-1],yAr,linewidth=2,label='Arithmetic means')\n","axs[1].plot(xL1[:-1],yL1,linewidth=2,label='L1 means')\n","axs[1].legend()\n","axs[1].set(xlabel='Mean values',ylabel='Count',ylim=[0,None],\n","           title='B) Distributions of arithmetic vs. L1 means')\n","\n","plt.tight_layout()\n","plt.savefig('ch3_proj8_part5.png')\n","plt.show()"],"metadata":{"id":"-BF_ZFlfOLUN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qEfg47bzR5tH"},"execution_count":null,"outputs":[]}]}