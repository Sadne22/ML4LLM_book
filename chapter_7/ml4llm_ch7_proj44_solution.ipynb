{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"167ENw4jid1g4t7B2QLNu5FTxWBQ1g5Zg","timestamp":1765857447854}],"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[44] Grammar tuning in MLP neurons</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"8UrqMO28-9ZL"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import scipy.stats as stats\n","!pip install pingouin\n","import pingouin as pg # for effect size calculations\n","\n","import requests\n","\n","import torch\n","from transformers import AutoModelForCausalLM,AutoTokenizer"],"metadata":{"id":"aoocnKDi-2RN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300\n","})"],"metadata":{"id":"U9F0prqyUFcE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BJR9NR3dr4pZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 1: Get nouns and verbs**"],"metadata":{"id":"uu2yxPwQ0M-g"}},{"cell_type":"code","source":["# Eleuther's tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125m')\n","\n","# load in GPTneo\n","model = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-125m')\n","model.eval()"],"metadata":{"id":"u1V-auBrPSS1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# main repo: https://github.com/david47k/top-english-wordlists/\n","\n","# lists of verbs\n","url = 'https://raw.githubusercontent.com/david47k/top-english-wordlists/refs/heads/master/top_english_verbs_lower_10000.txt'\n","all_verbs = requests.get(url).text.split('\\n')\n","\n","# initialize as empty list\n","verbs = []\n","len_verbs = []\n","\n","# loop over all the verbs\n","for word in all_verbs:\n","\n","  # tokenize with preceding space\n","  tok = tokenizer.encode(f' {word}')\n","\n","  # add to the list if its single-token\n","  if len(tok)==1:\n","    verbs.append(tok[0])\n","    len_verbs.append(len(word))\n","\n","\n","# split by odd/even\n","verbs_split1 = verbs[:2000:2]\n","verbs_split2 = verbs[1:2000:2]\n","\n","# and print\n","print(f'{len(verbs)} out of {len(all_verbs)} verbs are single-token.')\n","print(f'There are {len(verbs_split1)} split-1 and {len(verbs_split2)} split-2 samples.')"],"metadata":{"id":"ng-VWPU_eXPt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Note: the .split('\\n') method adds an extra element at the end, which is why there seems to be 10,001 verbs:\n","all_verbs[-1]"],"metadata":{"id":"KXj8ln5EmJUl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# repeat for nouns\n","url = 'https://raw.githubusercontent.com/david47k/top-english-wordlists/refs/heads/master/top_english_nouns_lower_10000.txt'\n","all_nouns = requests.get(url).text.split('\\n')\n","\n","# initialize as empty list\n","nouns = []\n","len_nouns = []\n","\n","# loop over all the nouns\n","for word in all_nouns:\n","\n","  # tokenize with preceding space\n","  tok = tokenizer.encode(f' {word}')\n","\n","  # add to the list if its single-token\n","  if len(tok)==1:\n","    nouns.append(tok[0])\n","    len_nouns.append(len(word))\n","\n","# split by odd/even\n","nouns_split1 = nouns[:2000:2]\n","nouns_split2 = nouns[1:2000:2]\n","\n","# and print\n","print(f'{len(nouns)} out of {len(all_nouns)} nouns are single-token.')\n","print(f'There are {len(nouns_split1)} split-1 and {len(nouns_split2)} split-2 samples.')"],"metadata":{"id":"D9LEmR85sH3W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('First 5 split-1 verbs:')\n","print([tokenizer.decode(v) for v in verbs_split1[:5]])\n","\n","print('\\nFirst 5 split-2 verbs:')\n","print([tokenizer.decode(v) for v in verbs_split2[:5]])\n","\n","\n","print('\\n\\nFirst 5 split-1 nouns:')\n","print([tokenizer.decode(n) for n in nouns_split1[:5]])\n","\n","print('\\nFirst 5 split-2 nouns:')\n","print([tokenizer.decode(n) for n in nouns_split2[:5]])"],"metadata":{"id":"indQK6aMJ2IS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check lengths\n","\n","# convenient to have in numpy\n","len_nouns = np.array(len_nouns[:2000])\n","len_verbs = np.array(len_verbs[:2000])\n","\n","yN = np.bincount(len_nouns)\n","yV = np.bincount(len_verbs)\n","\n","plt.figure(figsize=(10,3))\n","plt.bar(np.arange(len_nouns.max()+1)-.15,yN,width=.6,label='Nouns',alpha=.9,edgecolor='b')\n","plt.bar(np.arange(len_verbs.max()+1)+.15,yV,width=.6,label='Verbs',alpha=.9,edgecolor='r')\n","\n","tres = stats.ttest_ind(len_nouns,len_verbs)\n","cohensd = pg.compute_effsize(len_nouns,len_verbs,paired=False,eftype='cohen')\n","\n","plt.gca().set(xticks=range(np.max(len_nouns)),xlabel='Number of characters',ylabel='Count',\n","              title=f\"t({tres.df:g}) = {tres.statistic:.2f}, p = {tres.pvalue:.3f}\\nCohen's d = {abs(cohensd):.3f}\")\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj44_part1.png')\n","plt.show()"],"metadata":{"id":"-XUBLoadV1iw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8V9gC5JaHEim"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: Implant a hook and get activations**"],"metadata":{"id":"gEaT5J0hn0mw"}},{"cell_type":"code","source":["model"],"metadata":{"id":"lbUitFlPnuAb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# a hook function to grab the activations\n","mlp_acts = {}\n","\n","def hook(module,input,output):\n","  mlp_acts[f'{whichdata}'] = output.detach().numpy().squeeze()\n","\n","handle = model.transformer.h[8].mlp.c_fc.register_forward_hook(hook)"],"metadata":{"id":"-5ZWugpPnt85"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# make a batch\n","torch.tensor(nouns_split1).unsqueeze(1).shape"],"metadata":{"id":"x7CmqvCRHEaw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# this cell takes around 20 s\n","\n","# reinialize data-dictionary\n","mlp_acts = {}\n","\n","with torch.no_grad():\n","\n","  # run the split1 nouns\n","  whichdata = 'nouns_split1'\n","  model(torch.tensor(nouns_split1).unsqueeze(1))\n","\n","  # split2 nouns\n","  whichdata = 'nouns_split2'\n","  model(torch.tensor(nouns_split2).unsqueeze(1))\n","\n","  # the split1 verbs\n","  whichdata = 'verbs_split1'\n","  model(torch.tensor(verbs_split1).unsqueeze(1))\n","\n","  # and the split2 verbs\n","  whichdata = 'verbs_split2'\n","  model(torch.tensor(verbs_split2).unsqueeze(1))\n"],"metadata":{"id":"rj7kAE12HEdc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for k,v in mlp_acts.items():\n","  print(f\"mlp_acts['{k}'] has shape {list(v.shape)}\")"],"metadata":{"id":"7EAS4gIHOIU7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,3,figsize=(12,3.5))\n","\n","axs[0].imshow(mlp_acts['nouns_split1'],aspect='auto',vmin=-2,vmax=2,cmap='plasma')\n","axs[0].set(xlabel='Neurons',ylabel='Nouns (index)',title='A) Nouns activations')\n","\n","axs[1].plot(mlp_acts['nouns_split1'].mean(axis=0),'ko',markersize=5,markerfacecolor=[.9,.7,.9,.5])\n","axs[1].set(xlabel='Neurons',ylabel='Activation',title='B) Mean activations over all nouns')\n","\n","axs[2].plot(mlp_acts['nouns_split1'].mean(axis=0),mlp_acts['verbs_split1'].mean(axis=0),\n","            'ko',markersize=5,markerfacecolor=[.9,.7,.9,.5])\n","axs[2].set(xlabel='Nouns',ylabel='Verbs',title='C) Activations to all words')\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj44_part2.png')\n","plt.show()"],"metadata":{"id":"8xMmNCfIHEVw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fCtFiFHnJ_Fy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: T-tests on split-1 data**"],"metadata":{"id":"5sFuiDm7J_Ag"}},{"cell_type":"code","source":["nneurons = mlp_acts['nouns_split1'].shape[-1]\n","nneurons"],"metadata":{"id":"TMCWZRjw1MgY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# t-test on all neurons\n","T_split1 = stats.ttest_ind(mlp_acts['nouns_split1'],\n","                           mlp_acts['verbs_split1'],axis=0)\n","\n","# Cohen's d\n","cohensd = np.zeros(nneurons)\n","for i in range(nneurons):\n","  cohensd[i] = pg.compute_effsize(mlp_acts['nouns_split1'][:,i],mlp_acts['verbs_split1'][:,i],\n","                                  paired=False,eftype='cohen')\n","\n","# plot\n","plt.plot(cohensd,T_split1.statistic,'ko',markerfacecolor='w')\n","plt.gca().set(xlabel=\"Cohen's d\",ylabel='T-value')\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj44_part3a.png')\n","plt.show()"],"metadata":{"id":"fqXGfxMesLPI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,4))\n","\n","# pvalues FDR corrected\n","sigPvals1 = stats.false_discovery_control(T_split1.pvalue)<.05\n","\n","# plot the significant neurons\n","plt.plot(np.where(sigPvals1)[0],T_split1.statistic[sigPvals1],'go',markerfacecolor='w')\n","\n","# significant and large effect size (Cohen's d>.8)\n","plt.plot(np.where(abs(cohensd)>.8)[0],T_split1.statistic[abs(cohensd)>.8],'go')\n","\n","# non-significant\n","plt.plot(np.where(sigPvals1==False)[0],T_split1.statistic[sigPvals1==False],'rx')\n","\n","# adjustments\n","plt.gca().set(xlabel='Neuron index',ylabel='T-value',xlim=[-10,nneurons+10],\n","              title=f'{np.sum(sigPvals1)}/{len(sigPvals1)} were significant, {np.sum(abs(cohensd)>.8)}/{len(cohensd)} were large effects.')\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj44_part3b.png')\n","plt.show()"],"metadata":{"id":"27unvTBmWJUT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Hnktug2fJ-5R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 4: T-tests on split-2 data**"],"metadata":{"id":"i7uXAQheR01D"}},{"cell_type":"code","source":["# in split 2\n","T_split2 = stats.ttest_ind(mlp_acts['nouns_split2'],\n","                           mlp_acts['verbs_split2'],axis=0)\n","\n","# across the two splits\n","T_split12 = stats.ttest_ind(mlp_acts['nouns_split1'],\n","                            mlp_acts['verbs_split2'],axis=0)"],"metadata":{"id":"9iSknAAFX0mS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Bonferroni correction\n","sigPthresh = .05 / len(T_split2.pvalue)\n","\n","# find where one or both are significant\n","bothSig_2  = (T_split1.pvalue<sigPthresh).astype(int) + (T_split2.pvalue<sigPthresh).astype(int)\n","bothSig_12 = (T_split1.pvalue<sigPthresh).astype(int) + (T_split12.pvalue<sigPthresh).astype(int)\n","\n","# correlations between t-values\n","r_2  = np.corrcoef(T_split1.statistic,T_split2.statistic)[0,1]\n","r_12 = np.corrcoef(T_split1.statistic,T_split12.statistic)[0,1]\n","\n","\n","# visualizations\n","_,axs = plt.subplots(1,2,figsize=(8,3.5))\n","\n","# split-1 vs. split-2\n","axs[0].plot(T_split1.statistic[bothSig_2==2],T_split2.statistic[bothSig_2==2],'ks',markerfacecolor=[.7,.9,.7,.5],markersize=5,label='Both sig.')\n","axs[0].plot(T_split1.statistic[bothSig_2==0],T_split2.statistic[bothSig_2==0],'rx',markersize=3,alpha=.5,label='Neither sig')\n","axs[0].plot(T_split1.statistic[bothSig_2==1],T_split2.statistic[bothSig_2==1],'ko',markerfacecolor=[.9,.7,.7,.5],markersize=4,label='One sig.')\n","\n","# split-1 vs. split-12\n","axs[1].plot(T_split1.statistic[bothSig_12==2],T_split12.statistic[bothSig_12==2],'ks',markerfacecolor=[.7,.9,.7,.5],markersize=5,label='Both sig.')\n","axs[1].plot(T_split1.statistic[bothSig_12==0],T_split12.statistic[bothSig_12==0],'rx',markersize=3,alpha=.5,label='Neither sig')\n","axs[1].plot(T_split1.statistic[bothSig_12==1],T_split12.statistic[bothSig_12==1],'ko',markerfacecolor=[.9,.7,.7,.5],markersize=4,label='One sig.')\n","\n","# axis adjustments\n","axs[0].set(xlabel='Split-1 t-value',ylabel='Split-2 t-value',title=f'A) T-val comparison (r = {r_2:.3f})')\n","axs[1].set(xlabel='Split-1 t-value',ylabel='Split-12 t-value',title=f'B) T-val comparison (r = {r_12:.3f})')\n","\n","# common adjustments\n","for a in axs:\n","  a.axhline(0,color='k',linestyle='--',linewidth=.5)\n","  a.axvline(0,color='k',linestyle='--',linewidth=.5)\n","  a.legend()\n","\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj44_part4.png')\n","plt.show()"],"metadata":{"id":"F39i49JfbTaP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"IU0gJZa1iu-J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 5: Investigating distributions**"],"metadata":{"id":"TMtgfZcBiu7O"}},{"cell_type":"code","source":["# extract histograms\n","yNouns1,xNouns1 = np.histogram(mlp_acts['nouns_split1'],bins='fd')\n","yNouns2,xNouns2 = np.histogram(mlp_acts['nouns_split2'],bins='fd')\n","yVerbs1,xVerbs1 = np.histogram(mlp_acts['verbs_split1'],bins='fd')\n","yVerbs2,xVerbs2 = np.histogram(mlp_acts['verbs_split2'],bins='fd')\n","\n","# and visualize them\n","plt.figure(figsize=(9,3))\n","plt.plot(xNouns1[:-1],yNouns1,linewidth=2,label='Nouns 1')\n","plt.plot(xNouns2[:-1],yNouns2,linewidth=2,label='Nouns 2')\n","plt.plot(xVerbs1[:-1],yVerbs1,linewidth=2,label='Verbs 1')\n","plt.plot(xVerbs2[:-1],yVerbs2,linewidth=2,label='Verbs 2')\n","\n","plt.legend()\n","plt.gca().set(xlabel='MLP expansion activation',ylabel='Count',ylim=[0,None],\n","              title='Histograms of all MLP neurons')\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj44_part5a.png')\n","plt.show()"],"metadata":{"id":"RvU9BTGjerZf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# histograms of t>0 and t<0 subpopulations\n","yNouns1_neg,xNouns1_neg = np.histogram(mlp_acts['nouns_split1'][:,T_split1.statistic<0],bins='fd',density=True)\n","yVerbs1_neg,xVerbs1_neg = np.histogram(mlp_acts['verbs_split1'][:,T_split1.statistic<0],bins='fd',density=True)\n","yNouns1_pos,xNouns1_pos = np.histogram(mlp_acts['nouns_split1'][:,T_split1.statistic>0],bins='fd',density=True)\n","yVerbs1_pos,xVerbs1_pos = np.histogram(mlp_acts['verbs_split1'][:,T_split1.statistic>0],bins='fd',density=True)\n","\n","plt.figure(figsize=(9,3))\n","plt.plot(xNouns1_neg[:-1],yNouns1_neg,linewidth=2,label='Nouns t<0')\n","plt.plot(xVerbs1_neg[:-1],yVerbs1_neg,linewidth=2,label='Verbs t<0')\n","plt.plot(xNouns1_pos[:-1],yNouns1_pos,linewidth=2,label='Nouns t>0')\n","plt.plot(xVerbs1_pos[:-1],yVerbs1_pos,linewidth=2,label='Verbs t>0')\n","\n","plt.legend()\n","plt.gca().set(xlabel='MLP expansion activation',ylabel='Density',ylim=[0,None],\n","              title='Histograms separated by t-value sign')\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj44_part5b.png')\n","plt.show()"],"metadata":{"id":"sneWBw1WbbjV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find the neurons with the largest positive and negative t-values\n","max_t = np.argmax(T_split1.statistic)\n","min_t = np.argmin(T_split1.statistic)\n","\n","# and get their histograms\n","yNouns1_max,xNouns1_max = np.histogram(mlp_acts['nouns_split1'][:,max_t],bins='fd',density=True)\n","yVerbs1_max,xVerbs1_max = np.histogram(mlp_acts['verbs_split1'][:,max_t],bins='fd',density=True)\n","yNouns1_min,xNouns1_min = np.histogram(mlp_acts['nouns_split1'][:,min_t],bins='fd',density=True)\n","yVerbs1_min,xVerbs1_min = np.histogram(mlp_acts['verbs_split1'][:,min_t],bins='fd',density=True)\n","\n","plt.figure(figsize=(9,3))\n","plt.plot(xNouns1_max[:-1],yNouns1_max,'r',linewidth=2,label=f'Nouns (t = {T_split1.statistic[max_t]:.2f})')\n","plt.plot(xVerbs1_max[:-1],yVerbs1_max,'g',linewidth=2,label=f'Verbs (t = {T_split1.statistic[max_t]:.2f})')\n","plt.plot(xNouns1_min[:-1],yNouns1_min,'r--',linewidth=2,label=f'Nouns (t = {T_split1.statistic[min_t]:.2f})')\n","plt.plot(xVerbs1_min[:-1],yVerbs1_min,'g--',linewidth=2,label=f'Verbs (t = {T_split1.statistic[min_t]:.2f})')\n","\n","plt.gca().set(xlabel='MLP expansion activation',ylabel='Density',ylim=[0,None],\n","              title='Histograms from two neurons')\n","\n","plt.legend()\n","plt.tight_layout()\n","plt.savefig('ch7_proj44_part5c.png')\n","plt.show()"],"metadata":{"id":"PR53ejqAhEW_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FDUw-mCs3dMK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 6: Within-category tests**"],"metadata":{"id":"xnAf0BxC3dF_"}},{"cell_type":"code","source":["# within-category t-tests\n","T_withinNoun = stats.ttest_ind(mlp_acts['nouns_split1'],\n","                               mlp_acts['nouns_split2'],axis=0)\n","T_withinVerb = stats.ttest_ind(mlp_acts['verbs_split1'],\n","                               mlp_acts['verbs_split2'],axis=0)\n","\n","# and plot\n","fig,axs = plt.subplots(1,2,figsize=(9,4))\n","axs[0].plot(T_split1.statistic,T_withinNoun.statistic,'ko',markerfacecolor=[.9,.7,.7,.3])\n","axs[1].plot(T_split2.statistic,T_withinVerb.statistic,'ks',markerfacecolor=[.7,.9,.7,.3])\n","\n","axlim = np.max([abs(T_split1.statistic).max(),abs(T_split2.statistic).max()])*1.1\n","axs[0].set(xlim=[-axlim,axlim],ylim=[-axlim,axlim],xlabel='t(nouns,verbs), split 1',ylabel='t(nouns-1,nouns-2)',\n","           title='A) Across vs. within-nouns comparison')\n","axs[1].set(xlim=[-axlim,axlim],ylim=[-axlim,axlim],xlabel='t(nouns,verbs), split 2',ylabel='t(verbs-1,verbs-2)',\n","           title='B) Across vs. within-verbs comparison')\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj44_part6a.png')\n","plt.show()"],"metadata":{"id":"ZGCJHgIJVNSP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cohen's d\n","cohensd_within = np.zeros(nneurons)\n","for i in range(nneurons):\n","  cohensd_within[i] = pg.compute_effsize(mlp_acts['nouns_split1'][:,i],mlp_acts['nouns_split2'][:,i],paired=False,eftype='cohen')\n","\n","# histograms\n","yW,xW = np.histogram(abs(cohensd_within),bins='fd')\n","yA,xA = np.histogram(abs(cohensd),bins='fd')\n","\n","# visualize\n","plt.figure(figsize=(9,3))\n","plt.plot(xW[:-1],yW,'o-',linewidth=2,label='Within category')\n","plt.plot(xA[:-1],yA,'s-',linewidth=2,label='Across category')\n","\n","# indicating effect sizes\n","plt.axvline(.2,linestyle='--',color='r',label='Small effect')\n","plt.axvline(.8,linestyle=':',color='m',label='Large effect')\n","\n","plt.legend()\n","plt.gca().set(xlabel=\"Cohen's d\",ylabel='Count')\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj44_part6b.png')\n","plt.show()"],"metadata":{"id":"1gWJUqav2P-Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"j1cmJ5ghR0vn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 7: Laminar profile of tuning**"],"metadata":{"id":"Q-O2wjEMR0sy"}},{"cell_type":"code","source":["n_layers = len(model.transformer.h)"],"metadata":{"id":"6Bgn3ua6fL25"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# remove previous hook\n","handle.remove()\n","\n","def outerHook(layeri):\n","  def hook(module,input,output):\n","    mlp_acts[f'L{layeri}_{whichdata}'] = output.detach().numpy().squeeze()\n","  return hook\n","\n","\n","# surgery ;)\n","handles = []\n","for layeri in range(n_layers):\n","  h = model.transformer.h[layeri].mlp.c_fc.register_forward_hook(outerHook(layeri))\n","  handles.append(h)"],"metadata":{"id":"9D0hw0lhS-67"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# this cell takes around 20 s\n","\n","# reinialize data-dictionary\n","mlp_acts = {}\n","\n","with torch.no_grad():\n","\n","  # run the split1 nouns\n","  whichdata = 'nouns_split1'\n","  model(torch.tensor(nouns_split1).unsqueeze(1))\n","\n","  # split2 nouns\n","  whichdata = 'nouns_split2'\n","  model(torch.tensor(nouns_split2).unsqueeze(1))\n","\n","  # the split1 verbs\n","  whichdata = 'verbs_split1'\n","  model(torch.tensor(verbs_split1).unsqueeze(1))\n","\n","  # and the split2 verbs\n","  whichdata = 'verbs_split2'\n","  model(torch.tensor(verbs_split2).unsqueeze(1))\n"],"metadata":{"id":"n-nerRTER0qU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for k,v in mlp_acts.items():\n","  print(f\"mlp_acts['{k}'] has shape {list(v.shape)}\")"],"metadata":{"id":"0Z31wfJEf5vW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sig_neurons = np.zeros((n_layers,5))\n","\n","for layeri in range(n_layers):\n","\n","  # run the t-tests\n","  T_split1 = stats.ttest_ind(mlp_acts[f'L{layeri}_nouns_split1'],\n","                             mlp_acts[f'L{layeri}_verbs_split1'],axis=0)\n","  T_split2 = stats.ttest_ind(mlp_acts[f'L{layeri}_nouns_split2'],\n","                             mlp_acts[f'L{layeri}_verbs_split2'],axis=0)\n","\n","  # boolean of significant tests\n","  issig1 = stats.false_discovery_control(T_split1.pvalue)<.05\n","  issig2 = stats.false_discovery_control(T_split2.pvalue)<.05\n","\n","  # proportion of significant neurons\n","  sig_neurons[layeri,0] = np.mean(issig1)\n","  sig_neurons[layeri,1] = np.mean(issig2)\n","\n","  # average significant t-values\n","  sig_neurons[layeri,2] = np.mean(abs(T_split1.statistic[issig1]))\n","  sig_neurons[layeri,3] = np.mean(abs(T_split2.statistic[issig2]))\n","\n","  # correlation between them\n","  sig_neurons[layeri,4] = np.corrcoef(T_split1.statistic,T_split2.statistic)[0,1]\n"],"metadata":{"id":"h_z2WPvHgC3N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,3,figsize=(12,3))\n","\n","# proportion significant neurons\n","axs[0].plot(np.arange(n_layers)-.15,sig_neurons[:,0],'ko',markerfacecolor=[.7,.9,.7],markersize=10,label='Split-1')\n","axs[0].plot(np.arange(n_layers)+.15,sig_neurons[:,1],'ks',markerfacecolor=[.9,.7,.7],markersize=10,label='Split-2')\n","axs[0].set(xlabel='Transformer layer',ylabel='Proportion significant neurons',title='A) Proportion significant neurons')\n","axs[0].legend()\n","\n","# average t-values\n","axs[1].plot(np.arange(n_layers)-.15,sig_neurons[:,2],'ko',markerfacecolor=[.7,.9,.7],markersize=10,label='Split-1')\n","axs[1].plot(np.arange(n_layers)+.15,sig_neurons[:,3],'ks',markerfacecolor=[.9,.7,.7],markersize=10,label='Split-2')\n","axs[1].set(xlabel='Transformer layer',ylabel='Average t-values',title='B) |T| of significant neurons')\n","axs[1].legend()\n","\n","# correlation\n","axs[2].plot(sig_neurons[:,4],'kh',markersize=12,markerfacecolor=[.7,.7,.9])\n","axs[2].set(xlabel='Transformer layer',ylabel='Correlation coefficient',title='C) T-value split correlations')\n","\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj44_part7.png')\n","plt.show()"],"metadata":{"id":"HGplKVY2hnAP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"b2BbLDVrJ-2d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 8: Tokens in vs. out of order**"],"metadata":{"id":"WNggIWhPJ-zo"}},{"cell_type":"code","source":["# source: https://en.wikipedia.org/wiki/Coconut\n","text = 'The coconut (Cocos nucifera) is a member of the palm family (Arecaceae) and the only living species of the genus Cocos.'\n","\n","tokens = tokenizer.encode(text,return_tensors='pt')\n","print(f'There are {len(text)} characters and {len(tokens[0])} tokens.')"],"metadata":{"id":"O_f1XHEHR5Cw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# scramble and invert\n","scrambled_idx = torch.randperm(len(tokens[0]))\n","scrambled_tokens = tokens[0,scrambled_idx].unsqueeze(0)\n","inverse_idx = torch.argsort(scrambled_idx)\n","\n","print(f'Original sentence:\\n {tokenizer.decode(tokens[0,:])}\\n')\n","print(f'Scrambled sentence:\\n {tokenizer.decode(scrambled_tokens[0,:])}\\n')\n","print(f'Inverted scrambling:\\n {tokenizer.decode(scrambled_tokens[0,inverse_idx])}')\n"],"metadata":{"id":"67rl1zrY4CMw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mlp_acts = {}\n","\n","with torch.no_grad():\n","\n","  whichdata = 'sentence'\n","  model(tokens)\n","\n","  whichdata = 'words'\n","  model(tokens.T)\n","\n","  whichdata = 'scrambled'\n","  model(scrambled_tokens)"],"metadata":{"id":"exCcucb8J-w8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for k,v in mlp_acts.items():\n","  print(f\"mlp_acts['{k}'] has shape {list(v.shape)}\")"],"metadata":{"id":"8CyU_ERPJ-uP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,3,figsize=(12,3.5))\n","\n","skip = 10\n","layer = 3\n","\n","sent = mlp_acts[f'L{layer}_sentence'].flatten()[::skip]\n","word = mlp_acts[f'L{layer}_words'].flatten()[::skip]\n","scrm = mlp_acts[f'L{layer}_scrambled'][inverse_idx,:].flatten()[::skip]\n","\n","axs[0].plot(sent,word,'ko',markersize=3,markerfacecolor=[.7,.7,.9,.3])\n","axs[0].set(xlabel='Sentence',ylabel='Words',title=f'A) Sentence vs. words (r = {np.corrcoef(sent,word)[0,1]:.3f})')\n","\n","axs[1].plot(sent,scrm,'ks',markersize=3,markerfacecolor=[.7,.9,.7,.3])\n","axs[1].set(xlabel='Sentence',ylabel='Scrambled sentence',title=f'B) Sentence vs. scrambled (r = {np.corrcoef(sent,scrm)[0,1]:.3f})')\n","\n","axs[2].plot(scrm,word,'k^',markersize=3,markerfacecolor=[.9,.7,.7,.3])\n","axs[2].set(xlabel='Scrambled sentence',ylabel='Words',title=f'C) Scrambled vs. words (r = {np.corrcoef(scrm,word)[0,1]:.3f})')\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj44_part8a.png')\n","plt.show()"],"metadata":{"id":"NyzLEiDWTmWv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Rs = np.zeros((n_layers,3))\n","\n","# loop over layers\n","for layeri in range(n_layers):\n","\n","  # extract the activations\n","  sent = mlp_acts[f'L{layeri}_sentence'].flatten()\n","  word = mlp_acts[f'L{layeri}_words'].flatten()\n","  scrm = mlp_acts[f'L{layeri}_scrambled'][inverse_idx,:].flatten()\n","\n","  # correlation coefficients\n","  Rs[layeri,0] = np.corrcoef(sent,word)[0,1]\n","  Rs[layeri,1] = np.corrcoef(sent,scrm)[0,1]\n","  Rs[layeri,2] = np.corrcoef(scrm,word)[0,1]\n","\n","# and the visualizations\n","plt.figure(figsize=(10,3))\n","plt.plot(np.arange(n_layers)-.1,Rs[:,0],'s-',linewidth=2,markersize=8,markerfacecolor='w',label='Sentence-word')\n","plt.plot(np.arange(n_layers)   ,Rs[:,1],'o-',linewidth=2,markersize=8,markerfacecolor='w',label='Sentence-scrambled')\n","plt.plot(np.arange(n_layers)+.1,Rs[:,2],'^-',linewidth=2,markersize=8,markerfacecolor='w',label='Scrambled-word')\n","\n","plt.axhline(0,linestyle='--',color='k',linewidth=.4,zorder=-10)\n","plt.gca().set(xlabel='Transformer layer',ylabel='Correlation coefficient',\n","              title='Correlations across token organizations')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj44_part8b.png')\n","plt.show()"],"metadata":{"id":"qphGuEoEsMI8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kOao5UJnJ-rk"},"execution_count":null,"outputs":[]}]}