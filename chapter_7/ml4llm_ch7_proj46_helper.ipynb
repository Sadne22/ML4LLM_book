{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[46] Statistics-based lesioning in MLP neurons</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"8UrqMO28-9ZL"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.gridspec import GridSpec\n","\n","import scipy.stats as stats\n","import torch\n","from transformers import BertTokenizer, BertForMaskedLM"],"metadata":{"id":"aoocnKDi-2RN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300\n","})"],"metadata":{"id":"U9F0prqyUFcE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BJR9NR3dr4pZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 1: Get MLP activations for him vs. her**"],"metadata":{"id":"FA6scUjTNxKZ"}},{"cell_type":"code","source":["# Load pre-trained BERT model and tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n","model = BertForMaskedLM.from_pretrained('bert-large-uncased')\n","model.eval()"],"metadata":{"id":"83A8PoVGRFj-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nneurons = model.bert.\n","print(f'There are {nneurons} units it the expansion layer.')"],"metadata":{"id":"jqTjLQ3NrqHj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# FYI, for hook location\n","model.bert.encoder.layer[4].intermediate"],"metadata":{"id":"qWJGAZcNyHqp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dictionary to store the mlp activations\n","mlp_values = {}\n","\n","def hook(module,input,output):\n","  mlp_values[f'L{whichlayer}'] =\n","\n","# surgery ;)\n","whichlayer = 9\n","handle = model.bert.encoder."],"metadata":{"id":"pQ6zFv8PY4B4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generated by Claude.ai\n","sentences = [\n","    \"I saw him at the market.\",\n","    \"She gave him the book.\",\n","    \"They asked him for advice.\",\n","    \"We invited him to dinner.\",\n","    \"The dog followed him home.\",\n","    \"They asked him to join.\",\n","    \"He saw him at the park yesterday.\",\n","    \"Did you give him your address?\",\n","    \"I haven't seen him in ages.\",\n","    \"I told him the truth.\",\n","    \"They congratulated him on his success.\",\n","    \"She recognized him immediately.\",\n","    \"The teacher praised him for his work.\",\n","    \"I met him last summer.\",\n","    \"The child hugged him tightly.\",\n","    \"They warned him about the danger.\",\n","    \"She drove him to the airport.\",\n","    \"We waited for him for hours.\",\n","    \"The cat scratched him accidentally.\",\n","    \"They surprised him with a gift.\",\n","    \"She called him on the phone.\",\n","    \"The jury found him not guilty.\",\n","    \"I remembered him from school.\",\n","    \"They elected him as president.\",\n","    \"She forgave him for his mistake.\",\n","    \"The police questioned him yesterday.\",\n","    \"I helped him with his homework.\",\n","    \"They spotted him in the crowd.\",\n","    \"She visited him in the hospital.\",\n","    \"The manager promoted him last week.\",\n","    \"I trusted him completely.\",\n","    \"They respected him for his honesty.\",\n","    \"She taught him how to swim.\",\n","    \"The bird attacked him suddenly.\",\n","    \"I greeted him warmly.\",\n","    \"They supported him through difficult times.\",\n","    \"She ignored him at the party.\",\n","    \"The judge sentenced him to community service.\",\n","    \"I photographed him during the event.\",\n","    \"They believed him despite the evidence.\",\n","    \"She surprised him on his birthday.\",\n","    \"The guard stopped him at the entrance.\",\n","    \"I missed him terribly.\",\n","    \"They watched him leave the building.\",\n","    \"She accompanied him to the concert.\",\n","    \"The crowd cheered him enthusiastically.\",\n","    \"I described him to the police.\",\n","    \"They thanked him for his help.\",\n","    \"She admired him for his courage.\",\n","    \"The committee nominated him for the award.\",\n","    \"I married him last spring.\",\n","    \"They informed him about the changes.\",\n","    \"She introduced him to the parents.\",\n","    \"The author based the character on him.\",\n","\n","## same sentences but with \"her\"\n","\n","    \"I saw her at the market.\",\n","    \"She gave her the book.\",\n","    \"They asked her for advice.\",\n","    \"We invited her to dinner.\",\n","    \"The dog followed her home.\",\n","    \"They asked her to join.\",\n","    \"He saw her at the park yesterday.\",\n","    \"Did you give her your address?\",\n","    \"I haven't seen her in ages.\",\n","    \"I told her the truth.\",\n","    \"They congratulated her on his success.\",\n","    \"She recognized her immediately.\",\n","    \"The teacher praised her for his work.\",\n","    \"I met her last summer.\",\n","    \"The child hugged her tightly.\",\n","    \"They warned her about the danger.\",\n","    \"She drove her to the airport.\",\n","    \"We waited for her for hours.\",\n","    \"The cat scratched her accidentally.\",\n","    \"They surprised her with a gift.\",\n","    \"She called her on the phone.\",\n","    \"The jury found her not guilty.\",\n","    \"I remembered her from school.\",\n","    \"They elected her as president.\",\n","    \"She forgave her for his mistake.\",\n","    \"The police questioned her yesterday.\",\n","    \"I helped her with his homework.\",\n","    \"They spotted her in the crowd.\",\n","    \"She visited her in the hospital.\",\n","    \"The manager promoted her last week.\",\n","    \"I trusted her completely.\",\n","    \"They respected her for his honesty.\",\n","    \"She taught her how to swim.\",\n","    \"The bird attacked her suddenly.\",\n","    \"I greeted her warmly.\",\n","    \"They supported her through difficult times.\",\n","    \"She ignored her at the party.\",\n","    \"The judge sentenced her to community service.\",\n","    \"I photographed her during the event.\",\n","    \"They believed her despite the evidence.\",\n","    \"She surprised her on his birthday.\",\n","    \"The guard stopped her at the entrance.\",\n","    \"I missed her terribly.\",\n","    \"They watched her leave the building.\",\n","    \"She accompanied her to the concert.\",\n","    \"The crowd cheered her enthusiastically.\",\n","    \"I described her to the police.\",\n","    \"They thanked her for his help.\",\n","    \"She admired her for his courage.\",\n","    \"The committee nominated her for the award.\",\n","    \"I married her last spring.\",\n","    \"They informed her about the changes.\",\n","    \"She introduced her to the parents.\",\n","    \"The author based the character on her.\"\n","]\n","\n","print(f'There are {len(sentences)} sentences.')"],"metadata":{"id":"oqdAP54II-ht"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# identify the target token\n","target_token_him = tokenizer.encode('him'\n","target_token_her =\n","print(f'The target token indices are {target_token_him} and {target_token_her}\\n')\n","\n","# tokenize\n","tokens ="],"metadata":{"id":"fYPkVigeOv47"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokens"],"metadata":{"id":"GTWCnORaa_24"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prepare a vector of target indices per sentence as a torch tensor\n","target_indices = torch.zeros()\n","\n","# loop over sentences\n","for senti in range(len(sentences)):\n","  targBool = torch.isin(,)\n","  target_indices[senti] = torch.where()[0]\n","  # torch.where()[0] works here b/c each sentence contains exactly one occurrance of the target.\n","\n","target_indices"],"metadata":{"id":"2w4Db2EVUxdL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","  model\n","\n","handle.remove()\n","\n","mlp_values[f'L{whichlayer}'].shape"],"metadata":{"id":"YAMzPriHL7SD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# loop through sentences to get target activations\n","\n","acts = np.zeros((len(sentences),mlp_values[].shape[]))\n","\n","# get the activations per sentence\n","for senti in range(len(sentences)):\n","  acts[senti,:] = mlp_values[f'L{whichlayer}'][,,]\n","\n","acts.shape"],"metadata":{"id":"IG2NYyxvWtUp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bLsHRatV-7zu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: T-test in one layer**"],"metadata":{"id":"5cOJO8G3-7xC"}},{"cell_type":"code","source":["# t-test and find significant neurons via FDR (correction for multiple comparisons)\n","tres = stats.ttest_ind(acts[,],acts[,],axis=0)\n","issig = stats.()<\n","\n","# find the neurons\n","himNeurons = issig &\n","herNeurons ="],"metadata":{"id":"D1F5r62TWtRR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# setup the figure\n","fig = plt.figure(figsize=(12,4))\n","gs = GridSpec(1,3,figure=fig)\n","ax0 = fig.add_subplot(gs[:2])\n","ax1 = fig.add_subplot(gs[2])\n","\n","# draw the scatter plot\n","ax0.plot(,label='Non-sig.')\n","ax0.plot(,label='him > her')\n","ax0.plot(,label='her > him')\n","\n","ax0.legend()\n","ax0.set(xlim=[-1,nneurons],xlabel='MLP expansion neurons',ylabel='T-value',\n","              title=f'{himNeurons.sum()} \"him\" neurons and {herNeurons.sum()} \"her\" neurons')\n","\n","\n","# and the pie chart\n","ax1.pie([.sum(),.sum(),-.sum()-.sum()],autopct='%1.1f%%',\n","           labels=['\"him\" significant','\"her\" significant','Non-significant'],\n","           colors=[[.7,.9,.7],[.9,.7,.7,.7],[1,0,0,.7]],wedgeprops={'edgecolor':'k'})\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj46_part2.png')\n","plt.show()"],"metadata":{"id":"35Imp7CsqvFA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xuDExKrTNxHf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: T-tests in all layers**"],"metadata":{"id":"BhFIpC0VW_qM"}},{"cell_type":"code","source":["nlayers = len(model.bert.encoder.layer)"],"metadata":{"id":"Y9bDB4IKSoFu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dictionary to store the mlp t-test results\n","mlpTs = {}\n","\n","# hook function that runs a t-test on MLP activations\n","def implant_hook(layer_number):\n","  def hook(module,input,output):\n","\n","    # detach activations\n","    mlpVals =\n","\n","    # matrix of target activation values\n","    acts = np.zeros((len(sentences),mlpVals.shape[2]))\n","    for senti in range(len(sentences)):\n","      acts[senti,:] = mlpVals[,,:]\n","\n","    # t-test and find significance\n","    tres = stats.ttest_ind(\n","    issig =\n","\n","    # store the results\n","    mlpTs[f'L{layer_number}_him'] =\n","    mlpTs[f'L{layer_number}_her'] =\n","\n","    # ### for Part 7 (leave commented before Part 7!)\n","    # numsig_pos = (issig & (tres.statistic>0)).sum()\n","    # numsig_neg = (issig & (tres.statistic<0)).sum()\n","    # mlpTs[f'L{layer_number}_him'] = np.argsort(abs(tres.statistic))[:numsig_pos]\n","    # mlpTs[f'L{layer_number}_her'] = np.argsort(abs(tres.statistic))[:numsig_neg]\n","\n","  return hook\n","\n","\n","# implant into all layers\n","handles = []\n","for layeri in range(nlayers):\n","  h = model.bert.encoder.layer[layeri].intermediate.dense.register_forward_hook(\n","  handles."],"metadata":{"id":"it98Pt5hRy1t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# forward pass\n","with torch.no_grad(): model(**tokens)\n","\n","# remove handles\n"],"metadata":{"id":"44xgc8-_Wmmo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mlpTs.keys(), mlpTs['L4_her'].shape"],"metadata":{"id":"7Zt_6zU9RWDv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,3))\n","\n","# draw the percentage of significant t-tests per layer\n","for i in range(nlayers):\n","  plt.plot(i,100*mlpTs[].sum() / ,'ko',markerfacecolor=[.9,.9,.7,.7],markersize=12)\n","  plt.plot(i,100*mlpTs[].sum() / ,'ks',markerfacecolor=[.7,.7,.9,.7],markersize=12)\n","\n","\n","# hacky solution to get legend\n","plt.plot(100,100*mlpTs[f'L{i}_her'].sum() / nneurons,'ko',markerfacecolor=[.9,.9,.7,.7],markersize=12,label='her > him')\n","plt.plot(100,100*mlpTs[f'L{i}_him'].sum() / nneurons,'ks',markerfacecolor=[.7,.7,.9,.7],markersize=12,label='him > her')\n","plt.legend()\n","\n","plt.gca().set(xlim=[-.5,nlayers-.5],xlabel='Layers',ylabel='% sig. t-values')\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj46_part3.png')\n","plt.show()"],"metadata":{"id":"g-tKTj3WM5iK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rscZ2pi1wp6C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 4: Gender word predictions in an independent dataset**"],"metadata":{"id":"Fr48LXRAWtLE"}},{"cell_type":"code","source":["texts = [ 'Robert helped Lucy with her project, and she thanked him for his hard work.',\n","          'Robert helped Lucy with [MASK] project, and she thanked him for his hard work.',\n","          'Robert helped Lucy with her project, and she thanked [MASK] for his hard work.' ]\n","\n","# tokenize\n","testtokens = tokenizer(texts,return_tensors='pt')\n","testtokens"],"metadata":{"id":"quAcxaQxhklO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find indices of [MASK]\n","mask_idx_her = torch.where(\n","mask_idx_him = torch.where(\n","\n","print(f'Masks are at indices {mask_idx_her} and {mask_idx_him}')"],"metadata":{"id":"rXaX6d6RAuft"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","  out = model(**testtokens)\n","\n","logits = out.\n","logits.shape"],"metadata":{"id":"RU38n2SnWtH9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["target_logits_clean = np.zeros((3,2,2))\n","\n","for senti in range(3):\n","  target_logits_clean[senti,0,:] = logits[,,[,]]\n","  target_logits_clean[senti,1,:] = logits[,,[,]]\n","\n","target_logits_clean"],"metadata":{"id":"GZh5E5_tWtBc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,4))\n","\n","for i in range(3):\n","  plt.bar(np.array([-.1,.1])+i*1.5,target_logits_clean[,,],width=.2,facecolor=[[.9,.3,.9],[.3,.9,.9]],edgecolor='k')\n","  plt.bar(np.array([-.1,.1])+i*1.5+.5,target_logits_clean[,,],width=.2,facecolor=[[.9,.7,.9],[.7,.9,.9]],edgecolor='k')\n","\n","# create the bar labels\n","basetxt = 'her || him     her || him\\n-------------------+-------------------\\nher position   ||   him position'\n","xticklabels = [ basetxt + '\\n\\n|_______$\\\\bf{Clean\\\\; sentence}$______|',\n","                basetxt + '\\n\\n|____$\\\\bf{HER\\\\; mask\\\\; sentence}$____|',\n","                basetxt + '\\n\\n|____$\\\\bf{HIM\\\\; mask\\\\; sentence}$____|' ]\n","\n","plt.gca().set(xticks=np.arange(.25,3.5,1.5),xticklabels=xticklabels,ylabel='Logits')\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj46_part4.png')\n","plt.show()"],"metadata":{"id":"QsCvUrpdpGyl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yEwE21tFNxCN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 5: Ablate \"him\" and \"her\" neurons in MLP**"],"metadata":{"id":"Am4TmiarWs6T"}},{"cell_type":"code","source":["# run on the final transformer layer (\"-1\" b/c of 0-indexing)\n","whichlayer = nlayers - 1\n","\n","def ablation_hook(module,input,output):\n","  output[1,mask_idx_her,mlpTs[f'L{whichlayer}_her']] =\n","  output[2, =\n","  return output\n","\n","# implant\n","handle = model.bert.encoder.layer[whichlayer]."],"metadata":{"id":"8aviMiRSn2Yc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","  out = model(**testtokens)\n","\n","handle.remove()\n","logitsZero = out.\n","logitsZero.shape"],"metadata":{"id":"0gnLylyztdkU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["target_logitsZ = np.zeros((3,2,2)) # Z = zeroed\n","\n","for senti in range(3):\n","  target_logitsZ[senti,0,:] = logitsZero[\n","  target_logitsZ[senti,1,:] ="],"metadata":{"id":"8nykC36itdkV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = plt.figure(figsize=(13,4.5))\n","gs = GridSpec(1,3,figure=fig)\n","ax0 = fig.add_subplot(gs[:2])\n","ax1 = fig.add_subplot(gs[2])\n","\n","# calculate the difference in logits\n","deltaLogits =\n","\n","# show the bar plots\n","for i in range(3):\n","  ax0.bar(,,width=.2,facecolor=[[.9,.3,.9],[.3,.9,.9]],edgecolor='k')\n","  ax0.bar(,,width=.2,facecolor=[[.9,.7,.9],[.7,.9,.9]],edgecolor='k')\n","\n","ax0.axhline(0,color='k',linewidth=.2)\n","ax0.set(xticks=np.arange(.25,3.5,1.5),xticklabels=xticklabels,ylabel='$\\\\mathbf{\\\\Delta}$ logits',\n","        title=f'A) Clean - ablated logits (ablation in layer {whichlayer})')\n","\n","# scatter plot showing modulation impact\n","ax1.plot(,,'ko',markerfacecolor=[.7,.9,.9,.6],markersize=10)\n","ax1.set(xlabel='Clean logits',ylabel='Ablation logits',title='B) Clean vs. ablated model logits')\n","ax1.grid(linewidth=.4)\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj46_part5.png')\n","plt.show()"],"metadata":{"id":"GqIgUuzbn2bb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-YcxarLwQc8i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 6: Laminar nullification of MLP neurons**"],"metadata":{"id":"gWp8PEWTRN_9"}},{"cell_type":"code","source":["# results are (1) magnitude of modulation, (2) \"her\" impact, (3) \"him\" impact\n","results = np.zeros((nlayers,3))\n","\n","\n","# loop over layers\n","for layeri in range(nlayers):\n","\n","  # patch this layer\n","  def mlp_ablate_hook(module, input, output):\n","    # zero-out the \"her neurons\" on the HER token, and the \"him neurons\" on the HIM token\n","    output[1,mask_idx_her,mlpTs[f'L{layeri}_her'\n","    output[2,\n","    return output\n","  handle = model.bert.encoder.layer[layeri].intermediate.dense.register_forward_hook(mlp_ablate_hook)\n","\n","  # forward pass to get output logits, and remove hook\n","  with torch.no_grad(): out=model(**testtokens)\n","  logitsZero =\n","  handle\n","\n","  # get the logits for the target tokens\n","  target_logitsZ = np.zeros((,,)) # (\"Z\" for zeroed out)\n","\n","  for senti in range(3):\n","    target_logitsZ[senti,0,:] =\n","    target_logitsZ[senti,1,:] =\n","  deltaLogits =  # difference between clean and ablation logits\n","\n","  # measure the total magnitude change\n","  results[layeri,0] = np.mean(\n","\n","  # specific modulations\n","  results[layeri,1] = deltaLogits[\n","  results[layeri,2] = deltaLogits[\n"],"metadata":{"id":"9WCXkpgkB20b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,axs = plt.subplots(1,2,figsize=(12,4))\n","\n","axs[0].plot(results[:,0],'kh',markerfacecolor=[.9,.7,.7],markersize=12)\n","axs[0].set(xlabel='Transformer block',ylabel='Magnitude change in logits',\n","           ylim=[0,None],title='A) Overall impact of manipulation on target logits')\n","\n","axs[1].plot(,label='HER manipulation')\n","axs[1].plot(,label='HIM manipulation')\n","axs[1].axhline(0,color='k',zorder=-5,linewidth=.5)\n","axs[1].set(xlabel='Transformer block',ylabel='Signed change in logits',title='B) Clean - ablated target $\\\\mathbf{\\\\Delta}$ logits')\n","axs[1].legend()\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj46_part6.png')\n","plt.show()"],"metadata":{"id":"3To-OVghRN9F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WxjfyfqKRN6b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 7: Ablate the least-tuned neurons**"],"metadata":{"id":"C6OBaVxAJOE4"}},{"cell_type":"code","source":[],"metadata":{"id":"E1GWnx3jJOCE"},"execution_count":null,"outputs":[]}]}