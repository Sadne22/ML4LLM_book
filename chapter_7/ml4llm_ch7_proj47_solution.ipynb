{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[47] Supervised probing with XGBoost</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"8UrqMO28-9ZL"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.gridspec import GridSpec\n","\n","import xgboost as xgb\n","from sklearn.model_selection import train_test_split\n","\n","from datasets import load_dataset\n","\n","import torch\n","from transformers import AutoModelForCausalLM,GPT2Tokenizer"],"metadata":{"id":"aoocnKDi-2RN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300\n","})"],"metadata":{"id":"U9F0prqyUFcE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BJR9NR3dr4pZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 1: XGBoost in a toy example**"],"metadata":{"id":"EFb06hWEMctJ"}},{"cell_type":"code","source":["samplesize = 500\n","n_features = 10\n","\n","# class 0 has mean 0\n","X0 = np.random.normal(loc=0,scale=1,size=(samplesize,n_features))\n","\n","# class 1 has varying means\n","X1 = np.random.normal(loc=np.linspace(.7,-.7,n_features),scale=1,size=(samplesize,n_features))\n","\n","X = np.vstack([X0,X1])\n","y = np.hstack([np.zeros(samplesize),np.ones(samplesize)])\n","\n","X.shape"],"metadata":{"id":"I1Kd6e4MMcqW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,axs = plt.subplots(1,2,figsize=(10,3))\n","\n","h = axs[0].imshow(X,aspect='auto',vmin=-1,vmax=1,cmap='plasma')\n","fig.colorbar(h,ax=axs[0],pad=.01)\n","axs[0].axhline(samplesize,linestyle='--',color='k')\n","axs[0].set(xlabel='Feature (variable)',ylabel='Observation',title='A) Full data matrix',)\n","\n","for i in range(n_features):\n","  axs[1].plot(np.zeros(samplesize)+i/n_features/2-.25,X0[:,i],'ko',markerfacecolor=plt.cm.rainbow(i/n_features),alpha=.3)\n","  axs[1].plot(np.ones(samplesize)+i/n_features/2-.25,X1[:,i],'ks',markerfacecolor=plt.cm.rainbow(i/n_features),alpha=.3)\n","\n","axs[1].set(xlim=[-1,2],xticks=[0,1],xticklabels=['Data 0','Data 1'],ylabel='Data value',title='B) Scatter plot of all data')\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj47_part1a.png')\n","plt.show()"],"metadata":{"id":"3Gup2nwg2Piv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train/test split\n","X_train,X_test, y_train,y_test = train_test_split(X,y,test_size=.25,stratify=y)\n","\n","# create an XGBoost classifier object\n","model = xgb.XGBClassifier(\n","    n_estimators  = 100, # number of sequential trees\n","    max_depth     = 2,   # model complexity (depth of tree)\n","    learning_rate = .05, # learning rate\n","    subsample     = .8,  # percent data to sample at each iteration\n","    colsample_bytree=.8, # percent features to sample\n","    eval_metric   = 'logloss'\n",")\n","\n","# fit the model to the train set\n","model.fit(X_train,y_train)\n","\n","# test performance on train set\n","yHat_train = model.predict(X_train)\n","\n","# and on the test set\n","yHat_test = model.predict(X_test)\n","\n","# print the results\n","print(f'Train accuracy: {np.mean(yHat_train==y_train):.2%}')\n","print(f'Test accuracy: {np.mean(yHat_test==y_test):.2%}')"],"metadata":{"id":"aYUFv7wG3BZM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# feature importance\n","booster = model.get_booster()\n","importances = booster.get_score(importance_type='gain')\n","\n","# and visualize\n","plt.figure(figsize=(8,3))\n","plt.plot(importances.values(),'kh',markerfacecolor=[.9,.7,.7],markersize=15)\n","plt.gca().set(xlabel='Features',xticks=range(n_features),xticklabels=importances.keys(),ylabel='Gain')\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj47_part1b.png')\n","plt.show()"],"metadata":{"id":"oyPRCDXB441k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZOqdtY7t6j8C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: Get \"that\" categories**"],"metadata":{"id":"FA6scUjTNxKZ"}},{"cell_type":"code","source":["tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"],"metadata":{"id":"cJDiIqHfOvom"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import wikitext training data\n","wikitxt = load_dataset('wikitext','wikitext-2-raw-v1',split='train')\n","tokens = tokenizer.encode('\\n\\n'.join(wikitxt['text']))"],"metadata":{"id":"7VKNxFM-5DGt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'There are {len(tokens):,} tokens in the wikitext dataset')\n","\n","# token id for \" that\"\n","that_token = tokenizer.encode(' that')[0]"],"metadata":{"id":"EjVtlMJeIYVr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# words that typically follow \"that\" as complementizer\n","post_comp = [' he', ' she', ' it', ' they', ' we', ' I', ' you',\n","             ' the', ' a', ' an', ' this', ' that', ' these', ' those']\n","post_comp_toks = [tokenizer.encode(w)[0] for w in post_comp]\n","\n","# words that typically follow \"that\" as demonstrative pronoun\n","post_dp = [' is', ' was', ' are', ' were', ' has', ' have', ' had', ' do', ' did', ' does',\n","           ' works', ' happened', ' sucks', ' matters', ' helps', ' fails', ' changed']\n","post_dp_toks = [tokenizer.encode(w)[0] for w in post_dp]\n","\n","comp_idx = []   # complementizer \"that\"\n","detr_idx = []   # demonstrative determiner \"that\"\n","pron_idx = []   # demonstrative pronoun \"that\" (used here only as a rejective filter)\n","\n","\n","for i in range(len(tokens)-1):\n","\n","  # skip if its not a \"that\" token\n","  if tokens[i] != that_token: continue\n","\n","  # complementizer if \"that + pronoun\" or \"that + determiner\"\n","  if tokens[i+1] in post_comp_toks:\n","    comp_idx.append(i)\n","\n","  # demonstrative pronoun if \"that + finite verb\"\n","  elif tokens[i+1] in post_dp_toks:\n","    pron_idx.append(i)\n","\n","  # demonstrative determiner otherwise\n","  else:\n","    detr_idx.append(i)\n","\n","print('There are')\n","print(f'  {len(comp_idx)} complementizer \"that\"s')\n","print(f'  {len(detr_idx)} demonstrative determiner \"that\"s')\n","print(f'  {len(pron_idx)} demonstrative pronoun \"that\"s')"],"metadata":{"id":"3Dp_WMYERrd0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# some data parameters\n","batchsize    = 2500 # sample size\n","context_pre  = 5    # tokens before each target"],"metadata":{"id":"S1sy2jikiETI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize batches\n","batch_detr = torch.zeros((batchsize,context_pre+1),dtype=torch.long)\n","batch_comp = torch.zeros((batchsize,context_pre+1),dtype=torch.long)\n","\n","# select random tokens\n","detr_tokens = np.random.choice(detr_idx,batchsize,replace=False)\n","comp_tokens = np.random.choice(comp_idx,batchsize,replace=False)\n","\n","### create batches\n","for i in range(batchsize):\n","  batch_detr[i,:] = torch.tensor(tokens[detr_tokens[i]-context_pre:detr_tokens[i]+1])\n","  batch_comp[i,:] = torch.tensor(tokens[comp_tokens[i]-context_pre:comp_tokens[i]+1])\n","\n","\n","print('batch_detr has shape:',list(batch_detr.shape))\n","print('And it looks like this:\\n',batch_detr)\n","print('\\n\\n')\n","\n","print('batch_comp has shape:',list(batch_comp.shape))\n","print('And it looks like this:\\n',batch_comp)"],"metadata":{"id":"-FMLAjEmI1hm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CaJ_yZ7RLcRT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: Get MLP activations**"],"metadata":{"id":"NkFA16iTLcOf"}},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","gpt2 = AutoModelForCausalLM.from_pretrained('gpt2-medium')\n","gpt2 = gpt2.to(device)\n","gpt2.eval()"],"metadata":{"id":"Xo0l9aM-LcL0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_layers = len(gpt2.transformer.h)\n","print(f'There are {n_layers} transformer layers')"],"metadata":{"id":"A_F72NhOsfRG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dictionary to store the mlp activations\n","mlp_acts = {}\n","\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","    mlp_acts[f'L{layer_number}'] = output.detach().cpu().numpy()\n","  return hook\n","\n","# implant the hooks into the 'act' (gelu) layer\n","handles = []\n","for layi in range(n_layers):\n","  h = gpt2.transformer.h[layi].mlp.act.register_forward_hook(implant_hook(layi))\n","  handles.append(h)"],"metadata":{"id":"Nkxrce1YOWM1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# process the determiner tokens\n","with torch.no_grad():\n","  outs = gpt2(batch_detr.to(device))\n","\n","# note: 'mlp_acts' gets overwritten! make sure to copy beforehand :)\n","detr_mlp = mlp_acts.copy()\n","mlp_acts = {}\n","logits_clean_detr = outs.logits[:,-1,:].cpu()\n","\n","\n","# repeat for complementizer tokens\n","with torch.no_grad():\n","  outs = gpt2(batch_comp.to(device)) # variable 'outs' overwrites, but that saves on GPU RAM\n","comp_mlp = mlp_acts.copy()\n","logits_clean_comp = outs.logits[:,-1,:].cpu()\n","\n","# remove the handles\n","for h in handles:\n","  h.remove()\n"],"metadata":{"id":"aRlKrdYtOWJ5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check sizes of activations matrices\n","for k,v in detr_mlp.items():\n","  print(f'\"{k}\" has shape {list(v.shape)}')"],"metadata":{"id":"qY1SO_LIMcy5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualize\n","_,axs = plt.subplots(1,3,figsize=(12,4))\n","\n","whichlayer = 'L10'\n","\n","axs[0].imshow(detr_mlp[whichlayer][:,-1,:],aspect='auto',vmin=-.1,vmax=.1,cmap='plasma')\n","axs[0].set(xlabel='Expansion neurons',ylabel='Batch sequence',title='A) Determiner words activations')\n","\n","axs[1].imshow(comp_mlp[whichlayer][:,-1,:],aspect='auto',vmin=-.1,vmax=.1,cmap='plasma')\n","axs[1].set(xlabel='Expansion neurons',ylabel='Batch sequence',title='B) Complementizer words activations')\n","\n","binbounds = np.linspace(-.2,.2,101)\n","yd,_ = np.histogram(detr_mlp[whichlayer][:,-1,:].flatten(),bins=binbounds,density=True)\n","yc,_ = np.histogram(comp_mlp[whichlayer][:,-1,:].flatten(),bins=binbounds,density=True)\n","\n","axs[2].plot(binbounds[:-1],yd,linewidth=2,label='Determiner')\n","axs[2].plot(binbounds[:-1],yc,linewidth=2,label='Complementizer')\n","axs[2].legend()\n","axs[2].set(xlabel='Activation value',ylabel='Density',ylim=[0,None],xlim=[binbounds[0],binbounds[-1]],title='C) Distributions')\n","\n","plt.suptitle(f'MLP post-GELU expansion activations from layer {whichlayer}',fontsize=16,fontweight='bold')\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj47_part3a.png')\n","plt.show()"],"metadata":{"id":"CkOswjjVOgRf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["histbounds = np.linspace(-.2,.5,123)\n","\n","hist_diffs = np.zeros((n_layers,len(histbounds)-1))\n","for layi in range(n_layers):\n","  yi,_ = np.histogram(detr_mlp[f'L{layi}'][:,-1,:].flatten(),bins=histbounds,density=True)\n","  yt,_ = np.histogram(comp_mlp[f'L{layi}'][:,-1,:].flatten(),bins=histbounds,density=True)\n","  hist_diffs[layi,:] = yt-yi\n","\n","\n","plt.figure(figsize=(10,4))\n","plt.imshow(hist_diffs,origin='lower',aspect='auto',vmin=-.1,vmax=.1,cmap='plasma',\n","           extent=[histbounds[0],histbounds[-1],0,n_layers-1])\n","plt.colorbar(pad=.02)\n","plt.gca().set(xlabel='Activation values',ylabel='Layer index',\n","              title='Density difference for determiner minus complementizer')\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj47_part3b.png')\n","plt.show()"],"metadata":{"id":"QRmI7eIZypXP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"L4czPzU6McwF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 4: XGBoost in one layer**"],"metadata":{"id":"anXyXYj66j0b"}},{"cell_type":"code","source":["data = np.vstack((detr_mlp['L4'][:,-1,:],comp_mlp['L4'][:,-1,:]))\n","labels = np.hstack([np.zeros(batchsize),np.ones(batchsize)])\n","\n","print(f'Data matrix is size {data.shape} and labels vector is size {labels.shape}')"],"metadata":{"id":"6Rnt12hX77T3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train/test split\n","data_train,data_test, labels_train,labels_test = train_test_split(data,labels,test_size=.25,stratify=labels)\n","\n","# create an XGBoost classifier object\n","xgb_model = xgb.XGBClassifier(\n","    n_estimators  = 100, # number of boosting iterations\n","    max_depth     = 2,   # model complexity (depth of tree)\n","    learning_rate = .05, # learning rate\n","    subsample     = .8,  # percent data to sample at each iteration\n","    colsample_bytree=.8, # percent features to sample at each iteration\n","    reg_alpha     = 5,   # L1 sparsity\n","    reg_lambda    = 5,   # L2 shrinkage\n","    eval_metric   = 'logloss'\n",")\n","\n","\n","# fit the model to the train set\n","xgb_model.fit(data_train,labels_train)\n","\n","# test performance on train set\n","yHat_train = xgb_model.predict(data_train)\n","\n","# and on the test set\n","yHat_test = xgb_model.predict(data_test)\n","\n","# print the results\n","print(f'Train accuracy: {np.mean(yHat_train==labels_train):.2%}')\n","print(f'Test accuracy: {np.mean(yHat_test==labels_test):.2%}')"],"metadata":{"id":"hVSY7yhA88X5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# feature importance\n","importances = xgb_model.get_booster().get_score(importance_type='gain')\n","\n","# dense vector of importances\n","imp_by_features = np.random.normal(0,.03,data.shape[1])\n","for ni in importances.keys():\n","  imp_by_features[int(ni[1:])] += 1\n","\n","\n","# and visualize\n","fig = plt.figure(figsize=(11,3))\n","gs = GridSpec(1,3)\n","axs1 = fig.add_subplot(gs[:-1])\n","axs2 = fig.add_subplot(gs[-1])\n","\n","axs1.plot(imp_by_features,'kh',markerfacecolor=[.9,.7,.7,.7],markeredgewidth=.3,markersize=5)\n","axs1.set(xlabel='Neuron index',ylabel='Gain',yticks=[0,1],yticklabels=['Unused','Used'],ylim=[-.3,1.3],\n","           title=f'A) Scatter plot of important features ({len(importances.keys())}/{len(imp_by_features)} neurons)')\n","\n","axs2.hist(importances.values(),bins='fd',edgecolor='k',facecolor='m')\n","axs2.set(xlabel='Gain value',ylabel='Count',title='B) Histogram of feature gains')\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj47_part4.png')\n","plt.show()"],"metadata":{"id":"JEkP4YHvMWq6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6fStC1euMcnp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 5: Laminar sweep of XGBoost**"],"metadata":{"id":"CEgQxOVbLcI-"}},{"cell_type":"code","source":["results_clean = np.zeros((n_layers,2))\n","\n","important_neurons = []\n","\n","for layeri in range(n_layers):\n","\n","  # collect new data\n","  data = np.vstack((detr_mlp[f'L{layeri}'][:,-1,:],comp_mlp[f'L{layeri}'][:,-1,:]))\n","  data_train,data_test, labels_train,labels_test = train_test_split(data,labels,test_size=.25,stratify=labels)\n","\n","  # need to recreate object b/c weights are stored not re-initialized on .fit() call\n","  xgb_model = xgb.XGBClassifier(\n","      n_estimators  = 100, # number of boosting iterations\n","      max_depth     = 2,   # model complexity (depth of tree)\n","      learning_rate = .05, # learning rate\n","      subsample     = .8,  # percent data to sample at each iteration\n","      colsample_bytree=.8, # percent features to sample at each iteration\n","      reg_alpha     = 5,   # L1 sparsity\n","      reg_lambda    = 5,   # L2 shrinkage\n","      eval_metric   = 'logloss'\n","  )\n","\n","  # fit the model to the new data\n","  xgb_model.fit(data_train,labels_train)\n","\n","  # test performances\n","  yHat_train = xgb_model.predict(data_train)\n","  yHat_test = xgb_model.predict(data_test)\n","\n","  # capture the results\n","  results_clean[layeri,0] = np.mean(yHat_train==labels_train)\n","  results_clean[layeri,1] = np.mean(yHat_test==labels_test)\n","\n","  # save the diagnostic neurons\n","  importances = xgb_model.get_booster().get_score(importance_type='gain')\n","  important_neurons.append( [int(k[1:]) for k in importances.keys()] )\n","\n","  print(f'Finished layer {layeri+1:2}/{n_layers} with {results_clean[layeri,1]:.1%} test accuracy.')"],"metadata":{"id":"lzr1pBOxMWoG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualize\n","_,axs = plt.subplots(1,2,figsize=(12,3.5))\n","\n","axs[0].plot(np.arange(n_layers)-.1,results_clean[:,0],'ro-',linewidth=.5,\n","            markerfacecolor=[.9,.7,.7],markersize=12,label='Train accuracy')\n","axs[0].plot(np.arange(n_layers)+.1,results_clean[:,1],'gs-',linewidth=.5,\n","            markerfacecolor=[.7,.9,.7],markersize=12,label='Test accuracy')\n","axs[0].legend()\n","axs[0].set(xlabel='Transformer layer',ylabel='Accuracy',title='A) Train and test accuracy by layer')\n","\n","axs[1].plot([len(i) for i in important_neurons],'kh',markerfacecolor=[.7,.7,.9],markersize=12)\n","axs[1].set(xlabel='Transformer layer',ylabel='Number of \"important\" neurons',title='B) Non-zero-gain neurons per layer')\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj47_part5.png')\n","plt.show()"],"metadata":{"id":"nLvWKw_Ddqff"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"49WZ9-voNEkA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 6: Ablate the \"important\" neurons**"],"metadata":{"id":"EvERsIxdNEg2"}},{"cell_type":"code","source":["# initialize\n","results_ablate = np.zeros_like(results_clean)\n","logit_diffs_norms = np.zeros((2,n_layers))\n","important_neurons_ablate = []\n","\n","# loop over layers\n","for layeri in range(n_layers):\n","\n","\n","  ### ------ hook to manipulate this layer\n","  def mlp_ablate_hook(module,input,output):\n","\n","    # zero-out the important neurons in this layer\n","    out = output.clone() # copy of the output\n","    out[:,-1,important_neurons[layeri]] = 0\n","\n","    # and store those activations\n","    mlp_acts['acts'] = out.detach().cpu().numpy()\n","\n","    return out # return the modified output\n","\n","  handle = gpt2.transformer.h[layeri].mlp.act.register_forward_hook(mlp_ablate_hook)\n","  ### ------ hook to manipulate this layer\n","\n","\n","  # forward passes\n","  mlp_acts = {} # re-initialize dictionary\n","  with torch.no_grad():\n","    outs = gpt2(batch_detr.to(device))\n","  detr_mlp = mlp_acts.copy()\n","\n","  # compare logits matrices\n","  diffmat = outs.logits[:,-1,:].cpu()-logits_clean_detr\n","  logit_diffs_norms[0,layeri] = torch.norm( diffmat ).item()\n","\n","\n","\n","  mlp_acts = {} # re-initialize dictionary\n","  with torch.no_grad():\n","    outs = gpt2(batch_comp.to(device))\n","  comp_mlp = mlp_acts.copy()\n","\n","  # remove the hook\n","  handle.remove()\n","\n","  # compare logits matrices\n","  diffmat = outs.logits[:,-1,:].cpu()-logits_clean_comp\n","  logit_diffs_norms[1,layeri] = torch.norm( diffmat ).item()\n","\n","\n","\n","\n","  ### --- XGBoost analysis --- ###\n","  # collect new data\n","  data = np.vstack((detr_mlp['acts'][:,-1,:],comp_mlp['acts'][:,-1,:]))\n","  data_train,data_test, labels_train,labels_test = train_test_split(data,labels,test_size=.25,stratify=labels)\n","\n","  # need to recreate object b/c weights are stored not re-initialized on .fit() call\n","  xgb_model = xgb.XGBClassifier(\n","      n_estimators  = 100, # number of boosting iterations\n","      max_depth     = 2,   # model complexity (depth of tree)\n","      learning_rate = .05, # learning rate\n","      subsample     = .8,  # percent data to sample at each iteration\n","      colsample_bytree=.8, # percent features to sample at each iteration\n","      reg_alpha     = 5,   # L1 sparsity\n","      reg_lambda    = 5,   # L2 shrinkage\n","      eval_metric   = 'logloss'\n","  )\n","\n","  # fit the model to the new data\n","  xgb_model.fit(data_train,labels_train)\n","\n","  # test performances\n","  yHat_train = xgb_model.predict(data_train)\n","  yHat_test = xgb_model.predict(data_test)\n","\n","  # capture the results\n","  results_ablate[layeri,0] = np.mean(yHat_train==labels_train)\n","  results_ablate[layeri,1] = np.mean(yHat_test==labels_test)\n","\n","  # save the diagnostic neurons\n","  importances = xgb_model.get_booster().get_score(importance_type='gain')\n","  important_neurons_ablate.append( [int(k[1:]) for k in importances.keys()] )\n","\n","  print(f'Finished layer {layeri+1:2}/{n_layers} with {results_ablate[layeri,1]:.1%} test accuracy.')"],"metadata":{"id":"Nh0YNaHdNEa_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualize\n","_,axs = plt.subplots(1,3,figsize=(12,3.5))\n","\n","# train and test performance from the ablated LLM\n","axs[0].plot(np.arange(n_layers)-.1,results_ablate[:,0],'ro-',linewidth=.5,\n","            markerfacecolor=[.9,.7,.7],markersize=12,label='Train accuracy')\n","axs[0].plot(np.arange(n_layers)+.1,results_ablate[:,1],'gs-',linewidth=.5,\n","            markerfacecolor=[.7,.9,.7],markersize=12,label='Test accuracy')\n","axs[0].legend()\n","axs[0].set(xlabel='Transformer layer',ylabel='Accuracy',title='A) Classification accuracies')\n","\n","\n","# differences from clean model\n","axs[1].plot(np.arange(n_layers)-.1,results_clean[:,0]-results_ablate[:,0],'ro-',linewidth=.5,\n","            markerfacecolor=[.9,.7,.7],markersize=12,label='Train accuracy')\n","axs[1].plot(np.arange(n_layers)+.1,results_clean[:,1]-results_ablate[:,1],'gs-',linewidth=.5,\n","            markerfacecolor=[.7,.9,.7],markersize=12,label='Test accuracy')\n","axs[1].legend()\n","axs[1].set(xlabel='Transformer layer',ylabel='Accuracy',ylim=[-.05,.05],title='B) Change from \"clean\" run')\n","axs[1].axhline(0,linestyle='--',color='k',linewidth=.5)\n","\n","\n","# number of \"important\" neurons\n","axs[2].plot([len(i) for i in important_neurons_ablate],'kh',markerfacecolor=[.7,.7,.9],markersize=12)\n","axs[2].set(xlabel='Transformer layer',ylabel='Number of \"important\" neurons',title='C) Non-zero-gain neurons per layer')\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj47_part6a.png')\n","plt.show()"],"metadata":{"id":"20ZBpXQTNEYE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(9,3))\n","\n","plt.plot(logit_diffs_norms[0,:],'ro-',linewidth=.5,\n","            markerfacecolor=[.9,.7,.7],markersize=12,label='Determiner')\n","plt.plot(logit_diffs_norms[1,:],'bs-',linewidth=.5,\n","            markerfacecolor=[.7,.7,.9],markersize=12,label='Complementizer')\n","\n","plt.legend()\n","plt.gca().set(xlabel='Ablated layer',ylabel='Norm of difference matrix')\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj47_part6b.png')\n","plt.show()"],"metadata":{"id":"9oMVvxl6wDnW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jcYnUn18LcGa"},"execution_count":null,"outputs":[]}]}