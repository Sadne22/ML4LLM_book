{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"167ENw4jid1g4t7B2QLNu5FTxWBQ1g5Zg","timestamp":1765857447854}],"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[44] Grammar tuning in MLP neurons</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"8UrqMO28-9ZL"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import scipy.stats as stats\n","!pip install pingouin\n","import pingouin as pg # for effect size calculations\n","\n","import requests\n","\n","import torch\n","from transformers import AutoModelForCausalLM,AutoTokenizer"],"metadata":{"id":"aoocnKDi-2RN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300\n","})"],"metadata":{"id":"U9F0prqyUFcE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BJR9NR3dr4pZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 1: Get nouns and verbs**"],"metadata":{"id":"uu2yxPwQ0M-g"}},{"cell_type":"code","source":["# Eleuther's tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-125m')\n","\n","# load in GPTneo\n","model = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-125m')\n","model.eval()"],"metadata":{"id":"u1V-auBrPSS1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# main repo: https://github.com/david47k/top-english-wordlists/\n","\n","# lists of verbs\n","url = 'https://raw.githubusercontent.com/david47k/top-english-wordlists/refs/heads/master/top_english_verbs_lower_10000.txt'\n","all_verbs = requests.get(url).text.split('\\n')\n","\n","# initialize as empty list\n","verbs =\n","len_verbs =\n","\n","# loop over all the verbs\n","for word in all_verbs:\n","\n","  # tokenize with preceding space\n","  tok =\n","\n","  # add to the list if its single-token\n","  if len(tok)==1:\n","    verbs.\n","    len_verbs.\n","\n","\n","# split by odd/even\n","verbs_split1 =\n","verbs_split2 =\n","\n","# and print\n","print(f'{} out of {} verbs are single-token.')\n","print(f'There are {} split-1 and {} split-2 samples.')"],"metadata":{"id":"ng-VWPU_eXPt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Note: the .split('\\n') method adds an extra element at the end, which is why there seems to be 10,001 verbs:\n","all_verbs[-1]"],"metadata":{"id":"KXj8ln5EmJUl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# repeat for nouns\n","url = 'https://raw.githubusercontent.com/david47k/top-english-wordlists/refs/heads/master/top_english_nouns_lower_10000.txt'\n","all_nouns = requests.get(url).text.split('\\n')\n","\n","# initialize as empty list\n","nouns =\n","len_nouns =\n","\n","# loop over all the nouns\n","for word in\n","\n","  # tokenize with preceding space\n","  tok =\n","\n","  # add to the list if its single-token\n","  if len(tok)\n","    nouns.\n","    len_nouns.\n","\n","# split by odd/even\n","nouns_split1 =\n","nouns_split2 =\n","\n","# and print\n","print)\n","print()"],"metadata":{"id":"D9LEmR85sH3W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('First 5 split-1 verbs:')\n","print([tokenizer.decode(v) for v in v\n","\n","print('\\nFirst 5 split-2 verbs:')\n","print()\n","\n","\n","print('\\n\\nFirst 5 split-1 nouns:')\n","print()\n","\n","print('\\nFirst 5 split-2 nouns:')\n","print()"],"metadata":{"id":"indQK6aMJ2IS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check lengths\n","\n","# convenient to have in numpy\n","len_nouns =\n","len_verbs =\n","\n","yN =\n","yV =\n","\n","plt.figure(figsize=(10,3))\n","plt.bar(,width=.6,label='Nouns',alpha=.9,edgecolor='b')\n","plt.bar(,width=.6,label='Verbs',alpha=.9,edgecolor='r')\n","\n","tres = stats.ttest_ind(,)\n","cohensd = pg.compute_effsize(,,paired=,eftype=)\n","\n","plt.gca().set(xticks=range(np.max(len_nouns)),xlabel='Number of characters',ylabel='Count',\n","              title=f\"t() = {}, p = {}\\nCohen's d = {}\")\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj44_part1.png')\n","plt.show()"],"metadata":{"id":"-XUBLoadV1iw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8V9gC5JaHEim"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: Implant a hook and get activations**"],"metadata":{"id":"gEaT5J0hn0mw"}},{"cell_type":"code","source":["model"],"metadata":{"id":"lbUitFlPnuAb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# a hook function to grab the activations\n","mlp_acts = {}\n","\n","def hook(module,input,output):\n","  mlp_acts[f'{whichdata}'] =\n","\n","handle = model.tra"],"metadata":{"id":"-5ZWugpPnt85"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# make a batch\n","torch.tensor(nouns_split1).unsqueeze(1).shape"],"metadata":{"id":"x7CmqvCRHEaw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# this cell takes around 20 s\n","\n","# reinialize data-dictionary\n","mlp_acts =\n","\n","with torch.no_grad():\n","\n","  # run the split1 nouns\n","  whichdata = 'nouns_split1'\n","  model(torch.tensor().unsqueeze(..))\n","\n","  # split2 nouns\n","  whichdata = 'nouns_split2'\n","  model(\n","\n","  # the split1 verbs\n","  whichdata = 'verbs_split1'\n","  model(\n","\n","  # and the split2 verbs\n","  whichdata =\n","  model(\n"],"metadata":{"id":"rj7kAE12HEdc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for k,v in mlp_acts.items():\n","  print(f\"mlp_acts['{}'] has shape {}\")"],"metadata":{"id":"7EAS4gIHOIU7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,3,figsize=(12,3.5))\n","\n","axs[0].imshow()\n","axs[0].set(xlabel='Neurons',ylabel='Nouns (index)',title='A) Nouns activations')\n","\n","axs[1].plot(mlp_acts[].mean(axis=),'ko',markersize=5,markerfacecolor=[.9,.7,.9,.5])\n","axs[1].set(xlabel='Neurons',ylabel='Activation',title='B) Mean activations over all nouns')\n","\n","axs[2].plot(,,\n","            'ko',markersize=5,markerfacecolor=[.9,.7,.9,.5])\n","axs[2].set(xlabel='Nouns',ylabel='Verbs',title='C) Activations to all words')\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj44_part2.png')\n","plt.show()"],"metadata":{"id":"8xMmNCfIHEVw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fCtFiFHnJ_Fy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: T-tests on split-1 data**"],"metadata":{"id":"5sFuiDm7J_Ag"}},{"cell_type":"code","source":["nneurons = mlp_acts['nouns_split1'].\n","nneurons"],"metadata":{"id":"TMCWZRjw1MgY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# t-test on all neurons\n","T_split1 = stats.ttest_ind(,,axis=)\n","\n","# Cohen's d\n","cohensd = np.zeros()\n","for i in range(\n","  cohensd[i] = pg.compute_effsize(mlp_acts['nouns_split1'][,],mlp_acts['verbs_split1'][,],)\n","\n","# plot\n","plt.plot(,,'ko',markerfacecolor='w')\n","plt.gca().set(xlabel=\"Cohen's d\",ylabel='T-value')\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj44_part3a.png')\n","plt.show()"],"metadata":{"id":"fqXGfxMesLPI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,4))\n","\n","# pvalues FDR corrected\n","sigPvals1 = stats.false_discovery_control() <\n","\n","# plot the significant neurons\n","plt.plot(,,'go',markerfacecolor='w')\n","\n","# significant and large effect size (Cohen's d>.8)\n","plt.plot(,,'go')\n","\n","# non-significant\n","plt.plot(,,'rx')\n","\n","# adjustments\n","plt.gca().set(xlabel='Neuron index',ylabel='T-value',xlim=[-10,nneurons+10],\n","              title=f'{np.sum(sigPvals1)}/{len(sigPvals1)} were significant, {np.sum(abs(cohensd)>.8)}/{len(cohensd)} were large effects.')\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj44_part3b.png')\n","plt.show()"],"metadata":{"id":"27unvTBmWJUT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Hnktug2fJ-5R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 4: T-tests on split-2 data**"],"metadata":{"id":"i7uXAQheR01D"}},{"cell_type":"code","source":["# in split 2\n","T_split2 = stats.ttest_ind(,,axis=)\n","\n","# across the two splits\n","T_split12 = stats.ttest_ind(,,axis=)"],"metadata":{"id":"9iSknAAFX0mS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Bonferroni correction\n","sigPthresh = .05 /\n","\n","# find where one or both are significant\n","bothSig_2  = (T_split1.pvalue<sigPthresh).astype(int) +\n","bothSig_12 =\n","\n","# correlations between t-values\n","r_2  = np.corrcoef()[0,1]\n","r_12 = np.corrcoef()[0,1]\n","\n","\n","# visualizations\n","_,axs = plt.subplots(1,2,figsize=(8,3.5))\n","\n","# split-1 vs. split-2\n","axs[0].plot(T_split1.statistic[bothSig_2==],T_split2.statistic[bothSig_2==],'ks',markerfacecolor=[.7,.9,.7,.5],markersize=5,label='Both sig.')\n","axs[0].plot(,label='Neither sig')\n","axs[0].plot(,label='One sig.')\n","\n","# split-1 vs. split-12\n","axs[1].plot(,label='Both sig.')\n","axs[1].plot(,label='Neither sig')\n","axs[1].plot(,label='One sig.')\n","\n","# axis adjustments\n","axs[0].set(xlabel='Split-1 t-value',ylabel='Split-2 t-value',title=f'A) T-val comparison (r = {r_2:.3f})')\n","axs[1].set(xlabel='Split-1 t-value',ylabel='Split-12 t-value',title=f'B) T-val comparison (r = {r_12:.3f})')\n","\n","# common adjustments\n","for a in axs:\n","  a.axhline(0,color='k',linestyle='--',linewidth=.5)\n","  a.axvline(0,color='k',linestyle='--',linewidth=.5)\n","  a.legend()\n","\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj44_part4.png')\n","plt.show()"],"metadata":{"id":"F39i49JfbTaP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"IU0gJZa1iu-J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 5: Investigating distributions**"],"metadata":{"id":"TMtgfZcBiu7O"}},{"cell_type":"code","source":["# extract histograms\n","yNouns1,xNouns1 = np.histogram(mlp_acts['nouns_split1'],bins='fd')\n","yNouns2,xNouns2 =\n","yVerbs1,xVerbs1 =\n","yVerbs2,xVerbs2 =\n","\n","# and visualize them\n","plt.figure(figsize=(9,3))\n","plt.plot(,label='Nouns 1')\n","plt.plot(,label='Nouns 2')\n","plt.plot(,label='Verbs 1')\n","plt.plot(,label='Verbs 2')\n","\n","plt.legend()\n","plt.gca().set(xlabel='MLP expansion activation',ylabel='Count',ylim=[0,None],\n","              title='Histograms of all MLP neurons')\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj44_part5a.png')\n","plt.show()"],"metadata":{"id":"RvU9BTGjerZf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# histograms of t>0 and t<0 subpopulations\n","yNouns1_neg,xNouns1_neg = np.histogram(mlp_acts['nouns_split1'][:,T_split1.statistic<0],bins='fd',density=True)\n","yVerbs1_neg,xVerbs1_neg =\n","yNouns1_pos,xNouns1_pos =\n","yVerbs1_pos,xVerbs1_pos =\n","\n","plt.figure(figsize=(9,3))\n","plt.plot(xNouns1_neg[:-1],yNouns1_neg,linewidth=2,label='Nouns t<0')\n","plt.plot(,label='Verbs t<0')\n","plt.plot(,label='Nouns t>0')\n","plt.plot(,label='Verbs t>0')\n","\n","plt.legend()\n","plt.gca().set(xlabel='MLP expansion activation',ylabel='Density',ylim=[0,None],\n","              title='Histograms separated by t-value sign')\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj44_part5b.png')\n","plt.show()"],"metadata":{"id":"sneWBw1WbbjV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find the neurons with the largest positive and negative t-values\n","max_t =\n","min_t =\n","\n","# and get their histograms\n","yNouns1_max,xNouns1_max =\n","yVerbs1_max,xVerbs1_max =\n","yNouns1_min,xNouns1_min =\n","yVerbs1_min,xVerbs1_min =\n","\n","plt.figure(figsize=(9,3))\n","plt.plot(xNouns1_max[:-1],yNouns1_max,'r',linewidth=2,label=f'Nouns (t = {T_split1.statistic[max_t]:.2f})')\n","plt.plot(xVerbs1_max[:-1],yVerbs1_max,'g',linewidth=2,label=f'Verbs (t = {T_split1.statistic[max_t]:.2f})')\n","plt.plot(xNouns1_min[:-1],yNouns1_min,'r--',linewidth=2,label=f'Nouns (t = {T_split1.statistic[min_t]:.2f})')\n","plt.plot(xVerbs1_min[:-1],yVerbs1_min,'g--',linewidth=2,label=f'Verbs (t = {T_split1.statistic[min_t]:.2f})')\n","\n","plt.gca().set(xlabel='MLP expansion activation',ylabel='Density',ylim=[0,None],\n","              title='Histograms from two neurons')\n","\n","plt.legend()\n","plt.tight_layout()\n","plt.savefig('ch7_proj44_part5c.png')\n","plt.show()"],"metadata":{"id":"PR53ejqAhEW_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FDUw-mCs3dMK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 6: Within-category tests**"],"metadata":{"id":"xnAf0BxC3dF_"}},{"cell_type":"code","source":["# within-category t-tests\n","T_withinNoun = stats.ttest_ind(,,axis=0)\n","T_withinVerb = stats.ttest_ind(,,axis=0)\n","\n","# and plot\n","fig,axs = plt.subplots(1,2,figsize=(9,4))\n","axs[0].plot(,,'ko',markerfacecolor=[.9,.7,.7,.3])\n","axs[1].plot(,,'ks',markerfacecolor=[.7,.9,.7,.3])\n","\n","axlim =\n","axs[0].set(xlim=[-axlim,axlim],ylim=[-axlim,axlim],xlabel='t(nouns,verbs), split 1',ylabel='t(nouns-1,nouns-2)',\n","           title='A) Across vs. within-nouns comparison')\n","axs[1].set(xlim=[-axlim,axlim],ylim=[-axlim,axlim],xlabel='t(nouns,verbs), split 2',ylabel='t(verbs-1,verbs-2)',\n","           title='B) Across vs. within-verbs comparison')\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj44_part6a.png')\n","plt.show()"],"metadata":{"id":"ZGCJHgIJVNSP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cohen's d\n","cohensd_within = np.zeros(nneurons)\n","for i in range(nneurons):\n","  cohensd_within[i] = pg.compute_effsize(\n","\n","# histograms\n","yW,xW = np.histogram(,bins='fd')\n","yA,xA = np.histogram(,bins='fd')\n","\n","# visualize\n","plt.figure(figsize=(9,3))\n","plt.plot(xW[:-1],yW,'o-',linewidth=2,label='Within category')\n","plt.plot(xA[:-1],yA,'s-',linewidth=2,label='Across category')\n","\n","# indicating effect sizes\n","plt.axvline(.2,linestyle='--',color='r',label='Small effect')\n","plt.axvline(.8,linestyle=':',color='m',label='Large effect')\n","\n","plt.legend()\n","plt.gca().set(xlabel=\"Cohen's d\",ylabel='Count')\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj44_part6b.png')\n","plt.show()"],"metadata":{"id":"1gWJUqav2P-Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"j1cmJ5ghR0vn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 7: Laminar profile of tuning**"],"metadata":{"id":"Q-O2wjEMR0sy"}},{"cell_type":"code","source":["n_layers = len(model.transformer.h)"],"metadata":{"id":"6Bgn3ua6fL25"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# remove previous hook\n","handle.remove()\n","\n","def outerHook(layeri):\n","  def hook(module,input,output):\n","    mlp_acts[f'L{layeri}_{whichdata}'] = output.detach().numpy().squeeze()\n","  return hook\n","\n","\n","# surgery ;)\n","handles = []\n","for layeri in range(n_layers):\n","  h =\n","  handles."],"metadata":{"id":"9D0hw0lhS-67"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# this cell takes around 20 s\n","\n","# reinialize data-dictionary\n","mlp_acts\n","\n","with torch.no_grad():\n","\n","  # run the split1 nouns\n","  whichdata = 'nouns_split1'\n","  model(\n","\n","  # split2 nouns\n","  whichdata = 'nou\n","  model(\n","\n","  # the split1 verbs\n","  whichdata =\n","  model\n","\n","  # and the split2 verbs\n","  whichdata = ''\n","  model\n"],"metadata":{"id":"n-nerRTER0qU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for k,v in mlp_acts.items():\n","  print(f\"mlp_acts['{k}'] has shape {list(v.shape)}\")"],"metadata":{"id":"0Z31wfJEf5vW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sig_neurons = np.zeros((n_layers,5))\n","\n","for layeri in range(n_layers):\n","\n","  # run the t-tests\n","  T_split1 = stats.ttest_ind(\n","  T_split2 = stats.ttest_ind(\n","\n","  # boolean of significant tests\n","  issig1 = stats.<.05\n","  issig2 = stats.<.05\n","\n","  # proportion of significant neurons\n","  sig_neurons[layeri,0] =\n","  sig_neurons[layeri,1] =\n","\n","  # average significant t-values\n","  sig_neurons[layeri,2] =\n","  sig_neurons[layeri,3] =\n","\n","  # correlation between them\n","  sig_neurons[layeri,4] = np.corrcoef(T_split1.statistic,T_split2.statistic)[0,1]\n"],"metadata":{"id":"h_z2WPvHgC3N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,3,figsize=(12,3))\n","\n","# proportion significant neurons\n","axs[0].plot(,label='Split-1')\n","axs[0].plot(label='Split-2')\n","axs[0].set(xlabel='Transformer layer',ylabel='Proportion significant neurons',title='A) Proportion significant neurons')\n","axs[0].legend()\n","\n","# average t-values\n","axs[1].plot(,label='Split-1')\n","axs[1].plot(,label='Split-2')\n","axs[1].set(xlabel='Transformer layer',ylabel='Average t-values',title='B) |T| of significant neurons')\n","axs[1].legend()\n","\n","# correlations\n","axs[2].plot(sig_neurons[:,4],'kh',markersize=12,markerfacecolor=[.7,.7,.9])\n","axs[2].set(xlabel='Transformer layer',ylabel='Correlation coefficient',title='C) T-value split correlations')\n","\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj44_part7.png')\n","plt.show()"],"metadata":{"id":"HGplKVY2hnAP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"b2BbLDVrJ-2d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 8: Tokens in vs. out of order**"],"metadata":{"id":"WNggIWhPJ-zo"}},{"cell_type":"code","source":["# source: https://en.wikipedia.org/wiki/Coconut\n","text = 'The coconut (Cocos nucifera) is a member of the palm family (Arecaceae) and the only living species of the genus Cocos.'\n","\n","tokens = tokenizer.encode(text,return_tensors='pt')\n","print(f'There are {len(text)} characters and {len(tokens[0])} tokens.')"],"metadata":{"id":"O_f1XHEHR5Cw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# scramble and invert\n","scrambled_idx = torch.randperm(\n","scrambled_tokens = tokens[0,\n","inverse_idx =\n","\n","print(f'Original sentence:\\n {tokenizer.decode(tokens[0,:])}\\n')\n","print(f'Scrambled sentence:\\n {tokenizer.decode(scrambled_tokens[0,:])}\\n')\n","print(f'Inverted scrambling:\\n {}')\n"],"metadata":{"id":"67rl1zrY4CMw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mlp_acts = {}\n","\n","with torch.no_grad():\n","\n","  whichdata = 'sentence'\n","  model(tokens)\n","\n","  whichdata = 'words'\n","  model(\n","\n","  whichdata = 'scrambled'\n","  model("],"metadata":{"id":"exCcucb8J-w8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for k,v in mlp_acts.items():\n","  print(f\"mlp_acts['{k}'] has shape {list(v.shape)}\")"],"metadata":{"id":"8CyU_ERPJ-uP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,3,figsize=(12,3.5))\n","\n","skip = 10\n","layer = 3\n","\n","sent = mlp_acts[f'L{layer}_sentence'].flatten()[::skip]\n","word = mlp_acts[f'L{layer}_words'].\n","scrm =\n","\n","axs[0].plot(,,'ko',markersize=3,markerfacecolor=[.7,.7,.9,.3])\n","axs[0].set(xlabel='Sentence',ylabel='Words',title=f'A) Sentence vs. words (r = {np.corrcoef(sent,word)[0,1]:.3f})')\n","\n","axs[1].plot(,,'ks',markersize=3,markerfacecolor=[.7,.9,.7,.3])\n","axs[1].set(xlabel='Sentence',ylabel='Scrambled sentence',title=f'B) Sentence vs. scrambled (r = {np.corrcoef(sent,scrm)[0,1]:.3f})')\n","\n","axs[2].plot(,,'k^',markersize=3,markerfacecolor=[.9,.7,.7,.3])\n","axs[2].set(xlabel='Scrambled sentence',ylabel='Words',title=f'C) Scrambled vs. words (r = {np.corrcoef(scrm,word)[0,1]:.3f})')\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj44_part8a.png')\n","plt.show()"],"metadata":{"id":"NyzLEiDWTmWv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Rs = np.zeros((n_layers,3))\n","\n","# loop over layers\n","for layeri in range(n_layers):\n","\n","  # extract the activations\n","  sent = mlp_acts[f'L{layeri}_sentence\n","  word = mlp_acts\n","  scrm =\n","\n","  # correlation coefficients\n","  Rs[layeri,0] = np.corrcoef(sent,word)[0,1]\n","  Rs[layeri,1] =\n","  Rs[layeri,2] =\n","\n","# and the visualizations\n","plt.figure(figsize=(10,3))\n","plt.plot(,label='Sentence-word')\n","plt.plot(,label='Sentence-scrambled')\n","plt.plot(,label='Scrambled-word')\n","\n","plt.axhline(0,linestyle='--',color='k',linewidth=.4,zorder=-10)\n","plt.gca().set(xlabel='Transformer layer',ylabel='Correlation coefficient',\n","              title='Correlations across token organizations')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj44_part8b.png')\n","plt.show()"],"metadata":{"id":"qphGuEoEsMI8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kOao5UJnJ-rk"},"execution_count":null,"outputs":[]}]}