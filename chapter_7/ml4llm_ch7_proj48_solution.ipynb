{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"17Tg233GccCb9jtiUbK_zDde8TIbNKQ7t","timestamp":1766061553462}],"gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[48] \"Can\" vs. \"can't\" classification via logistic regression</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"fXOrNMhxq2Ya"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BvQj17hzqwM"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.gridspec import GridSpec\n","\n","from tqdm import tqdm\n","\n","import statsmodels.api as sm\n","from sklearn.model_selection import train_test_split\n","\n","from datasets import load_dataset\n","\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer"]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300\n","})"],"metadata":{"id":"BvREVw_VesPV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"h-ni2ehqyemp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 1: Create two batches of \"can\" tokens**"],"metadata":{"id":"CqifcaigyOOF"}},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained('gpt2')"],"metadata":{"id":"8GigJExGgspM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tokenizations\n","words = [ \"can\",\" can\",\"can't\",\" can't\" ]\n","for w in words:\n","  print(f'\"{w}\" comprises tokens {list(tokenizer.encode(w))}')"],"metadata":{"id":"wfoDCbJHKNvR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# FYI 1: alternative tokenizations\n","words = [ \" can't\",\" can’t\",\" can‘t\",\" canʼt\",\" can′t\", \" can՚t\" ]\n","for w in words:\n","  print(f'\"{w}\" comprises tokens {list(tokenizer.encode(w))}')"],"metadata":{"id":"gNky6YFQ5j9i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# FYI 2: unicode for different apostrophes\n","for a in [ \"'\",\"’\",\"‘\",\"ʼ\",\"′\",\"՚\" ]:\n","  print(f'{a} is U+{ord(a):04X}')"],"metadata":{"id":"7o_iLaOt8OXA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# setup the c4 dataset for streaming\n","dataset = load_dataset('allenai/c4','en',split='train',streaming=True)\n","dataset"],"metadata":{"id":"8RvDaAX7bAfI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dataset parameters\n","context_pre = 10\n","min_samplesize = 500\n","\n","# target tokens\n","cantok = tokenizer.encode(' can')[0]\n","attok = tokenizer.encode(\"'t\")[0]\n","\n","# \"can\" is a noun if these words follow\n","excluded_following = [tokenizer.encode(word)[0] for word in [' of',' with',' from',' in',' on',' for',' not']]\n","\n","# initialize empty lists\n","cant_tokens = []\n","can_tokens = []\n","\n","# keep streaming in new samples\n","for i,sample in enumerate(dataset):\n","\n","  # stop when enough tokens\n","  if len(cant_tokens)>=min_samplesize:\n","    break\n","\n","  # tokenize the text from this sample\n","  tokens = tokenizer.encode(sample['text'])\n","\n","  # loop over tokens\n","  for ti in range(context_pre,len(tokens)-1):\n","\n","    # if this token is \"can\"\n","    if tokens[ti]==cantok:\n","\n","      # next token starts with a space\n","      if tokenizer.decode(tokens[ti+1]).startswith(' '):\n","        if tokens[ti+1] not in excluded_following:\n","          can_tokens.append(tokens[ti-context_pre:ti+1])\n","\n","      # if the next token is \"'t\"\n","      if tokens[ti+1]==attok:\n","        cant_tokens.append(tokens[ti-context_pre:ti+1])\n","\n","        # print a status update\n","        if len(cant_tokens)%50==0:\n","          print(f'Found {len(cant_tokens)} \"can\\'t\" tokens so far...')\n","\n","len(can_tokens), len(cant_tokens)"],"metadata":{"id":"nfexa4eYbAcg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# some examples\n","print('Some \"can\" sequences:')\n","for i in range(5):\n","  print(tokenizer.decode(can_tokens[i]))\n","\n","print('\\nSome \"can\\'t\" sequences:')\n","for i in range(5):\n","  print(tokenizer.decode(cant_tokens[i]))\n"],"metadata":{"id":"oVbpBHPL5yDv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create batches\n","batch_can  = torch.tensor(can_tokens)\n","batch_cant = torch.tensor(cant_tokens)\n","\n","batch_can, batch_cant"],"metadata":{"id":"jFCT1MvIHxXb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# match length\n","minN = min(len(can_tokens), len(cant_tokens))\n","\n","# note: the solution below works, but mixes numpy and pytorch (kinda ugly)\n","# batch_cant = batch_cant[np.random.choice(np.arange(len(cant_tokens)),minN,replace=False),:]\n","\n","# torch has no equivalent of np.random.choice, so you can permute and select the first N\n","idx = torch.randperm(len(batch_can))[:minN]\n","batch_can = batch_can[idx,:]\n","\n","idx = torch.randperm(len(batch_can))[:minN]\n","batch_cant = batch_cant[idx,:]\n","\n","batch_can.shape, batch_cant.shape"],"metadata":{"id":"Sl3wa5ZV6UkN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1xSO3AVtfd-V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: Get MLP activations**"],"metadata":{"id":"jZ_MnC6QEGZz"}},{"cell_type":"code","source":["# load GPT2 model and tokenizer\n","model = AutoModelForCausalLM.from_pretrained('gpt2-medium')\n","\n","# use GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","# move the model to the GPU\n","model = model.to(device)\n","model.eval()"],"metadata":{"id":"vEwYHoqWz0nB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["activations = {}\n","\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","\n","    # store in the dictionary\n","    activations[f'mlp_{layer_number}'] = output.detach().cpu().numpy()\n","  return hook\n","\n","# put hooks in all layers\n","handles = []\n","for layeri in range(len(model.transformer.h)):\n","  h = model.transformer.h[layeri].mlp.c_fc.register_forward_hook(implant_hook(layeri))\n","  handles.append(h)"],"metadata":{"id":"x01-3ZnZ4VgC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# process the can tokens\n","with torch.no_grad():\n","  model(batch_can.to(device))\n","\n","# copy the activations\n","can_activations = activations.copy()\n","\n","\n","### repeat for can't tokens\n","with torch.no_grad():\n","  model(batch_cant.to(device))\n","cant_activations = activations.copy()"],"metadata":{"id":"KtC8DXBhxqkl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(can_activations.keys(),'\\n')\n","\n","can_activations['mlp_5'].shape"],"metadata":{"id":"obLbw14UeX0g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MCiMqALDrlL6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: Logistic regression in all neurons from one layer**"],"metadata":{"id":"Xj3IFDcODv0O"}},{"cell_type":"code","source":["# we'll use this vector repeatedly\n","category_labels = np.hstack((np.zeros(minN,bool),np.ones(minN,bool)))\n","category_labels"],"metadata":{"id":"weyicIH1LFYR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# some definitions\n","\n","# MLP transformer layer\n","whichLayer2use = 3\n","\n","# for train/test split\n","test_prop = .2\n","\n","# number of expansion neurons\n","nneurons = can_activations['mlp_5'].shape[-1]"],"metadata":{"id":"VoBfPMPDEkoM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize matrix to store the classifier results\n","classifierResults = np.full((nneurons,4),np.nan)\n","\n","# loop over neurons for per-neuron analysis\n","for neuroni in tqdm(range(nneurons)):\n","\n","  # isolate the final-token activations\n","  catC = can_activations[f'mlp_{whichLayer2use}'][:,-1,neuroni]\n","  catT = cant_activations[f'mlp_{whichLayer2use}'][:,-1,neuroni]\n","  X = sm.add_constant(np.hstack((catC,catT)))\n","\n","  # split the data\n","  X_train,X_test, y_train,y_test = train_test_split(X,category_labels,test_size=test_prop,stratify=category_labels)\n","\n","  # build and run the model\n","  result = sm.Logit(y_train,X_train\n","      ).fit_regularized(maxiter=3000,disp=0,method='l1',alpha=.1)\n","\n","  # extract the results (p-value and beta)\n","  classifierResults[neuroni,0] = result.pvalues[1]\n","  classifierResults[neuroni,1] = result.params[1]\n","  classifierResults[neuroni,2] = 100*((result.predict(X_train)>.5) == y_train).mean()\n","  classifierResults[neuroni,3] = 100*((result.predict(X_test)>.5) == y_test).mean()"],"metadata":{"id":"7NiErlVG8Z3-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualization of model significance and sign\n","\n","# setup the figure\n","fig = plt.figure(figsize=(11,7))\n","gs = GridSpec(2,3,figure=fig)\n","\n","ax0 = fig.add_subplot(gs[0,:])\n","ax1 = fig.add_subplot(gs[1,0])\n","ax2 = fig.add_subplot(gs[1,1])\n","ax3 = fig.add_subplot(gs[1,2])\n","\n","# find the negative and positive betas, and the supra-threshold results\n","negBetas = classifierResults[:,1]<0\n","posBetas = classifierResults[:,1]>0\n","pvalThresh = .05/nneurons # p<.05, Bonferroni-corrected\n","sigBetas = classifierResults[:,0] < pvalThresh\n","\n","\n","# positive significant betas\n","idx2plot = posBetas & sigBetas\n","ax0.plot(np.where(idx2plot)[0],classifierResults[idx2plot,1],'ro',markerfacecolor=[.7,.7,.7],label='Positive and sig.')\n","ax1.plot(classifierResults[idx2plot,1],-np.log10(classifierResults[idx2plot,0]),'ro',markerfacecolor=[.7,.7,.7,.5])\n","ax2.plot(classifierResults[idx2plot,2],-np.log10(classifierResults[idx2plot,0]),'ro',markerfacecolor=[.7,.7,.7,.5])\n","ax3.plot(classifierResults[idx2plot,3],-np.log10(classifierResults[idx2plot,0]),'ro',markerfacecolor=[.7,.7,.7,.5])\n","\n","# positive non-significant betas\n","idx2plot = posBetas & ~sigBetas\n","ax0.plot(np.where(idx2plot)[0],classifierResults[idx2plot,1],'rx',markersize=3,label='Positive and non-sig.')\n","ax1.plot(classifierResults[idx2plot,1],-np.log10(classifierResults[idx2plot,0]),'rx',markersize=3)\n","ax2.plot(classifierResults[idx2plot,2],-np.log10(classifierResults[idx2plot,0]),'rx',markersize=3)\n","ax3.plot(classifierResults[idx2plot,3],-np.log10(classifierResults[idx2plot,0]),'rx',markersize=3)\n","\n","# negative significant betas\n","idx2plot = negBetas & sigBetas\n","ax0.plot(np.where(idx2plot)[0],classifierResults[idx2plot,1],'go',markerfacecolor=[.7,.7,.7],label='Negative and sig.')\n","ax1.plot(classifierResults[idx2plot,1],-np.log10(classifierResults[idx2plot,0]),'go',markerfacecolor=[.7,.7,.7,.5])\n","ax2.plot(classifierResults[idx2plot,2],-np.log10(classifierResults[idx2plot,0]),'go',markerfacecolor=[.7,.7,.7,.5])\n","ax3.plot(classifierResults[idx2plot,3],-np.log10(classifierResults[idx2plot,0]),'go',markerfacecolor=[.7,.7,.7,.5])\n","\n","# negative non-significant betas\n","idx2plot = negBetas & ~sigBetas\n","ax0.plot(np.where(idx2plot)[0],classifierResults[idx2plot,1],'gx',markersize=3,label='Negative and non-sig.')\n","ax1.plot(classifierResults[idx2plot,1],-np.log10(classifierResults[idx2plot,0]),'gx',markersize=3)\n","ax2.plot(classifierResults[idx2plot,2],-np.log10(classifierResults[idx2plot,0]),'gx',markersize=3)\n","ax3.plot(classifierResults[idx2plot,3],-np.log10(classifierResults[idx2plot,0]),'gx',markersize=3)\n","\n","ax0.set(ylabel='Beta coefficient',xlabel='Neuron index',xlim=[-10,nneurons+9],\n","              title='A) Statistical parameters of \"can\" classification')\n","ax0.legend(fontsize=8)\n","\n","\n","ax1.axhline(-np.log10(pvalThresh),linestyle='--',color='b',label='Significance threshold')\n","ax2.axhline(-np.log10(pvalThresh),linestyle='--',color='b',label='Significance threshold')\n","ax3.axhline(-np.log10(pvalThresh),linestyle='--',color='b',label='Significance threshold')\n","ax2.axvline(50,linestyle='--',color='k',linewidth=.5,label='Chance')\n","ax3.axvline(50,linestyle='--',color='k',linewidth=.5,label='Chance')\n","ax2.legend(fontsize=8)\n","\n","ax1.set(xlabel='Beta coeff',ylabel='$-log_{10}(p)$',title='B) Betas by p-values')\n","ax2.set(xlabel='Prediction accuracy (%)',ylabel='$-log_{10}(p)$',title='C) TRAIN accuracy by p-values')\n","ax3.set(xlabel='Prediction accuracy (%)',ylabel='$-log_{10}(p)$',title='D) TEST accuracy by p-values')\n","ax1.legend(fontsize=8)\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj48_part3a.png')\n","plt.show()"],"metadata":{"id":"K3BPO_j-_MwJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["yTrain,xTrain = np.histogram(classifierResults[:,2],bins='fd',density=True)\n","yTest,xTest   = np.histogram(classifierResults[:,3],bins='fd',density=True)\n","\n","plt.figure(figsize=(8,3))\n","plt.plot(xTrain[:-1],yTrain,linewidth=2,label='TRAIN')\n","plt.plot(xTest[:-1],yTest,linewidth=2,label='TEST')\n","plt.axvline(50,linestyle='--',color='k',linewidth=.5,label='Chance')\n","\n","plt.gca().set(xlabel='Prediction accuracy (%)',ylabel='Density',title='Distribution of prediction accuracies')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj48_part3b.png')\n","plt.show()"],"metadata":{"id":"MVT1M6Qh33Zk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"v_NdfrwEOzBY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 4: Laminar profile of classification**"],"metadata":{"id":"PfH-c__nTdmy"}},{"cell_type":"code","source":["# initialize matrix to store the classifier results\n","pvalues  = np.ones((model.config.n_layer,nneurons)) # initialize to 1's to ignore in subsequent mask\n","betas    = np.zeros((model.config.n_layer,nneurons))\n","accuracy = np.zeros((model.config.n_layer,nneurons,2))\n","\n","\n","# loop over layers\n","for layeri in tqdm(range(model.config.n_layer)):\n","\n","  # loop over neurons for per-neuron analysis\n","  for neuroni in range(nneurons):\n","\n","    # isolate the final-token activations\n","    catC = can_activations[f'mlp_{layeri}'][:,-1,neuroni]\n","    catT = cant_activations[f'mlp_{layeri}'][:,-1,neuroni]\n","    X = sm.add_constant(np.hstack((catC,catT)))\n","\n","    # split the data\n","    X_train,X_test, y_train,y_test = train_test_split(X,category_labels,test_size=test_prop,stratify=category_labels)\n","\n","    # build and run the model\n","    result = sm.Logit(y_train,X_train).fit_regularized(maxiter=3000,disp=0,method='l1',alpha=.1)\n","\n","    # extract the results (p-value, beta, and accuracy)\n","    pvalues[layeri,neuroni]    = result.pvalues[1]\n","    betas[layeri,neuroni]      = result.params[1]\n","    accuracy[layeri,neuroni,0] = 100*((result.predict(X_train)>.5) == y_train).mean()\n","    accuracy[layeri,neuroni,1] = 100*((result.predict(X_test)>.5) == y_test).mean()"],"metadata":{"id":"HVLbGTqNTdkE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create two masks\n","pvalue_mask = pvalues<.05/nneurons\n","posbet_mask = betas>0\n","negbet_mask = betas<0\n","\n","# get accuracy only from masked neurons\n","masked_accuracyPosB_train = accuracy[:,:,0].copy()\n","masked_accuracyPosB_train[~(posbet_mask & pvalue_mask)] = np.nan\n","\n","masked_accuracyNegB_train = accuracy[:,:,0] + 0\n","masked_accuracyNegB_train[~(negbet_mask & pvalue_mask)] = np.nan\n","\n","masked_accuracyPosB_test = accuracy[:,:,1].copy()\n","masked_accuracyPosB_test[~(posbet_mask & pvalue_mask)] = np.nan\n","\n","masked_accuracyNegB_test = accuracy[:,:,1] + 0\n","masked_accuracyNegB_test[~(negbet_mask & pvalue_mask)] = np.nan\n","\n","\n","\n","# make the plot\n","_,axs = plt.subplots(1,2,figsize=(12,3.5))\n","\n","axs[0].plot(100*np.mean(pvalue_mask,axis=1),'kH',markerfacecolor=[.9,.7,.9],markersize=12)\n","axs[0].set(xlabel='Layer',ylabel='Percent significant neurons (%)',title='A) Laminar profile of significance')\n","\n","axs[1].plot(np.nanmean(masked_accuracyPosB_train,axis=1),'gs',markerfacecolor=[.7,.9,.7],markersize=10,label='TRAIN $\\\\beta$s>0')\n","axs[1].plot(np.nanmean(masked_accuracyNegB_train,axis=1),'b^',markerfacecolor=[.7,.7,.9],markersize=10,label='TRAIN $\\\\beta$s<0')\n","\n","axs[1].plot(np.nanmean(masked_accuracyPosB_test,axis=1),'gs-',markerfacecolor=[.7,.9,.7],markersize=5,zorder=-10,label='TEST $\\\\beta$s>0')\n","axs[1].plot(np.nanmean(masked_accuracyNegB_test,axis=1),'b^-',markerfacecolor=[.7,.7,.9],markersize=5,zorder=-10,label='TEST $\\\\beta$s<0')\n","\n","axs[1].legend()\n","axs[1].set(xlabel='Layer',ylabel='Prediction accuracy (%)',title='B) Average prediction accuracies in significant neurons')\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj48_part4.png')\n","plt.show()"],"metadata":{"id":"lE3bcYh34zW_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"itwyjxKL-dDN"},"execution_count":null,"outputs":[]}]}