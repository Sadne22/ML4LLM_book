{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[50] Recommender systems with MLP projections</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"fXOrNMhxq2Ya"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BvQj17hzqwM"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer"]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300\n","})"],"metadata":{"id":"BvREVw_VesPV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CnBu2EbamHAk"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# **Part 1: Tokens and MLP projections**"],"metadata":{"id":"CqifcaigyOOF"}},{"cell_type":"code","source":["model = AutoModelForCausalLM.from_pretrained('gpt2')\n","tokenizer = AutoTokenizer.from_pretrained('gpt2')\n","model.eval()\n","\n","n_layers = model.config.n_layer"],"metadata":{"id":"saafwV3q9ocH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_A = \"I'll tell thee everything I can; There's little to relate. I saw an aged aged man, a-sitting on a gate.\"\n","text_B = '\"Who are you, aged man?\" I said. \"And how is it you live?\"'\n","text_C = 'And his answer trickled through my head, like water through a sieve.'\n","\n","tokens_A = tokenizer.encode()\n","tokens_B = tokenizer\n","tokens_C =\n","\n","n_toks_A =\n","n_toks_B =\n","n_toks_C =\n","\n","print(' Text | tokens')\n","print('------+-------')\n","print(f'  A   |  {}')\n","print(f'  B   |  {}')\n","print(f'  C   |  {}')"],"metadata":{"id":"FD6NVUwxnw-9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mlp_projs = {}\n","\n","def outerhook(layernum):\n","  def hook(module,input,output):\n","    mlp_projs[f'{whichtext}_{layernum}'] = output.detach().numpy()\n","  return hook\n","\n","handles = []\n","for layi in range(n_layers):\n","  h = model.transformer.h[layi].mlp.\n","  handles.append(h)"],"metadata":{"id":"3XAVEeglnw7c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","  whichtext =\n","  model(tokens_\n","\n","  whichtext = 'B'\n","  model\n","\n","  whichtext ="],"metadata":{"id":"kLBtoiCDnw4Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for k,v in mlp_projs.items():\n","  print(f\"mlp['{k}'] has size {list(v.shape)}\")"],"metadata":{"id":"4ZWw_Idxnw1c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize layers-by-tokens matrices\n","norm_A = np.zeros((,))\n","norm_B = np.zeros((,))\n","norm_C = np.zeros((,))\n","\n","# calculate the norms for all tokens in all layers\n","for layeri in range(n_layers):\n","  norm_A[layeri,:] = np.linalg.norm(,axis=)\n","  norm_B[layeri,:] = np.linalg.norm(\n","  norm_C[layeri,:] = np.linalg.norm(\n","\n","\n","fig,axs = plt.subplots(1,3,figsize=(12,3.3))\n","\n","# text A\n","cmin,cmax = np.percentile(,[,])\n","h = axs[0].imshow(norm_A,aspect='auto',cmap='magma',origin='lower')\n","axs[0].set(xlabel='Token position',ylabel='Transformer layer',title='Norms of sentence A')\n","fig.colorbar(h,ax=axs[0],pad=.01)\n","\n","# text B\n","cmin,cmax =\n","h = axs[1].imshow()\n","axs[1].set(xlabel='Token position',ylabel='Transformer layer',title='Norms of sentence B')\n","fig.colorbar(h,ax=axs[1],pad=.01)\n","\n","# text C\n","cmin,cmax = np.percentile(norm_C,[5,95])\n","h = axs[2].\n","axs[2].set(xlabel='Token position',ylabel='Transformer layer',title='Norms of sentence C')\n","fig.colorbar(h,ax=axs[2],pad=.01)\n","\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj50_part1.png')\n","plt.show()"],"metadata":{"id":"clQ4-SbLs0lQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qPV8EjEPr4B5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: Token-average target vector recommender**"],"metadata":{"id":"rnsjRDgPr3_G"}},{"cell_type":"code","source":["whichlayer = 6\n","\n","# average the vectors within each text\n","mean_A = np.mean(\n","mean_B = np.mean(\n","\n","# cosine similarity between the mean vectors\n","cs_AB = cosine_similarity(,)\n","\n","# the target vector\n","target_vect ="],"metadata":{"id":"JMrf5xPir9hD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(10,3.5))\n","\n","axs[0].plot(,label='Vector A')\n","axs[0].plot(,label='Vector B')\n","axs[1].plot(,,'kh',markerfacecolor=[.7,.9,.9,.5])\n","\n","axs[0].set(xlabel='Embeddings dimension',ylabel='Value',title='A) Mean vectors')\n","axs[0].legend()\n","\n","axs[1].set(xlabel='Mean vector A',ylabel='Mean vector B',title=f'B) Scatter plot ($S_c$ = {cs_AB.item():.2f})')\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj50_part2a.png')\n","plt.show()"],"metadata":{"id":"aeMOzqHbr38w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# cosine similarity between each token in C and the target\n","cossims = cosine_similarity(,)\n","\n","# visualize and choose\n","plt.figure(figsize=(12,3))\n","\n","plt.plot(cossims,'kh',markerfacecolor=[.9,.7,.9],markersize=14)\n","plt.axhline(,linestyle='--',linewidth=.5,color='k',zorder=-1)\n","plt.axvline(,linestyle='--',linewidth=.5,color='k',zorder=-1)\n","\n","plt.gca().set(xticks=range(n_toks_C),xticklabels=[tokenizer.decode(i) for i in tokens_C[0,1:]],\n","              ylabel='Similarity with target vector')\n","plt.xticks(rotation=45)\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj50_part2b.png')\n","plt.show()"],"metadata":{"id":"9PvPHQ6or36C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"t3bv7kqWr30q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: Recommendations through the layers**"],"metadata":{"id":"QFwGhocwE_PQ"}},{"cell_type":"code","source":["plt.figure(figsize=(10,3))\n","xlabels = []\n","\n","\n","for layeri in range(n_layers):\n","\n","  mean_A = np.mean(\n","  mean_B = np.mean(\n","\n","  target_vect =\n","  cossims = cosine_similarity\n","\n","  plt.plot(layeri,,'kh',markersize=14,markerfacecolor=[.9,.7,.7])\n","  plt.text(,,,ha='center',fontweight='bold')\n","\n","\n","plt.gca().set(xlabel='Transformer layer',ylabel='Cosine similarity',title='Recommended token')\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj50_part3.png')\n","plt.show()"],"metadata":{"id":"ckyHRgGBE_Mq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"v-tXlPPfE_KL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 4: Recommending specific pairs**"],"metadata":{"id":"ARBnv6eJE_Hn"}},{"cell_type":"code","source":["whichlayer = 6\n","\n","x_A = mlp_projs[f'A_][,,]\n","x_B =\n","x_C =\n","\n","cMat = np.zeros((,,))\n","\n","for i in range():\n","  for j in range():\n","\n","    # create the directional extrapolation (aka analogy) vector\n","    targetvect =\n","\n","    # calculate cosine similarity with each C vector\n","    for k in range(n_toks_C):\n","\n","      cMat[i,j,k] = cosine_similarity(\n","\n","\n","print(f'Matrix cMat has size {cMat.shape}, which is {np.prod(cMat.shape):,} elements!')"],"metadata":{"id":"f0yLXuEQH2bz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["token_idxs = np.linspace(0,n_toks_C-1,4,dtype=int)\n","\n","fig,axs = plt.subplots(1,len(token_idxs),figsize=(14,5))\n","\n","for tidx in range(len(token_idxs)):\n","\n","  # find the best match\n","  idx = np.argmax(\n","  i,j = np.unravel_index(idx,cMat[:,:,token_idxs[tidx]].shape)\n","\n","  bestpair = f'(\"{}\"  \"{}\")'\n","\n","  # subplot title\n","  title = f'Similarity with \"{tokenizer.decode(tokens_C[,])}\"\\n{bestpair}'\n","\n","\n","  # show the full matrix\n","  axs[tidx].imshow(cMat[,vmin=.05,vmax=.3,aspect='auto',cmap='magma')\n","\n","  # adjust the axis and image features\n","  axs[tidx].set(title=title,\n","                xticks=range(n_toks_B),xticklabels=,\n","                yticks=range(n_toks_A),yticklabels=)\n","  axs[tidx].tick_params(axis='x',labelrotation=90)\n","\n","\n","axs[0].set(xlabel='Tokens from Text B',ylabel='Tokens from Text A')\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj50_part4.png')\n","plt.show()"],"metadata":{"id":"Emwyi5IdH2Yw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_Y99UNfmr3yH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 5: Layer-specific recommendations**"],"metadata":{"id":"Tw-6OEZvnwyi"}},{"cell_type":"code","source":["max_similarities = np.zeros((,))\n","\n","for layeri in range(n_layers):\n","\n","  print(f'\\nLayer {layeri}:')\n","\n","  x_A = mlp_projs\n","  x_B = mlp_projs\n","  x_C = mlp_projs\n","\n","  cMat = np.zeros((n_toks_A,n_toks_B,n_toks_C))\n","\n","  for i in range(n_toks_A):\n","    for j in range(n_toks_B):\n","\n","      # create the target vector\n","      targetvect =\n","\n","      # calculate cosine similarity with each C vector\n","      for k in range(n_toks_C):\n","        cMat[i,j,k] =\n","\n","\n","  # populated, now find max for each C token\n","  for k in range(n_toks_C):\n","    idx = np.argmax(\n","    i,j = np.unravel_index(idx,cMat[:,:,k].shape)\n","    max_similarities[layeri,k] = np.max(\n","\n","    bestpair = f'({tokenizer.decode(tokens_A[0,i+1])},{tokenizer.decode(tokens_B[0,j+1])})'\n","    print(f'  \"{}\" is paired with {}')"],"metadata":{"id":"2DZomMuSPNm3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,4))\n","\n","# color limits using data percentiles\n","cmin,cmax =\n","\n","# show the heatmap\n","plt.imshow()\n","\n","# and make it look nicer\n","plt.gca().set(ylabel='Transformer layer',title='Max similarities to target vector',\n","              xticks=range(n_toks_C),xticklabels=[tokenizer.decode(i) for i in tokens_C[0,1:]])\n","plt.colorbar(pad=.01)\n","plt.xticks(rotation=45)\n","\n","plt.tight_layout()\n","plt.savefig('ch7_proj50_part5.png')\n","plt.show()"],"metadata":{"id":"0ClhU2x2R6Em"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EHxic-ZhPNqX"},"execution_count":null,"outputs":[]}]}