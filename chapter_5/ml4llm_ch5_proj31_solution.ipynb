{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[31] Logit lens</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","source":["# Reference:\n","# https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens"],"metadata":{"id":"B2DURtllkH9B"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OxmEbIoa-yv0"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.linear_model import LinearRegression\n","\n","import torch\n","import torch.nn.functional as F\n","\n","from transformers import RobertaTokenizer, RobertaForMaskedLM"]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300\n","})"],"metadata":{"id":"dy4A-ah8kzZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0JaacnaV4Bu6"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# **Part 1: Single-layer, unmasked lens**"],"metadata":{"id":"ny-nH3gniySB"}},{"cell_type":"code","source":["# import roberta model and tokenizer\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n","model = RobertaForMaskedLM.from_pretrained('roberta-large')\n","\n","model.eval()"],"metadata":{"id":"pFyxDs3UaDiw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_hidden = model.config.num_hidden_layers + 1 # +1 b/c 1st hidden_states is embeddings\n","n_hidden"],"metadata":{"id":"ze6VI74PgZ6u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = 'The way you do anything is the way you do everything'\n","tokens = tokenizer.encode(text,return_tensors='pt')\n","numTokens = len(tokens[0])\n","\n","for t in tokens[0]:\n","  print(f'Token {t:5} is \"{tokenizer.decode(t)}\"')\n","\n","with torch.no_grad():\n","  output = model(tokens,output_hidden_states=True)"],"metadata":{"id":"cPawkvwDWD9M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(output.hidden_states), output.hidden_states[3].shape"],"metadata":{"id":"P5uSuUfBWD6m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["layer = n_hidden - 2\n","\n","# extract the activations from one layer\n","activations = output.hidden_states[layer][0]\n","\n","# calculate the raw logits\n","logits = model.lm_head(activations)\n","\n","# check the shape\n","logits.shape"],"metadata":{"id":"R7KjYbIkcoYD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# softmax and plot\n","lsm_outs = F.log_softmax(logits,dim=-1).detach().numpy()\n","\n","# max-softmax as prediction\n","predictedToken = np.argmax(lsm_outs[3,:])\n","print(f'   Actual token is \"{tokenizer.decode(tokens[0,3])}\"')\n","print(f'Predicted token is \"{tokenizer.decode(predictedToken)}\"')\n","print(f'Predicted log-sm is {lsm_outs[3,predictedToken]:.5f}')\n","\n","# show softmax for one token\n","plt.figure(figsize=(10,3))\n","plt.plot(predictedToken,lsm_outs[3,predictedToken],'ro',markersize=8)\n","plt.plot(lsm_outs[3,:],'k.',alpha=.3)\n","plt.gca().set(xlabel='Token index',ylabel='Log-softmax prob',xlim=[-30,tokenizer.vocab_size+30],\n","              title=f'Log-softmax logits for the token \"{tokenizer.decode(tokens[0,3])}\"')\n","\n","plt.tight_layout()\n","plt.savefig('ch5_proj31_part1.png')\n","plt.show()"],"metadata":{"id":"s4YGy1I6WD36"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lQ8aMkqZZGPX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: Single-layer masked lens**"],"metadata":{"id":"DIgbQOy4ZGMU"}},{"cell_type":"code","source":["text = f'The way you do anything is the {tokenizer.mask_token} you do everything'\n","tokens_wMask = tokenizer.encode(text,return_tensors='pt')\n","\n","mask_idx = tokenizer.encode(tokenizer.mask_token,add_special_tokens=False)[0]\n","mask_pos_idx = torch.where(tokens_wMask[0]==mask_idx)[0].item()\n","\n","for t in tokens_wMask[0]:\n","  print(f'Token {t:5} is \"{tokenizer.decode(t)}\"')\n","\n","with torch.no_grad():\n","  output_wMask = model(tokens_wMask,output_hidden_states=True)"],"metadata":{"id":"EeamY-aebRu-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get logits from one layer\n","activations = output_wMask.hidden_states[layer][0]\n","logits = model.lm_head(activations)\n","lsm_outs_wMask = F.log_softmax(logits,dim=-1).detach().numpy()"],"metadata":{"id":"gPe9y9ozbRu_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# max-softmax is the next prediction\n","predictedToken = np.argmax(lsm_outs_wMask[mask_pos_idx,:])\n","print(f' Unmasked token is \"{tokenizer.decode(tokens[0,mask_pos_idx])}\"')\n","print(f'   Masked token is \"{tokenizer.decode(tokens_wMask[0,mask_pos_idx])}\"')\n","print(f'Predicted token is \"{tokenizer.decode(predictedToken)}\"')\n","print(f'Predicted log-sm is {lsm_outs_wMask[mask_pos_idx,predictedToken]:.5f}')\n","\n","# show softmax for one token\n","plt.figure(figsize=(10,3))\n","plt.plot(predictedToken,lsm_outs_wMask[mask_pos_idx,predictedToken],'ro',markersize=8)\n","plt.plot(lsm_outs_wMask[mask_pos_idx,:],'k.',alpha=.3)\n","plt.gca().set(xlabel='Token index',ylabel='Log-softmax prob',xlim=[-30,tokenizer.vocab_size+30],\n","              title=f'Log-softmax logits for the token \"{tokenizer.decode(tokens_wMask[0,mask_pos_idx])}\"')\n","\n","plt.tight_layout()\n","plt.savefig('ch5_proj31_part2.png')\n","plt.show()"],"metadata":{"id":"41qfdz92bRu_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# FYI: log-prob to percent-prob\n","100*np.exp(-0.00003)"],"metadata":{"id":"1f6exyafZSOO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ThJdlF45frdF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: Laminar lens for one masked token**"],"metadata":{"id":"1YIMH21lfrWL"}},{"cell_type":"code","source":["lsm_target = np.zeros(n_hidden)\n","\n","for layeri in range(n_hidden):\n","\n","  # get logits from one layer\n","  activations = output_wMask.hidden_states[layeri][0]\n","  logits = model.lm_head(activations)\n","  lsm_outs_wMask = F.log_softmax(logits,dim=-1).detach().numpy()\n","\n","  # predicted token\n","  predictedToken = np.argmax(lsm_outs_wMask[mask_pos_idx,:])\n","  lsm_target[layeri] = lsm_outs_wMask[mask_pos_idx,tokens[0][mask_pos_idx]]\n","\n","  # build up the text\n","  txt = tokenizer.decode(tokens_wMask[0,:mask_pos_idx])\n","  txt += f'\"{tokenizer.decode(predictedToken)}\"'\n","  txt += tokenizer.decode(tokens_wMask[0,mask_pos_idx+1:])\n","\n","  # and print it\n","  print(f'Layer {layeri:2} lens: {txt}')"],"metadata":{"id":"GrVoFdjifrS4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the predictor (note the reshape: LinearRegression requires a multidimensional array\n","sqrtLayerIdx = np.sqrt(np.arange(n_hidden).reshape(-1,1))\n","\n","# fit the model and print the results\n","reg = LinearRegression().fit(sqrtLayerIdx,lsm_target)\n","print(f'const: {reg.intercept_:6.2f}')\n","print(f'slope: {reg.coef_[0]:6.2f}')\n","\n","# predicted data\n","yHat = reg.intercept_ + sqrtLayerIdx*reg.coef_\n","\n","# fitted equation\n","fiteq = fr'$\\hat{{y}} = {reg.intercept_:.3f} + {reg.coef_[0]:.3f}\\sqrt{{k}}$'\n","\n","# plot observed and predicted\n","plt.figure(figsize=(8,5))\n","plt.plot(yHat,'r',label=fiteq)\n","plt.legend()\n","plt.plot(lsm_target,'ko',markersize=10,markerfacecolor=[.7,.9,.7])\n","plt.gca().set(xlabel='Hidden layer',ylabel='Log-softmax prob',title=f'Logits from layer {layer}')\n","\n","plt.tight_layout()\n","plt.savefig('ch5_proj31_part3.png')\n","plt.show()"],"metadata":{"id":"QgvdXBnAkKOk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"H0G4YD_wfrQA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 4: Sliding masked sequences**"],"metadata":{"id":"GtmA8lMMiYdB"}},{"cell_type":"code","source":["# loop over tokens, replace with [MASK], and get logits\n","for idx,tok in enumerate(tokens[0]):\n","\n","  # make a copy and replace a token with mask\n","  masked_tokens = tokens.clone()\n","  masked_tokens[0,idx] = mask_idx\n","\n","  # confirmation:\n","  print(masked_tokens[0].tolist())\n","  print(tokenizer.decode(masked_tokens[0]),'\\n')"],"metadata":{"id":"Fh_LyMTFiYZB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SUhJtarkiYWG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 5: The logit lens**"],"metadata":{"id":"e4Bz1uNJf6bN"}},{"cell_type":"code","source":["predictedTokens = np.zeros((n_hidden,len(tokens[0])),dtype=int)\n","lsm_target = np.zeros((n_hidden,len(tokens[0])))\n","lsm_max = np.zeros((n_hidden,len(tokens[0])))\n","slopes = np.zeros((2,len(tokens[0])))\n","\n","# loop over tokens, replace with [MASK], and get logits\n","for midx,tok in enumerate(tokens[0]):\n","\n","  # make a copy and replace a token with mask\n","  masked_tokens = tokens.clone()\n","  masked_tokens[0,midx] = mask_idx\n","\n","  # forward pass with masked tokens\n","  with torch.no_grad():\n","    output_wMask = model(masked_tokens,output_hidden_states=True)\n","\n","  ### loop over layers\n","  for layeri in range(n_hidden):\n","\n","    # get logits from one layer\n","    activations = output_wMask.hidden_states[layeri][0]\n","    logits = model.lm_head(activations)\n","    lsm_outs_wMask = F.log_softmax(logits,dim=-1).detach().numpy()\n","\n","    # predicted token\n","    predictedTokens[layeri,midx] = np.argmax(lsm_outs_wMask[midx,:])\n","    lsm_target[layeri,midx] = lsm_outs_wMask[midx,tokens[0][midx]]\n","    lsm_max[layeri,midx] = lsm_outs_wMask[midx,predictedTokens[layeri,midx]]\n","\n","\n","  # regression slope for the target token\n","  reg = LinearRegression().fit(sqrtLayerIdx,lsm_target[:,midx])\n","  slopes[0,midx] = reg.coef_[0]\n","\n","  # and again for the max-token\n","  reg = LinearRegression().fit(sqrtLayerIdx,lsm_max[:,midx])\n","  slopes[1,midx] = reg.coef_[0]"],"metadata":{"id":"Qo9rjrsqf6PB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,axs = plt.subplots(1,3,figsize=(12,4))\n","\n","h = axs[0].imshow(lsm_target,vmin=-20,vmax=0,origin='lower',cmap='magma',aspect='auto')\n","fig.colorbar(h,ax=axs[0],pad=.02)\n","axs[0].set(ylabel='Layer',title='A) Target token log-sm prob',xticks=range(len(tokens[0])),\n","           xticklabels=[tokenizer.decode(t) for t in tokens[0]])\n","axs[0].tick_params(axis='x',labelrotation=90)\n","\n","h = axs[1].imshow(lsm_max,vmin=-5,vmax=0,origin='lower',cmap='magma',aspect='auto')\n","fig.colorbar(h,ax=axs[1],pad=.02)\n","axs[1].set(ylabel='Layer',title='B) Max token log-sm prob',xticks=range(len(tokens[0])),\n","           xticklabels=[tokenizer.decode(t) for t in tokens[0]])\n","axs[1].tick_params(axis='x',labelrotation=90)\n","\n","axs[2].plot(slopes[0,:],'ko',markersize=10,markerfacecolor=[.7,.7,.9],label='Target')\n","axs[2].plot(slopes[1,:],'ks',markersize=10,markerfacecolor=[.9,.7,.7],label='Max token')\n","h = axs[2].legend()\n","h.get_frame().set_facecolor([.9,.9,.9]) # gray legend to avoid marker-confusion\n","\n","axs[2].axhline(0,linestyle='--',color='gray',linewidth=.6,zorder=-30)\n","axs[2].set(ylabel='Slope ($\\\\beta_1$)',xticks=range(len(tokens[0])),\n","           xticklabels=[tokenizer.decode(t) for t in tokens[0]],\n","           title='C) Regression slopes')\n","axs[2].tick_params(axis='x',labelrotation=90)\n","\n","plt.tight_layout()\n","plt.savefig('ch5_proj31_part5.png')\n","plt.show()"],"metadata":{"id":"YuJX9ncZf6LL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KrgoPwsof6FT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 6: Logit Lens text heatmap**"],"metadata":{"id":"hhnmDbihhUtk"}},{"cell_type":"code","source":["lsm_max_scaled = (lsm_max-lsm_max.min()) / (lsm_max.max()-lsm_max.min())"],"metadata":{"id":"jVKtmmbzdz0e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,ax = plt.subplots(1,figsize=(13,11))\n","\n","# original text (separated into a list of decoded tokens)\n","target = [tokenizer.decode(t) for t in tokens[0]]\n","numTokens = len(target)\n","\n","# loop over layers\n","for layeri in range(n_hidden):\n","\n","  # y-axis coordinate for this layer\n","  yCoord = 1-layeri/n_hidden\n","\n","  # print the layer number in the left margin\n","  ax.text(-.1,yCoord,f'Layer {layeri}:',ha='right')\n","\n","  # loop over the predicted tokens in this layer\n","  for xi,tok in enumerate(predictedTokens[layeri]):\n","    ax.text(xi/numTokens,yCoord,tokenizer.decode(tok),ha='center',\n","            bbox=dict(boxstyle='round,pad=0.3', facecolor=plt.cm.Reds(lsm_max_scaled[layeri,xi]), edgecolor='none',alpha=.5))\n","\n","ax.axis('off')\n","\n","# finally, draw the target tokens at the bottom\n","ax.text(-.1,yCoord-.05,f'Target:',ha='right',fontweight='bold')\n","for xi,tok in enumerate(target):\n","  ax.text(xi/numTokens,yCoord-.05,tok,ha='center',fontsize=12,fontweight='bold')\n","\n","plt.tight_layout()\n","plt.savefig('ch5_proj31_part6.png')\n","plt.show()"],"metadata":{"id":"oaH9ZTWKhUqp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SdBzF78cZGGA"},"execution_count":null,"outputs":[]}]}