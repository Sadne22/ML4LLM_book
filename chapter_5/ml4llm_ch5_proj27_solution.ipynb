{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "gpuType": "L4",
   "authorship_tag": "ABX9TyPiFBLtVc6ZvbHesH7wT0WZ"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n",
    "|-|:-:|\n",
    "|<h2>Project:</h2>|<h1><b>[27] Impact of layer-specific noise and scaling</b></h1>|\n",
    "|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>Using the code without reading the book may lead to confusion or errors.</i>"
   ],
   "metadata": {
    "id": "py_eibYAH3Q-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OxmEbIoa-yv0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "R5SI-Iyy4dtm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "### matplotlib adjustments (commented lines are for dark mode)\n",
    "\n",
    "# svg plots (higher-res)\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
    "\n",
    "plt.rcParams.update({\n",
    "    # 'figure.facecolor': '#282a2c',\n",
    "    # 'figure.edgecolor': '#282a2c',\n",
    "    # 'axes.facecolor':   '#282a2c',\n",
    "    # 'axes.edgecolor':   '#DDE2F4',\n",
    "    # 'axes.labelcolor':  '#DDE2F4',\n",
    "    # 'xtick.color':      '#DDE2F4',\n",
    "    # 'ytick.color':      '#DDE2F4',\n",
    "    # 'text.color':       '#DDE2F4',\n",
    "    'axes.spines.right': False,\n",
    "    'axes.spines.top':   False,\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.labelweight': 'bold',\n",
    "    'savefig.dpi':300\n",
    "})"
   ],
   "metadata": {
    "id": "dy4A-ah8kzZQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Z5tNq57P0eBn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Part 1: Model, tokens, and clean activations**"
   ],
   "metadata": {
    "id": "6c5Ox7bF8QfB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# load in GPT2-large and its tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2-large',output_hidden_states=True)\n",
    "model.eval()"
   ],
   "metadata": {
    "id": "mkIgNVHF0eR6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# move to the gpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device);"
   ],
   "metadata": {
    "id": "hgAfdi936S12"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(f'This LLM has {model.config.n_layer} transformer layers.')"
   ],
   "metadata": {
    "id": "fJfyguGUoObZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "text = 'Pay no attention to that man behind the'\n",
    "tokens = tokenizer.encode(text,return_tensors='pt')\n",
    "target_token = tokenizer.encode(' curtain')[0]\n",
    "\n",
    "for t in tokens[0]:\n",
    "  print(f'Token {t:>5} is \"{tokenizer.decode(t)}\"')"
   ],
   "metadata": {
    "id": "0i2FPAc10ePQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with torch.no_grad(): # ~8s on CPU, <1s on GPU\n",
    "  outputs_clean = model(tokens.to(device))\n",
    "\n",
    "outputs_clean.hidden_states[0].shape"
   ],
   "metadata": {
    "id": "r4A3Q27t12aM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "logsm = outputs_clean.logits[0,-1,:].log_softmax(dim=-1)\n",
    "log_sm_target_clean = logsm[target_token].item()\n",
    "\n",
    "nextword_clean = torch.argmax(logsm)\n",
    "print(f'Next word is \"{tokenizer.decode(nextword_clean)}\" (token index {nextword_clean}) with {np.exp(log_sm_target_clean):.2%} probability.')"
   ],
   "metadata": {
    "id": "7HKZ8kxI12Vy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "\n",
    "plt.axhline(log_sm_target_clean,color='g',linestyle='--',linewidth=.5)\n",
    "plt.axvline(target_token,color='g',linestyle='--',linewidth=.5)\n",
    "\n",
    "plt.plot(logsm.cpu(),'kh',markerfacecolor=[.7,.9,.7,.3],markersize=4)\n",
    "plt.gca().set(xlabel='Token index',ylabel='log-softmax prob',\n",
    "              title='log-softmax of final token',xlim=[-150,tokenizer.vocab_size+150])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ch5_proj27_part1.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "RG6AHWC5BrI4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "YJI8pgt512Rg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Part 2: Hook to inject noise**"
   ],
   "metadata": {
    "id": "QzahTS938fuu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# hooking functions\n",
    "def implant_noise_hook(layer_number):\n",
    "  def noise_hook(module,input,output):\n",
    "\n",
    "    # only change one layer\n",
    "    if layer_number == layer2noise:\n",
    "\n",
    "      # unpack tuple\n",
    "      hidden, *rest = output\n",
    "\n",
    "      # generate a matrix of noise\n",
    "      h_std = hidden.std() / 2\n",
    "      noise = torch.randn_like(hidden)*h_std\n",
    "\n",
    "      # add that noise to the hidden states\n",
    "      hidden += noise\n",
    "      # print(f'Changed layer {layer_number} with noise')\n",
    "\n",
    "      # reconstruct output\n",
    "      output = tuple([hidden]+rest)\n",
    "\n",
    "    return output\n",
    "  return noise_hook\n",
    "\n",
    "\n",
    "# loop over layers and do surgery\n",
    "handles = []\n",
    "for layeri in range(model.config.n_layer):\n",
    "  baselayer = model.transformer.h[layeri]\n",
    "  h = baselayer.register_forward_hook(implant_noise_hook(layeri))\n",
    "  handles.append(h)"
   ],
   "metadata": {
    "id": "Yq7invM00eMh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# test with one layer\n",
    "layer2noise = 15\n",
    "\n",
    "with torch.no_grad():\n",
    "  outputs_noise = model(tokens.to(device))\n",
    "\n",
    "outputs_noise.hidden_states[0].shape"
   ],
   "metadata": {
    "id": "aTIRXQu80eHB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# initialize\n",
    "diffnorms = torch.zeros(model.config.n_layer)\n",
    "\n",
    "# loop over layers\n",
    "for layeri in range(model.config.n_layer):\n",
    "\n",
    "  # extract hidden states for this layer\n",
    "  hs_c = outputs_clean.hidden_states[layeri].cpu()\n",
    "  hs_n = outputs_noise.hidden_states[layeri].cpu()\n",
    "\n",
    "  # norm of difference matrix\n",
    "  diffnorms[layeri] = torch.norm(hs_c-hs_n)\n",
    "\n",
    "# and plot\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(diffnorms,'kh',markerfacecolor=[.9,.7,.7],markersize=12)\n",
    "plt.axvline(layer2noise+1,color='k',linestyle='--',zorder=-10)\n",
    "plt.axhline(0,color='k',linestyle=':',zorder=-10)\n",
    "plt.gca().set(xlabel='Layer',ylabel='Norm of difference')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ch5_proj27_part2.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "k7zjkLxx88Jc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# predicted next token and its probability\n",
    "max_logit = torch.argmax(outputs_noise.logits[0,-1,:])\n",
    "log_sm = outputs_noise.logits[0,-1,:].softmax(dim=-1)\n",
    "\n",
    "print(f' Clean model: next token is \"{tokenizer.decode(nextword_clean)}\" with {np.exp(log_sm_target_clean):.2%} probability.')\n",
    "print(f'Noised model: next token is \"{tokenizer.decode(max_logit)}\" with {log_sm[max_logit].item():.2%} probability.')"
   ],
   "metadata": {
    "id": "7_malGNQ88G_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "EDDgr__NDYo0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Part 3: Impacts of layer-specific noising**"
   ],
   "metadata": {
    "id": "sLgHNpPS88Eh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# initializations\n",
    "log_sm_targets = torch.zeros((model.config.n_layer,2))\n",
    "\n",
    "# loop over layers\n",
    "for layer2noise in range(model.config.n_layer):\n",
    "\n",
    "  # run the model\n",
    "  with torch.no_grad():\n",
    "    outputs_noise = model(tokens.to(device))\n",
    "\n",
    "  # log-softmax the final token logits\n",
    "  logsm = outputs_noise.logits[0,-1,:].log_softmax(dim=-1)\n",
    "\n",
    "  # get the target (\"curtain\") value\n",
    "  log_sm_targets[layer2noise,0] = logsm[target_token].item()\n",
    "\n",
    "  # get the max value\n",
    "  maxtok = torch.argmax(outputs_noise.logits[0,-1,:])\n",
    "  log_sm_targets[layer2noise,1] = logsm[maxtok].item()\n",
    "\n",
    "  # print the completed text\n",
    "  print(f'L{layer2noise:2}: {text}\"{tokenizer.decode(maxtok)}\"')"
   ],
   "metadata": {
    "id": "7ayL72r7-Lsd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "_,axs = plt.subplots(1,2,figsize=(12,3.5))\n",
    "\n",
    "# plot the impact of the perturbations\n",
    "axs[0].axhline(log_sm_target_clean,color='g',linestyle='--',label='Clean')\n",
    "axs[0].plot(log_sm_targets[:,0],'kh',markerfacecolor=[.9,.7,.7,.7],markersize=12,label='Noisified')\n",
    "axs[0].legend()\n",
    "axs[0].set(xlabel='Layer',ylabel='log-softmax prob',\n",
    "              title=f'A) Impact of noise on log-softmax of \"{tokenizer.decode(target_token)}\"')\n",
    "\n",
    "# and the max logit\n",
    "axs[1].axhline(log_sm_target_clean,color='g',linestyle='--',label='Clean')\n",
    "axs[1].plot(log_sm_targets[:,1],'ks',markerfacecolor=[.7,.7,.9,.7],markersize=12)\n",
    "axs[1].set(xlabel='Layer',ylabel='log-softmax prob',ylim=axs[0].get_ylim(),\n",
    "              title='B) Impact of noise on log-softmax of max logit')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ch5_proj27_part3.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "DIM5NyA0-Lpo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# remove handles\n",
    "for h in handles:\n",
    "  h.remove()"
   ],
   "metadata": {
    "id": "E7YE5pEIBU4z"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "pBRUyjjF88B8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Part 4: Layer-specific scalar dampening**"
   ],
   "metadata": {
    "id": "Bg7OfEQ20eJ-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# hooking functions\n",
    "def implant_scale_hook(layer_number):\n",
    "  def scale_hook(module, input, output):\n",
    "\n",
    "    # only change one layer\n",
    "    if layer_number == layer2scale:\n",
    "\n",
    "      # unpack tuple\n",
    "      hidden, *rest = output\n",
    "\n",
    "      # in-place method to scale down the hidden states\n",
    "      hidden.mul_(.5)\n",
    "\n",
    "      # reconstruct output\n",
    "      output = tuple([hidden]+rest)\n",
    "\n",
    "    return output\n",
    "  return scale_hook\n",
    "\n",
    "\n",
    "# loop over layers and do surgery\n",
    "handles = []\n",
    "for layeri in range(model.config.n_layer):\n",
    "  baselayer = model.transformer.h[layeri]\n",
    "  h = baselayer.register_forward_hook(implant_scale_hook(layeri))\n",
    "  handles.append(h)"
   ],
   "metadata": {
    "id": "wmxYlUoREAh3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# test with one layer\n",
    "layer2scale = 15\n",
    "\n",
    "with torch.no_grad():\n",
    "  outputs_scale = model(tokens.to(device))\n",
    "\n",
    "outputs_scale.hidden_states[0].shape"
   ],
   "metadata": {
    "id": "RZGq28bcEAh4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# initialize\n",
    "diffnorms = torch.zeros(model.config.n_layer)\n",
    "\n",
    "# loop over layers\n",
    "for layeri in range(model.config.n_layer):\n",
    "\n",
    "  # extract hidden states for this layer\n",
    "  hs_c = outputs_clean.hidden_states[layeri].cpu()\n",
    "  hs_n = outputs_scale.hidden_states[layeri].cpu()\n",
    "\n",
    "  # norm of difference matrix\n",
    "  diffnorms[layeri] = torch.norm(hs_c-hs_n)\n",
    "\n",
    "# and plot\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(diffnorms,'kh',markerfacecolor=[.9,.7,.7],markersize=12)\n",
    "plt.axvline(layer2scale+1,color='k',linestyle='--')\n",
    "plt.axhline(0,color='k',linestyle=':',zorder=-10)\n",
    "plt.gca().set(xlabel='Layer',ylabel='Norm of difference')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ch5_proj27_part4a.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "uFkTnmdQEAh4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# initializations\n",
    "log_sm_targets = torch.zeros((model.config.n_layer,2))\n",
    "\n",
    "# loop over layers\n",
    "for layer2scale in range(model.config.n_layer):\n",
    "\n",
    "  # run the model\n",
    "  with torch.no_grad():\n",
    "    outputs_scale = model(tokens.to(device))\n",
    "\n",
    "  # log-softmax the final token logits\n",
    "  logsm = outputs_scale.logits[0,-1,:].log_softmax(dim=-1)\n",
    "\n",
    "  # get the target (\"curtain\") value\n",
    "  log_sm_targets[layer2scale,0] = logsm[target_token].item()\n",
    "\n",
    "  # get the max value\n",
    "  maxtok = torch.argmax(outputs_scale.logits[0,-1,:])\n",
    "  log_sm_targets[layer2scale,1] = logsm[maxtok].item()\n",
    "\n",
    "  # print the completed text\n",
    "  print(f'L{layer2scale:2}: {text}\"{tokenizer.decode(maxtok)}\"')"
   ],
   "metadata": {
    "id": "JGvKgKTPFPPT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "_,axs = plt.subplots(1,2,figsize=(12,3.5))\n",
    "\n",
    "# plot the impact of the perturbations\n",
    "axs[0].axhline(log_sm_target_clean,color='g',linestyle='--',label='Clean')\n",
    "axs[0].plot(log_sm_targets[:,0],'kh',markerfacecolor=[.9,.7,.7,.7],markersize=12,label='Scaled')\n",
    "axs[0].legend()\n",
    "axs[0].set(xlabel='Layer',ylabel='log-softmax prob',#ylim=[-16,1],\n",
    "              title=f'Impact of down-scaling on log-softmax of \"{tokenizer.decode(target_token)}\"')\n",
    "\n",
    "# and the max logit\n",
    "axs[1].axhline(log_sm_target_clean,color='g',linestyle='--',label='Clean')\n",
    "axs[1].plot(log_sm_targets[:,1],'ks',markerfacecolor=[.7,.7,.9,.7],markersize=12)\n",
    "axs[1].set(xlabel='Layer',ylabel='log-softmax prob',ylim=axs[0].get_ylim(),\n",
    "              title='Impact of down-scaling on log-softmax of max logit')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ch5_proj27_part4b.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "VVDHEiVAFPPU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "WagH1MfBD5Eq"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}