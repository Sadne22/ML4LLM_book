{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[31] Logit lens</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","source":["# Reference:\n","# https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens"],"metadata":{"id":"B2DURtllkH9B"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OxmEbIoa-yv0"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.linear_model import LinearRegression\n","\n","import torch\n","import torch.nn.functional as F\n","\n","from transformers import RobertaTokenizer, RobertaForMaskedLM"]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300\n","})"],"metadata":{"id":"dy4A-ah8kzZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0JaacnaV4Bu6"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# **Part 1: Single-layer, unmasked lens**"],"metadata":{"id":"ny-nH3gniySB"}},{"cell_type":"code","source":["# import roberta model and tokenizer\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n","model = RobertaForMaskedLM.from_pretrained('roberta-large')\n","\n","model.eval()"],"metadata":{"id":"pFyxDs3UaDiw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_hidden =\n","n_hidden"],"metadata":{"id":"ze6VI74PgZ6u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = 'The way you do anything is the way you do everything'\n","tokens = tokenizer.encode(text,return_tensors='pt')\n","numTokens =\n","\n","for t in tokens[0]:\n","  print\n","\n","with torch.no_grad():\n","  output ="],"metadata":{"id":"cPawkvwDWD9M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(output.hidden_states), output.hidden_states[3].shape"],"metadata":{"id":"P5uSuUfBWD6m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["layer =\n","\n","# extract the activations from one layer\n","activations =\n","\n","# calculate the raw logits\n","logits =\n","\n","# check the shape\n","logits.shape"],"metadata":{"id":"R7KjYbIkcoYD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# softmax and plot\n","lsm_outs = F.log_softmax\n","\n","# max-softmax as prediction\n","predictedToken = np.argmax\n","print(f'   Actual token is \"{}\"')\n","print(f'Predicted token is \"{}\"')\n","print(f'Predicted log-sm is {}')\n","\n","# show softmax for one token\n","plt.figure(figsize=(10,3))\n","plt.plot(,'ro',markersize=8)\n","plt.plot(,'k.',alpha=.3)\n","plt.gca().set(xlabel='Token index',ylabel='Log-softmax prob',xlim=[-30,tokenizer.vocab_size+30],\n","              title=f'Log-softmax logits for the token \"{tokenizer.decode(tokens[0,3])}\"')\n","\n","plt.tight_layout()\n","plt.savefig('ch5_proj31_part1.png')\n","plt.show()"],"metadata":{"id":"s4YGy1I6WD36"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lQ8aMkqZZGPX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: Single-layer masked lens**"],"metadata":{"id":"DIgbQOy4ZGMU"}},{"cell_type":"code","source":["text = f'The way you do anything is the {tokenizer.mask_token} you do everything'\n","tokens_wMask = tokenizer.encode(text,return_tensors='pt')\n","\n","mask_idx =\n","mask_pos_idx =\n","\n","for t in tokens_wMask[0]:\n","  print(f'Token {t:5} is \"{tokenizer.decode(t)}\"')\n","\n","with torch.no_grad():\n","  output_wMask = model(tokens_wMask,output_hidden_states=True)"],"metadata":{"id":"EeamY-aebRu-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get logits from one layer\n","activations = output_wMask.hidden_states[layer][0]\n","logits =\n","lsm_outs_wMask ="],"metadata":{"id":"gPe9y9ozbRu_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# max-softmax is the next prediction\n","predictedToken = np.argmax(\n","print(f' Unmasked token is \"{}\"')\n","print(f'   Masked token is \"{}\"')\n","print(f'Predicted token is \"{}\"')\n","print(f'Predicted log-sm is {}')\n","\n","# show softmax for one token\n","plt.figure(figsize=(10,3))\n","plt.plot(,'ro',markersize=8)\n","plt.plot(,'k.',alpha=.3)\n","plt.gca().set(xlabel='Token index',ylabel='Log-softmax prob',xlim=[-30,tokenizer.vocab_size+30],)\n","\n","plt.tight_layout()\n","plt.savefig('ch5_proj31_part2.png')\n","plt.show()"],"metadata":{"id":"41qfdz92bRu_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# FYI: log-prob to percent-prob\n","100*np.exp(-0.00003)"],"metadata":{"id":"1f6exyafZSOO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ThJdlF45frdF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: Laminar lens for one masked token**"],"metadata":{"id":"1YIMH21lfrWL"}},{"cell_type":"code","source":["lsm_target = np.zeros(n_hidden)\n","\n","for layeri in range(n_hidden):\n","\n","  # get logits from one layer\n","  activations = .\n","  logits = model.\n","  lsm_outs_wMask = F.log_softmax\n","\n","  # predicted token\n","  predictedToken = np.argmax(\n","  lsm_target[layeri] = lsm_outs_wMask\n","\n","  # build up the text\n","  txt =\n","  txt +=\n","  txt +=\n","\n","  # and print it\n","  print(f'Layer {layeri:2} lens: {txt}')"],"metadata":{"id":"GrVoFdjifrS4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the predictor (note the reshape: LinearRegression requires a multidimensional array\n","sqrtLayerIdx = np.sqrt(\n","\n","# fit the model and print the results\n","reg = LinearRegression().fit(\n","print(f'const: {:6.2f}')\n","print(f'slope: {:6.2f}')\n","\n","# predicted data\n","yHat =  + *\n","\n","# fitted equation\n","fiteq = fr'$\\hat{{y}} = {reg.intercept_:.3f} + {reg.coef_[0]:.3f}\\sqrt{{L}}$'\n","\n","# plot observed and predicted\n","plt.figure(figsize=(8,5))\n","plt.plot(,'r',label=fiteq)\n","plt.legend()\n","plt.plot(,'ko',markersize=10,markerfacecolor=[.7,.9,.7])\n","plt.gca().set(xlabel='Hidden layer',ylabel='Log-softmax prob',title=f'Logits from layer {layer}')\n","\n","plt.tight_layout()\n","plt.savefig('ch5_proj31_part3.png')\n","plt.show()"],"metadata":{"id":"QgvdXBnAkKOk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"H0G4YD_wfrQA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 4: Sliding masked sequences**"],"metadata":{"id":"GtmA8lMMiYdB"}},{"cell_type":"code","source":["# loop over tokens, replace with [MASK], and get logits\n","for idx,tok in enumerate(tokens[0]):\n","\n","  # make a copy and replace a token with mask\n","  masked_tokens = tokens.clone()\n","  masked_tokens[0,idx] =\n","\n","  # confirmation:\n","  print"],"metadata":{"id":"Fh_LyMTFiYZB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SUhJtarkiYWG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 5: The logit lens**"],"metadata":{"id":"e4Bz1uNJf6bN"}},{"cell_type":"code","source":["predictedTokens = np.zeros()\n","lsm_target = np.zeros()\n","lsm_max = np.zeros()\n","slopes = np.zeros()\n","\n","# loop over tokens, replace with [MASK], and get logits\n","for midx,tok in enumerate(\n","\n","  # make a copy and replace a token with mask\n","  masked_tokens =\n","  masked_tokens[0,midx] =\n","\n","  # forward pass with masked tokens\n","  with torch.no_grad():\n","    output_wMask = model(\n","\n","  ### loop over layers\n","  for layeri in range(n_hidden):\n","\n","    # get logits from one layer\n","    activations =\n","    logits =\n","    lsm_outs_wMask =\n","\n","    # predicted token\n","    predictedTokens[layeri,midx] = np.argmax\n","    lsm_target[layeri,midx] = lsm_outs_wMask\n","    lsm_max[layeri,midx] = lsm_outs_wMask\n","\n","\n","  # regression slope for the target token\n","  reg = LinearRegression().fit(\n","  slopes[0,midx] = reg.coef_[0]\n","\n","  # and again for the max-token\n","  reg = LinearRegression().fit(\n","  slopes[1,midx] ="],"metadata":{"id":"Qo9rjrsqf6PB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,axs = plt.subplots(1,3,figsize=(12,4))\n","\n","h = axs[0].imshow()\n","fig.colorbar(h,ax=axs[0],pad=.02)\n","axs[0].set(ylabel='Layer',title='A) Target token log-sm prob',xticks=range(len(tokens[0])),\n","           xticklabels=[tokenizer.decode(t) for t in tokens[0]])\n","axs[0].tick_params(axis='x',labelrotation=90)\n","\n","h = axs[1].imshow()\n","fig.colorbar(h,ax=axs[1],pad=.02)\n","axs[1].set(ylabel='Layer',title='B) Max token log-sm prob',xticks=range(len(tokens[0])),\n","           xticklabels=[tokenizer.decode(t) for t in tokens[0]])\n","axs[1].tick_params(axis='x',labelrotation=90)\n","\n","axs[2].plot(,label='Target')\n","axs[2].plot(,label='Max token')\n","axs[2].legend()\n","axs[2].axhline(0,linestyle='--',color='gray',linewidth=.6,zorder=-30)\n","axs[2].set(ylabel='Slope ($\\\\beta_1$)',xticks=range(len(tokens[0])),\n","           xticklabels=[tokenizer.decode(t) for t in tokens[0]],\n","           title='C) Regression slopes')\n","axs[2].tick_params(axis='x',labelrotation=90)\n","\n","plt.tight_layout()\n","plt.savefig('ch5_proj31_part5.png')\n","plt.show()"],"metadata":{"id":"YuJX9ncZf6LL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KrgoPwsof6FT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 6: Logit Lens text heatmap**"],"metadata":{"id":"hhnmDbihhUtk"}},{"cell_type":"code","source":["lsm_max_scaled = (lsm_max-lsm_max.min()) /"],"metadata":{"id":"jVKtmmbzdz0e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,ax = plt.subplots(1,figsize=(13,11))\n","\n","# original text (separated into a list of decoded tokens)\n","target = [tokenizer.decode(t) for t in tokens[0]]\n","numTokens = len(target)\n","\n","# loop over layers\n","for layeri in range(n_hidden):\n","\n","  # y-axis coordinate for this layer\n","  yCoord =\n","\n","  # print the layer number in the left margin\n","  ax.text(-.1,yCoord,f'Layer {layeri}:',ha='right')\n","\n","  # loop over the predicted tokens in this layer\n","  for xi,tok in enumerate(predictedTokens[layeri]):\n","    ax.text(,,,ha='center',\n","            bbox=dict(boxstyle='round,pad=0.3', facecolor=plt.cm.Reds(lsm_max_scaled[layeri,xi]), edgecolor='none',alpha=.5))\n","\n","ax.axis('off')\n","\n","# finally, draw the target tokens at the bottom\n","ax.text(-.1,yCoord-.05,f'Target:',ha='right',fontweight='bold')\n","for xi,tok in enumerate(target):\n","  ax.text(,,tok,ha='center',fontsize=12,fontweight='bold')\n","\n","plt.tight_layout()\n","plt.savefig('ch5_proj31_part6.png')\n","plt.show()"],"metadata":{"id":"oaH9ZTWKhUqp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SdBzF78cZGGA"},"execution_count":null,"outputs":[]}]}