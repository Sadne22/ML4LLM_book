{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"L4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[28] Effective dimensionality of hidden layers</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OxmEbIoa-yv0"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","import requests\n","\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer"]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300\n","})"],"metadata":{"id":"dy4A-ah8kzZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SDtqs90O2uGL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Concept of effective dimensionality**"],"metadata":{"id":"gLhUL-hM9o9E"}},{"cell_type":"code","source":["# a bit of data\n","x = np.random.uniform(low=0,high=5,size=50)\n","y = np.random.normal(loc=0,scale=.07,size=len(x))\n","\n","_,axs = plt.subplots(1,2,figsize=(9,3.5))\n","axs[0].plot(x,x,'ko',markerfacecolor=[.9,.7,.7,.5],markersize=10)\n","axs[1].plot(x,x+y,'ko',markerfacecolor=[.7,.9,.9,.5],markersize=10)\n","\n","for a in axs:\n","  a.axis('square')\n","  a.plot([0,5],[0,5],color='gray',zorder=-2)\n","\n","axs[0].set(xlabel='x',ylabel='y',title='A) 1-dimensional data')\n","axs[1].set(xlabel='x',ylabel='y',title='B) 1 effective dimension')\n","\n","plt.tight_layout()\n","plt.savefig('ch5_proj28_part0.png')\n","plt.show()"],"metadata":{"id":"vP8XWGCB9p6p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"aHUQN2609o6L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 1: Logits from real and shuffled token sequences**"],"metadata":{"id":"uZo9uw2k2uDm"}},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"1Cno6-hj2uAi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load GPT2 model and tokenizer\n","model = AutoModelForCausalLM.from_pretrained('gpt2-xl')\n","tokenizer = AutoTokenizer.from_pretrained('gpt2-xl')\n","\n","max_seq_len = model.config.n_ctx\n","\n","model.to(device)\n","model.eval()"],"metadata":{"id":"Fs5uYfMAOeL3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### use this cell for Part 4\n","# from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","# tokenizer = AutoTokenizer.from_pretrained('EleutherAI/pythia-2.8b')\n","# model = AutoModelForCausalLM.from_pretrained('EleutherAI/pythia-2.8b')\n","\n","# max_seq_len = model.config.max_position_embeddings\n","\n","# model.to(device)\n","# model.eval()"],"metadata":{"id":"a8QL76qpNSCq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Through the looking glass (Alice in Wonderland)\n","text = requests.get('https://www.gutenberg.org/cache/epub/11/pg11.txt').text\n","\n","# for Part 6\n","# text = requests.get('https://www.gutenberg.org/').text\n","\n","\n","allTokens = tokenizer.encode(text,return_tensors='pt')\n","\n","# get context-length from middle of the book\n","start_idx = len(allTokens[0])//2\n","end_idx = start_idx + max_seq_len\n","\n","tokens = allTokens[:,start_idx:end_idx]\n","\n","print(tokenizer.decode(tokens[0]))"],"metadata":{"id":"1QCtJSG8LRVO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokensShuffle = tokens[0,torch.randperm(len(tokens[0]))].unsqueeze(0)\n","print(tokenizer.decode(tokensShuffle[0]))"],"metadata":{"id":"_KEpro9XLRDg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# push through the model (~3 mins with gpt2-xl on standard CPU, or <1s on GPU, lol)\n","with torch.no_grad():\n","  outputs_real = model(tokens.to(device),output_hidden_states=True)\n","  outputs_shuf = model(tokensShuffle.to(device),output_hidden_states=True)\n","\n","outputs_real.hidden_states[0].shape"],"metadata":{"id":"DfiAwJNxLRSe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# calculate log-softmax of logits (note the method instead of function)\n","log_sm_real = outputs_real.logits.log_softmax(dim=-1).cpu()\n","log_sm_shuf = outputs_shuf.logits.log_softmax(dim=-1).cpu()\n","\n","# find the indices of the max log-sm values\n","maxidx = torch.argmax(log_sm_real,dim=-1).squeeze()\n","\n","# then find the actual log-sm values from those indices\n","maxvals = [log_sm_real[0,i,maxidx[i]].item() for i in range(len(maxidx))]\n","\n","# and calculate the histogram\n","yReal,xReal = np.histogram(maxvals,bins='fd',density=True)\n","\n","\n","### repeat for shuffled\n","maxidx = torch.argmax(log_sm_shuf,dim=-1).squeeze()\n","maxvals = [log_sm_shuf[0,i,maxidx[i]].item() for i in range(len(maxidx))]\n","yShuf,xShuf = np.histogram(maxvals,bins='fd',density=True)\n","\n","\n","# visualize\n","plt.figure(figsize=(9,3))\n","plt.plot(xReal[:-1],yReal,'bs-',markerfacecolor=[.7,.7,.9],markersize=12,linewidth=2,label='Real')\n","plt.plot(xShuf[:-1],yShuf,'ro-',markerfacecolor=[.9,.7,.7],markersize=12,linewidth=2,label='Shuffled')\n","\n","plt.gca().set(xlabel='Max log softmax',ylabel='Density',title='Maximum token probabilities')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.savefig('ch5_proj28_part1.png')\n","plt.show()"],"metadata":{"id":"XBiOIK-S2t9Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"v6M3uj_R-fpU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: Effective dimensionality in one layer (numpy)**"],"metadata":{"id":"GfJhg4lk-fme"}},{"cell_type":"code","source":["layeri = 10\n","\n","# extract all the activations from this layer\n","acts = outputs_real.hidden_states[layeri][0,:,:].cpu().numpy()\n","\n","# mean-center the activations\n","acts -= acts.mean(axis=0,keepdims=True)\n","\n","# get singular values\n","s = np.linalg.svd(acts)[1]\n","\n","# percent explained (and cumulative)\n","pctExplained = 100 * s**2 / np.sum(s**2)\n","cumVarExplained = np.cumsum(pctExplained)\n","\n","\n","_,axs = plt.subplots(1,2,figsize=(10,3))\n","\n","axs[0].plot(pctExplained,'ks',markersize=10,markerfacecolor=[.7,.9,.7,.7])\n","axs[0].set(xlim=[-2,60],xlabel='Component number',ylabel='Percent variance explained',\n","           title='A) Singular values spectrum')\n","\n","axs[1].plot(cumVarExplained,'ks',markersize=10,markerfacecolor=[.7,.7,.9,.7])\n","axs[1].axhline(cumVarExplained[50],linestyle='--',color='gray')\n","axs[1].axvline(50,linestyle='--',color='gray')\n","axs[1].set(xlim=[-2,60],xlabel='Component number',ylabel='Cumulative % variance explained',\n","           title='B) Cumulative singular values')\n","\n","plt.tight_layout()\n","plt.savefig('ch5_proj28_part2.png')\n","plt.show()"],"metadata":{"id":"vluVtJ_l-fj5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["threshold = 95\n","\n","# count the components until 95% variance is explained\n","effectiveCompCount = np.where(cumVarExplained>threshold)[0][0]\n","print(f'{effectiveCompCount} (out of {len(s)}) components explains {cumVarExplained[effectiveCompCount]/100:.2%} variability')"],"metadata":{"id":"7-B2A7Ob27Ox"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6Ns9hwCU2t53"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: Laminar profile of effective dimensionality (PyTorch)**"],"metadata":{"id":"R5SI-Iyy4dtm"}},{"cell_type":"code","source":["numHidden = len(outputs_real.hidden_states)\n","numHidden"],"metadata":{"id":"ns3fdePRDnGH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# redefine threshold\n","threshold = 95\n","\n","# initialize\n","effectiveCompCount = torch.zeros((numHidden,2),dtype=int)\n","hs_ranks = torch.zeros((numHidden,2),dtype=int)\n","\n","\n","# loop over layers\n","for layeri in tqdm(range(numHidden)):\n","\n","  # extract all the activations from this layer (assuming no batches!)\n","  acts = outputs_real.hidden_states[layeri][0,:,:]\n","\n","  # mean-center the activations and calcuate rank\n","  acts -= acts.mean(axis=0,keepdims=True)\n","  hs_ranks[layeri,0] = torch.linalg.matrix_rank(acts).item()\n","\n","  # get singular values\n","  s = torch.linalg.svdvals(acts)\n","\n","  # percent explained (cumulative)\n","  pctExplained = 100 * s**2 / torch.sum(s**2)\n","  cumVarExplained = torch.cumsum(pctExplained,dim=-1).cpu()\n","\n","  # count the components until 95% variance is explained\n","  compcount = torch.where(cumVarExplained>threshold)[0][0]\n","  effectiveCompCount[layeri,0] = compcount\n","\n","\n","\n","  ### repeat for shuffled tokens\n","  acts = outputs_shuf.hidden_states[layeri][0,:,:]\n","  acts -= acts.mean(axis=0,keepdims=True)\n","  hs_ranks[layeri,1] = torch.linalg.matrix_rank(acts).item()\n","  s = torch.linalg.svdvals(acts) # get singular values\n","  pctExplained = 100 * s**2 / torch.sum(s**2) # percent explained\n","  cumVarExplained = torch.cumsum(pctExplained,dim=-1).cpu() # cumulative\n","  effectiveCompCount[layeri,1] = torch.where(cumVarExplained>threshold)[0][0]\n"],"metadata":{"id":"54gA1dd1HXEm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for reference with the scatter plot\n","print(f'There are {len(np.unique(tokens[0]))} out of {len(tokens[0])} unique tokens.')"],"metadata":{"id":"CDqGWOeXWuWs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,ax = plt.subplots(1,figsize=(10,3))\n","\n","## plot the \"effective subspace dimensionality\" of each layer\n","ax.plot(hs_ranks[:,1],'ks',markerfacecolor=[.9,.7,.7,.5],markersize=12,label='Shuffled tokens')\n","ax.plot(hs_ranks[:,0],'ko',markerfacecolor=[.7,.9,.7,.5],markersize=12,label='Real tokens')\n","ax.legend()\n","ax.set(xlabel='Hidden layer',ylabel='Number of dimensions',title='Maximum possible dimensionality',xlim=[-1,numHidden])\n","\n","plt.tight_layout()\n","plt.savefig('ch5_proj28_part3a.png')\n","plt.show()"],"metadata":{"id":"-Madfi1ZTgHS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hs_ranks"],"metadata":{"id":"Fs4eFErPkfcE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,ax = plt.subplots(1,figsize=(12,4))\n","\n","## plot the \"effective subspace dimensionality\" of each layer\n","ax.plot(effectiveCompCount[:,1],'ks',markerfacecolor=[.9,.7,.7,.7],markersize=12,label='Shuffled tokens')\n","ax.plot(effectiveCompCount[:,0],'ko',markerfacecolor=[.7,.9,.7,.7],markersize=12,label='Real tokens')\n","ax.legend(facecolor='w')\n","ax.grid(color=[.3,.3,.3])\n","ax.set(xlabel='Hidden layer',ylabel='Number of dimensions',title='Effective dimensionality',xlim=[-1,numHidden])\n","\n","ax2 = ax.twinx()\n","a = 100*effectiveCompCount[:,1] / hs_ranks[:,1]\n","b = 100*effectiveCompCount[:,0] / hs_ranks[:,0]\n","ax2.plot(a,linewidth=2,color=[.9,.7,.7])\n","ax2.plot(b,linewidth=2,color=[.7,.9,.7])\n","ax2.set(ylabel='% max dimensionality')\n","\n","plt.tight_layout()\n","plt.savefig('ch5_proj28_part3b.png')\n","plt.show()"],"metadata":{"id":"vpOplVxoPKVE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PtIoa3avDsrR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 4: Repeat with Pythia-2.8b**"],"metadata":{"id":"GwhrdMr0DsoQ"}},{"cell_type":"code","source":[],"metadata":{"id":"Wk2KST6QMsp6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 5: Repeat with html code**"],"metadata":{"id":"cBad1athDsOe"}},{"cell_type":"code","source":[],"metadata":{"id":"NB1xDNnU3Z7F"},"execution_count":null,"outputs":[]}]}