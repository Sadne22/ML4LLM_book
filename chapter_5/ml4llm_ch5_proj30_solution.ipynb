{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100",
   "private_outputs": true,
   "authorship_tag": "ABX9TyP82RHN9eqxdIpN6abN9rUI"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n",
    "|-|:-:|\n",
    "|<h2>Project:</h2>|<h1><b>[30] Sentiment analysis with decision trees</b></h1>|\n",
    "|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n",
    "\n",
    "<br>\n",
    "\n",
    "<i>Using the code without reading the book may lead to confusion or errors.</i>"
   ],
   "metadata": {
    "id": "py_eibYAH3Q-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OxmEbIoa-yv0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "R5SI-Iyy4dtm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "### matplotlib adjustments (commented lines are for dark mode)\n",
    "\n",
    "# svg plots (higher-res)\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
    "\n",
    "plt.rcParams.update({\n",
    "    # 'figure.facecolor': '#282a2c',\n",
    "    # 'figure.edgecolor': '#282a2c',\n",
    "    # 'axes.facecolor':   '#282a2c',\n",
    "    # 'axes.edgecolor':   '#DDE2F4',\n",
    "    # 'axes.labelcolor':  '#DDE2F4',\n",
    "    # 'xtick.color':      '#DDE2F4',\n",
    "    # 'ytick.color':      '#DDE2F4',\n",
    "    # 'text.color':       '#DDE2F4',\n",
    "    'axes.spines.right': False,\n",
    "    'axes.spines.top':   False,\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.labelweight': 'bold',\n",
    "    'savefig.dpi':300\n",
    "})"
   ],
   "metadata": {
    "id": "dy4A-ah8kzZQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "qilMvpnHGqDb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Part 1: Import BERT and dataset**"
   ],
   "metadata": {
    "id": "pc_8Bl8XGqAi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "model = BertModel.from_pretrained('bert-large-uncased', output_hidden_states=True)\n",
    "model.eval()"
   ],
   "metadata": {
    "id": "APaaXsHI-4bB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# move the model to the GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device);"
   ],
   "metadata": {
    "id": "43gNqb7r_0UO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# load a subset (5%) of sst-2 sentiment dataset\n",
    "dataset = load_dataset('glue','sst2',split='train[:5%]')\n",
    "\n",
    "dataset"
   ],
   "metadata": {
    "id": "qj5QoIec-8jo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset[123]"
   ],
   "metadata": {
    "id": "ZSt4fK97HGml"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# extract and count the labels\n",
    "labels = [sample['label'] for sample in dataset]\n",
    "\n",
    "num_samples = len(labels)\n",
    "\n",
    "uniq,counts = np.unique(labels,return_counts=True)\n",
    "for u,c in zip(uniq,counts):\n",
    "  print(f'\"{u}\" appeared {c} times ({c/num_samples:.1%}).')"
   ],
   "metadata": {
    "id": "Vv5UkkRxHKxZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "gQp1N3DAHKrR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Part 2: Create batches**"
   ],
   "metadata": {
    "id": "rDA6vxswIYlV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "batchsize = 32\n",
    "num_batch = num_samples//batchsize\n",
    "sample_size = batchsize * num_batch\n",
    "\n",
    "print(f'There are {num_batch} batches of size {batchsize}, leading to {sample_size} total samples.')"
   ],
   "metadata": {
    "id": "nEfzMrp1IYif"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "allbatches = []\n",
    "alllabels = []\n",
    "\n",
    "for batchi in range(num_batch):\n",
    "\n",
    "  # start and end indices\n",
    "  startidx = batchi*batchsize\n",
    "  endidx = startidx + batchsize\n",
    "\n",
    "  # append texts\n",
    "  tmp_texts = []\n",
    "  tmp_labels = []\n",
    "  for samplei in range(startidx,endidx):\n",
    "    tmp_texts.append(dataset[samplei]['sentence'])\n",
    "    tmp_labels.append(dataset[samplei]['label'])\n",
    "\n",
    "  # tokenize\n",
    "  tokens = tokenizer(tmp_texts,return_tensors='pt',padding=True)\n",
    "  allbatches.append( tokens )\n",
    "  alllabels.append( tmp_labels )"
   ],
   "metadata": {
    "id": "dLyaDnboInrN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "len(alllabels), len(allbatches)"
   ],
   "metadata": {
    "id": "PFsWUPGrKVK3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "len(alllabels[10]), allbatches[10], allbatches[10]['input_ids'].shape"
   ],
   "metadata": {
    "id": "RRe1B90jKaZ_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# lengths of different batches\n",
    "seqlens = [allbatches[i]['input_ids'].shape[1] for i in range(len(allbatches))]\n",
    "numseqs = [allbatches[i]['input_ids'].shape[0] for i in range(len(allbatches))]\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.plot(numseqs,'ks',markerfacecolor=[.7,.7,.9],label='Number of sequences')\n",
    "plt.plot(seqlens,'ko',markerfacecolor=[.7,.9,.7],label='Number of tokens')\n",
    "plt.gca().set(xlabel='Batch number',ylabel='Counts',ylim=[10,70])\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ch5_proj30_part2.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "hBJhrGHB8NJD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# just one batch to confirm shapes\n",
    "outputs = model(**allbatches[10].to(device))\n",
    "outputs.hidden_states[4].shape"
   ],
   "metadata": {
    "id": "KMmWkoAmHKoe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "JGlxzvmaHKlq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Part 3: Get the [CLS] hidden state activations from one layer**"
   ],
   "metadata": {
    "id": "dpXODZelKt34"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# this code block takes a very long time on a standard CPU, ~5 mins on a high-power CPU, and a few seconds on a GPU\n",
    "cls_activations = []\n",
    "\n",
    "for batchi in tqdm(range(num_batch)):\n",
    "\n",
    "  # run the model\n",
    "  outputs = model(**allbatches[batchi].to(device))\n",
    "\n",
    "  # extract the CLS activation from the hidden states\n",
    "  cls_batch = outputs.hidden_states[-1][:,0,:].detach().cpu().numpy()\n",
    "\n",
    "  # append to a list\n",
    "  cls_activations.append(cls_batch)\n",
    "\n",
    "# and convert to numpy\n",
    "cls_activations = np.vstack(cls_activations)\n",
    "labels = np.hstack(alllabels)\n",
    "\n",
    "cls_activations.shape, labels.shape"
   ],
   "metadata": {
    "id": "Zbra0tR6Kyl1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# extract average activations\n",
    "acts0 = cls_activations[labels==0,:].mean(axis=1)\n",
    "acts1 = cls_activations[labels==1,:].mean(axis=1)\n",
    "\n",
    "# t-test\n",
    "t = ttest_ind(acts0,acts1)\n",
    "\n",
    "# the violin plot\n",
    "plt.figure(figsize=(8,5))\n",
    "v = plt.violinplot([acts0,acts1])\n",
    "\n",
    "# change the colors\n",
    "v['bodies'][0].set_facecolor([.9,.7,.7])\n",
    "v['bodies'][1].set_facecolor([.7,.9,.7])\n",
    "v['cbars'].set_edgecolor('k')\n",
    "v['cmins'].set_edgecolor('k')\n",
    "v['cmaxes'].set_edgecolor('k')\n",
    "\n",
    "# draw all the dots\n",
    "plt.plot(np.random.normal(1,.03,len(acts0)),acts0,'.',color=[.9,.7,.7,.3])\n",
    "plt.plot(np.random.normal(2,.03,len(acts1)),acts1,'.',color=[.7,.9,.7,.3])\n",
    "\n",
    "# and finishing touches\n",
    "plt.axhline(0,linestyle='--',linewidth=.2,color='k')\n",
    "plt.gca().set(xticks=[1,2],xticklabels=['Negative','Positive'],ylabel='Mean [CLS] Activation',\n",
    "              xlim=[.5,2.5],title=f'Mean [CLS] acts (t={t.statistic:.2f}, p={t.pvalue:.3f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ch5_proj30_part3.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "gZkZb_G2NWVO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "mRVn5Xq7KtxP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Part 4: Single-layer decision-tree classifier**"
   ],
   "metadata": {
    "id": "JBm5slnkSqMY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# dimension reduction with pca\n",
    "pca = PCA(n_components=.8)\n",
    "act_reduced = pca.fit_transform(cls_activations)\n",
    "\n",
    "print(f'Kept {pca.n_components_} components explaining {pca.explained_variance_ratio_.sum():.2%} variance.\\n')\n",
    "print(f'Original data size is {cls_activations.shape}')\n",
    "print(f' Reduced data size is {act_reduced.shape}\\n')\n",
    "print(f'Observations:features ratio is {act_reduced.shape[0]}:{act_reduced.shape[1]} = {act_reduced.shape[0]/act_reduced.shape[1]:.1f}')"
   ],
   "metadata": {
    "id": "tvcGLX6r_ADA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# split the data into train and test\n",
    "X_train,X_test, y_train,y_test = train_test_split(act_reduced,labels,test_size=.2,stratify=labels)\n",
    "dectree = DecisionTreeClassifier(max_depth=4)\n",
    "dectree.fit(X_train, y_train)\n",
    "\n",
    "# train and test accuracy\n",
    "train_acc = (dectree.predict(X_train) == y_train).mean()\n",
    "test_acc  = (dectree.predict(X_test) == y_test).mean()\n",
    "\n",
    "print(f'Accuracies: Train {train_acc:.2%}, Test: {test_acc:.2%}')"
   ],
   "metadata": {
    "id": "BHfbPZ1XQ0w3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# how many times to repeat the random data split\n",
    "num_reps = 20\n",
    "\n",
    "# re-initialize as arrays\n",
    "train_acc = np.zeros(num_reps)\n",
    "test_acc = np.zeros(num_reps)\n",
    "\n",
    "# loop over reps\n",
    "for i in range(num_reps):\n",
    "\n",
    "  # run the analysis\n",
    "  X_train,X_test,y_train,y_test = train_test_split(act_reduced,labels,test_size=.2,stratify=labels)\n",
    "  dectree = DecisionTreeClassifier(max_depth=4)\n",
    "  dectree.fit(X_train,y_train)\n",
    "\n",
    "  # calculate and store accuracies\n",
    "  train_acc[i] = (dectree.predict(X_train) == y_train).mean()\n",
    "  test_acc[i]  = (dectree.predict(X_test) == y_test).mean()\n"
   ],
   "metadata": {
    "id": "JPA4NnUdRoIA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.plot(train_acc,'ko',markersize=12,markerfacecolor=[.7,.7,.9],label='Train')\n",
    "plt.plot(test_acc,'ks',markersize=12,markerfacecolor=[.9,.7,.7],label='Test')\n",
    "\n",
    "plt.axhline(train_acc.mean(),linestyle='--',color=[.7,.7,.9])\n",
    "plt.axhline(test_acc.mean(),linestyle='--',color=[.9,.7,.7])\n",
    "\n",
    "plt.gca().set(ylim=[.6,.9],xticks=range(0,num_reps,2),xlabel='Run',ylabel='Accuracy',\n",
    "              title='Variability due to random data splitting')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ch5_proj30_part4a.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "BcNgJYzVSgvX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# feature importances (of final run from previous cell)\n",
    "importances = dectree.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.title(\"Feature Importances (Top 10 PCs)\")\n",
    "for i in range(10):\n",
    "  imp_val = importances[indices[i]]\n",
    "  plt.bar(i,imp_val,color=plt.cm.plasma(imp_val/importances.max()),edgecolor='k')\n",
    "plt.xticks(range(10), [f\"PC{indices[i]+1}\" for i in range(10)], rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ch5_proj30_part4b.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "goQNXkxJ_HYu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# FYI (not part of the exercise)\n",
    "# The tree plot shows which features were used in each leaf split.\n",
    "# However, this plot isn't really interpretable for PCA data.\n",
    "import sklearn.tree\n",
    "plt.figure(figsize=(14,10))\n",
    "sklearn.tree.plot_tree(dectree,fontsize=10);"
   ],
   "metadata": {
    "id": "dQxYkpwKTP2o"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "GLNzuDVBAN_5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Part 5: Collect [CLS] activations from all layers**"
   ],
   "metadata": {
    "id": "H7YrMYmUQ0tl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# the number of hidden-states from the model output (transformers + 1)\n",
    "num_layers = model.config.num_hidden_layers + 1\n",
    "\n",
    "# list to store all the activations\n",
    "hs_acts = [np.zeros((sample_size,model.config.hidden_size)) for _ in range(num_layers)]\n",
    "\n",
    "# loop over all batches\n",
    "idx = 0\n",
    "for batchi in tqdm(range(num_batch)):\n",
    "\n",
    "  # run the model\n",
    "  outputs = model(**allbatches[batchi].to(device))\n",
    "\n",
    "  # extract the CLS activation from the hidden states\n",
    "  for hsi,hs in enumerate(outputs.hidden_states):\n",
    "    hs_acts[hsi][idx:idx+batchsize] = hs[:,0,:].detach().cpu().numpy()\n",
    "  idx += batchsize\n"
   ],
   "metadata": {
    "id": "snrHaz6LQ0qs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "len(hs_acts), hs_acts[hsi].shape"
   ],
   "metadata": {
    "id": "Oa2Ipc7uBYcv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# visualize the vector averages\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "for hsi in range(len(hs_acts)):\n",
    "  plt.plot(np.random.normal(hsi-.2,.03,(labels==0).sum()),\n",
    "           hs_acts[hsi][labels==0,:].mean(axis=1),'ko',markerfacecolor=[.9,.7,.7,.3],linewidth=0)\n",
    "  plt.plot(np.random.normal(hsi+.2,.03,(labels==1).sum()),\n",
    "           hs_acts[hsi][labels==1,:].mean(axis=1),'ks',markerfacecolor=[.7,.9,.7,.3],linewidth=.1)\n",
    "\n",
    "\n",
    "plt.gca().set(xlabel='Hidden layer',ylabel='CLS activation (mean)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ch5_proj30_part5a.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "FHfujiay6dkX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "XDLOEkjC8q_B"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Part 6: Laminar profile of classification accuracy**"
   ],
   "metadata": {
    "id": "9u0ivKf58q8F"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# main analysis\n",
    "\n",
    "# initializations\n",
    "accuracies = np.zeros((num_layers,2))\n",
    "\n",
    "num_reps = 5\n",
    "train_acc = np.zeros(num_reps)\n",
    "test_acc = np.zeros(num_reps)\n",
    "\n",
    "\n",
    "# loop over layers\n",
    "for layeri in range(num_layers):\n",
    "\n",
    "\n",
    "  # loop over repetitions for stability\n",
    "  for i in range(num_reps):\n",
    "\n",
    "    # 1) split the data\n",
    "    X_train,X_test,y_train,y_test = train_test_split(hs_acts[layeri],labels,test_size=.2,stratify=labels)\n",
    "\n",
    "    # 2) fit the PCA on the train data\n",
    "    pca = PCA(n_components=.8)\n",
    "    X_train_pca = pca.fit_transform(X_train) # apply to train data\n",
    "\n",
    "    # 3) apply the PCA transform to the test set\n",
    "    X_test_pca  = pca.transform(X_test) # apply to test data\n",
    "\n",
    "    # 4) fit the decision-tree model\n",
    "    dectree = DecisionTreeClassifier(max_depth=4)\n",
    "    dectree.fit(X_train_pca,y_train)\n",
    "\n",
    "    # train accuracy\n",
    "    train_acc[i] = (dectree.predict(X_train_pca) == y_train).mean()\n",
    "    test_acc[i]  = (dectree.predict(X_test_pca) == y_test).mean()\n",
    "\n",
    "  # accuracies for this layer\n",
    "  accuracies[layeri,0] = train_acc.mean()\n",
    "  accuracies[layeri,1] = test_acc.mean()\n",
    "\n",
    "  print(f'Finished layer {layeri+1:2}/{num_layers} with {accuracies[layeri,1]:.1%} test accuracy.')"
   ],
   "metadata": {
    "id": "0e4_tO-CSkJM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(accuracies[:,0],'ks-',linewidth=.2,markerfacecolor=[.7,.9,.7],markersize=12,label='Train')\n",
    "plt.plot(accuracies[:,1],'ko-',linewidth=.2,markerfacecolor=[.7,.7,.9],markersize=12,label='Test')\n",
    "\n",
    "plt.legend()\n",
    "plt.gca().set(xlabel='Hidden state layer',ylabel='Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ch5_proj30_part5b.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "pOEadxFQSkGY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "8l8TZ9QbXQxg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Part 7: Performance benefit per transformer**"
   ],
   "metadata": {
    "id": "jv71O5YqXQud"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# create the predictor variable (IV)\n",
    "predictor = np.arange(1,num_layers).reshape(-1,1)\n",
    "\n",
    "# fit the model\n",
    "reg = LinearRegression(fit_intercept=True).fit(predictor,accuracies[1:,1])\n",
    "print(f'beta_0: {reg.intercept_:6.3%}')\n",
    "print(f'beta_1: {reg.coef_[0]:7.3%}')\n",
    "\n",
    "# calculate predicted accuracies\n",
    "yHat = reg.predict(predictor)"
   ],
   "metadata": {
    "id": "R62HgIrmVtTt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# and visualize\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(predictor,yHat,'k',label='Linear fit')\n",
    "plt.plot(predictor,accuracies[1:,1],'ko-',linewidth=.2,markerfacecolor=[.7,.7,.9],markersize=12,label='Data')\n",
    "\n",
    "plt.gca().set(xlabel='Number of transformers',ylabel='Accuracy',\n",
    "              title=f'Increase of {reg.coef_[0]:.2%} accuracy per transformer')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ch5_proj30_part6.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "yLwsFLi-Z2ID"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(f'Regression-predicted boost per transformer  : {reg.coef_[0]:.4%}')\n",
    "print(f'Empirically calculated boost per transformer: {np.mean(np.diff(accuracies[1:,1])):.4%}')"
   ],
   "metadata": {
    "id": "NwldPR3HSkDs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "-0Rq3-76YZx2"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}