{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[19] LLM loss function</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","source":[],"metadata":{"id":"R5SI-Iyy4dtm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from transformers import AutoModelForCausalLM, AutoTokenizer"],"metadata":{"id":"rn8Fcyf87EXx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300,\n","})"],"metadata":{"id":"dy4A-ah8kzZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nxqgPabM0FuN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 1: Manual negative log likelihood loss**"],"metadata":{"id":"DA3zcyDTHsH7"}},{"cell_type":"code","source":["# start with three outputs (raw model outputs for three tokens in the vocab)\n","model_output = np.array([ -1, 2.3, .1 ])\n","print('Raw model outputs:\\n  ',model_output,'\\n')\n","\n","# NLLLoss expects log-softmax inputs!\n","softmax = np.exp(model_output) / np.exp(model_output).sum()\n","logsoftmax_output = np.log(softmax)\n","print('Log-softmax model outputs:\\n  ',logsoftmax_output,'\\n')\n","\n","\n","# check the loss for different targets\n","for target in range(len(model_output)):\n","\n","  # calculate the loss\n","  theloss = -logsoftmax_output[target]\n","\n","  # and print\n","  print(f'Loss is {theloss:.4f} when correct output is index \"{target}\"')"],"metadata":{"id":"VY7CzJLhlsrd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HWRj_qJmpBCH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: NLLLoss in PyTorch**"],"metadata":{"id":"HHG6X1yEpA_M"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HU06kyWmre5Q"},"outputs":[],"source":["# create a loss function instance\n","loss_function = nn.NLLLoss()\n","dir(loss_function)"]},{"cell_type":"code","source":["# start with three outputs (raw model outputs for three tokens in the vocab)\n","model_output = torch.tensor([[ -1, 2.3, .1 ]])\n","print('Raw model outputs:\\n  ',model_output[0].tolist(),'\\n')\n","\n","# NLLLoss expects log-softmax inputs!\n","logsoftmax_output = F.log_softmax(model_output,dim=-1)\n","print('Log-softmax model outputs:\\n  ',logsoftmax_output[0],'\\n')\n","\n","\n","# check the loss for different targets\n","for target in range(len(model_output[0])):\n","\n","  # which output is the target (correct response)?\n","  target = torch.tensor([target])\n","\n","  # calculate the loss\n","  theloss = loss_function(logsoftmax_output,target)\n","\n","  # and print\n","  print(f'Loss is {theloss.item():.4f} when correct output is index \"{target.item()}\"')"],"metadata":{"id":"0Uh9evVlsrP2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create some tensors\n","T1 = torch.tensor([3.4])\n","T2 = torch.tensor([3,4])\n","T3 = torch.tensor([ [3.1],[4] ])\n","\n","print(f'T1 is of type {type(T1)}, and T1.item() is of type {type(T1.item())}\\n')\n","# print(f'T2 is of type {type(T2)}, and T2.item() is of type {type(T2.item())}\\n')\n","# print(f'T3 is of type {type(T3)}, and T3.item() is of type {type(T3.item())}\\n')"],"metadata":{"id":"wH3C3uugVFeE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# otherwise:\n","T3.numpy()"],"metadata":{"id":"-QErbMUoYV0Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"mYbtfZDGlsoq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: Cross-entropy loss for next-token prediction**"],"metadata":{"id":"txJd0RSKo8xD"}},{"cell_type":"code","source":["# load pretrained GPT-2 model and tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('gpt2')\n","model = AutoModelForCausalLM.from_pretrained('gpt2')\n","model.eval()"],"metadata":{"id":"ROL219FwtKJw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from the Beatles :<)\n","text = 'There are places I remember all my life, though some have changed. Some forever, not for better, some have gone and some remain.'\n","\n","tokens = tokenizer.encode(text,return_tensors='pt')\n","print(f'There are {len(tokens[0])} tokens, {len(set(tokens[0]))} of which are unique.')"],"metadata":{"id":"pvl77X52sACU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# targets are the next-tokens\n","targets = tokens[0,1:]\n","\n","# print the input-target table\n","print(' Input | Target')\n","print('-------+--------')\n","for i in range(len(tokens[0])-1):\n","  print(f' {tokens[0,i]:4}  |  {targets[i]:4}')"],"metadata":{"id":"zK5v57Fft5qv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# forward pass and get logits\n","with torch.no_grad():\n","  outputs = model(tokens)\n","logits = outputs.logits[0,:-1,:]\n","\n","# NLLLoss expects log-softmax\n","logits_log_sm = F.log_softmax(logits,dim=-1)\n","\n","print(f'Shape of logits: {list(logits.shape)}')\n","print(f'Shape of targets: {list(targets.shape)}')"],"metadata":{"id":"oETrtZRCpKoJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# losses\n","loss = loss_function(logits_log_sm,targets)\n","loss"],"metadata":{"id":"CahWrwbWjCQs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"O0HAea-Atsrb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 4: Manual loss calculation**"],"metadata":{"id":"jogJumzXuxRo"}},{"cell_type":"code","source":["losses = np.zeros(len(tokens[0])-1)\n","\n","for i in range(len(tokens[0])-1):\n","\n","  # get the log-softmax for this token\n","  log_sm = F.log_softmax( outputs.logits[0,i,:] ,dim=-1)\n","\n","  # pick out the logsm for target\n","  losses[i] = -log_sm[tokens[0,i+1]].item()\n","\n","losses.mean()"],"metadata":{"id":"jCTq4OG0vZDE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(12,4))\n","\n","loss_scale = (losses-losses.min()) / (losses.max()-losses.min())\n","xlabels = [tokenizer.decode(t) for t in tokens[0][1:]]\n","\n","plt.bar(range(len(losses)),losses,edgecolor='k',linewidth=.3,color=plt.cm.magma(loss_scale))\n","plt.gca().set(xticks=range(len(losses)),xlabel='Token',ylabel='Loss',\n","              xticklabels=xlabels)\n","plt.xticks(rotation=90)\n","\n","plt.tight_layout()\n","plt.savefig('ch4_proj19_part4.png')\n","plt.show()"],"metadata":{"id":"1rk8Df7ixUQi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_pShjXRRvY9b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 5: Using the model's internal loss calculation**"],"metadata":{"id":"sSXcKNrfpKlO"}},{"cell_type":"code","source":["with torch.no_grad():\n","  outputs = model(tokens,labels=tokens)\n","outputs.loss"],"metadata":{"id":"BJy05_5lo8uO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Lt6246RBjZDL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 6: Expected loss from random tokens**"],"metadata":{"id":"rSATFN4zjZAH"}},{"cell_type":"code","source":["# the expected loss of random tokens is log(vocab size) (or is it??)\n","expectedLoss = -torch.log(torch.tensor(1/tokenizer.vocab_size)) # math note: -log(1/V) == log(V)\n","print(f'Expected loss: {expectedLoss:.4f}')"],"metadata":{"id":"KVWZirPlUz_f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["losses = np.zeros(10)\n","for i in range(len(losses)):\n","\n","  # generate random tokens\n","  randtokens = torch.randint(tokenizer.vocab_size,(1,1024))\n","\n","  # get the loss\n","  with torch.no_grad():\n","    outputs = model(randtokens,labels=randtokens)\n","  losses[i] = outputs.loss.item()\n","\n","  print(f'Finished iteration {i+1} of {len(losses)}')"],"metadata":{"id":"3I1MfGxHV4PO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the text the model tried to predict :|\n","tokenizer.decode(randtokens[0])"],"metadata":{"id":"xvi-pwSIs0Lm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(8,3))\n","\n","plt.plot(losses,'kh',markerfacecolor=[.9,.7,.7],markersize=12,label='Empirical')\n","plt.axhline(expectedLoss.item(),color=[.7,.7,.9],linestyle='--',label='Expected')\n","\n","plt.gca().set(xlabel='Iteration',ylabel='Loss')\n","\n","plt.tight_layout()\n","plt.savefig('ch4_proj19_part6.png')\n","plt.show()"],"metadata":{"id":"N_-XrKt2owUO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"m2GGHASZo8gT"},"execution_count":null,"outputs":[]}]}