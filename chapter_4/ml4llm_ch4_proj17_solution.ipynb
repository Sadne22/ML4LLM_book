{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNRdU5+O5nzARuHeyTrSQ78"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[17] Probabilistic token selection</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","source":[],"metadata":{"id":"R5SI-Iyy4dtm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn.functional as F\n","\n","from transformers import AutoModelForCausalLM, AutoTokenizer"],"metadata":{"id":"rn8Fcyf87EXx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300,\n","})"],"metadata":{"id":"dy4A-ah8kzZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nxqgPabM0FuN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 1: PyTorch's multinomial function**"],"metadata":{"id":"Re1qRlOOtbs3"}},{"cell_type":"code","source":["# a vector (must be tensor)\n","vect = torch.tensor([1,2,5],dtype=torch.float)\n","\n","# sample a number\n","torch.multinomial(vect,1)"],"metadata":{"id":"3Vmuap7vsEse"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sample 10 times from that vector\n","vect[torch.multinomial(vect,10,replacement=True)]"],"metadata":{"id":"OG-6JqqTr9JM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 10k samples!\n","mn = torch.multinomial(vect,10000,replacement=True)\n","\n","# collect the distribution\n","vals,counts = torch.unique(mn,return_counts=True)\n","\n","# print the output values and how often they occurred\n","for v,c in zip(vals,counts):\n","  print(f'\"{v}\" was sampled {c} times ({c/len(mn):.2%})')"],"metadata":{"id":"NSlCgVr8r9MJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# treat the vector as if it contains (scaled) probability values\n","\n","print('Value | Observed | Expected')\n","print('------+----------+----------')\n","\n","# again with more information\n","for v,c,vectval in zip(vals,counts,vect):\n","\n","  observedFrequency = c/len(mn)\n","  expectedFrequency = vectval/torch.sum(vect)\n","\n","  print(f' \"{v}\"  |  {observedFrequency:.2%}  |  {expectedFrequency:.2%}')"],"metadata":{"id":"oiJHySjUr9PU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.multinomial??"],"metadata":{"id":"7L1anmfrKsZp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"eG6y_V9s2uE1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: Common errors with torch.multinomial**"],"metadata":{"id":"v-d5BGGr4F72"}},{"cell_type":"code","source":["# # error 1: requires torch tensor\n","# torch.multinomial([1.,2,.3],1)\n","# torch.multinomial(np.array([1.,2,.3]),1)"],"metadata":{"id":"gQ18ToW2vfEJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # error 2: default is no replacement\n","# torch.multinomial(vect,len(vect)+1)"],"metadata":{"id":"xrwXj_xsukli"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # error 3: only floats\n","# torch.multinomial(torch.tensor([1,1,1]),1)"],"metadata":{"id":"Y9R32w7RutAS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # error 4: only non-negative numbers\n","# torch.multinomial(torch.tensor([-1,1.,1]),1)"],"metadata":{"id":"pUjqD1e_vAAx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qzN6xParxzqE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: Deterministic token selection**"],"metadata":{"id":"cnRXFKQN2uBr"}},{"cell_type":"code","source":["# load pretrained GPT-2 model and tokenizer\n","llm = AutoModelForCausalLM.from_pretrained('gpt2')\n","tokenizer = AutoTokenizer.from_pretrained('gpt2')\n","\n","llm.eval() # switch to eval mode"],"metadata":{"id":"4m885jVKGvHB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["txt = \"I don't want to grow\"\n","tokens = tokenizer.encode(txt,return_tensors='pt')\n","tokens"],"metadata":{"id":"tJSJ7xWWgsJI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# forward pass through the model\n","with torch.no_grad():\n","  outputs = llm(tokens)\n","outputs"],"metadata":{"id":"jFPKdkvuiB2E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outputs.logits.shape"],"metadata":{"id":"_W-rnqMSiBzP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logits = outputs.logits[0,-1,:].detach()\n","logits_sm = F.softmax(logits,dim=-1)\n","logits.shape"],"metadata":{"id":"4saM--Ikicvb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find the maximum\n","max_logit = logits.argmax()\n","print(f'The maximum logit is #{max_logit} with a softmax probability of {logits_sm[max_logit]:.2%}')\n","print(f'The max word is \"{tokenizer.decode(max_logit)}\"')"],"metadata":{"id":"yqH00PEWiLoj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot the raw and softmax logits\n","_,axs = plt.subplots(1,2,figsize=(12,3))\n","\n","axs[0].plot(max_logit,logits[max_logit],'rh')\n","axs[0].plot(logits,'gh',linewidth=.2,markerfacecolor=[.7,.9,.7,.3])\n","axs[0].set(xlabel='Token index',ylabel='Output logits (raw)',title='A) Raw output logits')\n","\n","axs[1].plot(max_logit,logits_sm[max_logit],'rh')\n","axs[1].plot(logits_sm,'bh',linewidth=.2,markerfacecolor=[.7,.9,.7,.3])\n","axs[1].set(xlabel='Token index',ylabel='Softmax probability',title='B) Softmax probabilities')\n","\n","plt.tight_layout()\n","plt.savefig('ch4_proj17_part3.png')\n","plt.show()"],"metadata":{"id":"w6IB8lw0ikvU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"COxAJqH_3HcD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 4: Multinomial probabilistic selection**"],"metadata":{"id":"9_i2mKYj3HZB"}},{"cell_type":"code","source":["for t in torch.multinomial(logits_sm,10,replacement=True):\n","  print(f'{txt}\"{tokenizer.decode(t)}\"\\t({logits_sm[t]:.2%})')"],"metadata":{"id":"N9M3W8573HV-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"IQ49SftV3UlW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 5: Top-k sampling**"],"metadata":{"id":"mb0GdmsZ3Udk"}},{"cell_type":"code","source":["k = 10\n","top_k = torch.topk(logits_sm,k)\n","\n","print(txt,'___\\n')\n","\n","for i in range(k):\n","  val = top_k.values[i]\n","  tok = top_k.indices[i]\n","  print(f'{tok:5} ({val:5.1%}) is \"{tokenizer.decode(tok)}\"')"],"metadata":{"id":"eWuhrE533UaT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'The top {k} options account for {logits_sm[top_k.indices].sum():.1%} of the probability mass.')"],"metadata":{"id":"PFP0NXw55Zwt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# re-normalize to sum to 1\n","sm_top_k = F.softmax(logits[top_k.indices],dim=-1)\n","\n","# what they look like:\n","print(sm_top_k)\n","\n","# confirm: sum to 1\n","print(sm_top_k.sum())"],"metadata":{"id":"Re53i1cm6dpB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for t in torch.multinomial(sm_top_k,10):\n","  tidx = top_k.indices[t] # token index into vocab (not top-k)\n","  print(f'{txt}\"{tokenizer.decode(tidx)}\"\\t({sm_top_k[t]:6.2%})')"],"metadata":{"id":"irSCRcAa49-p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fI-NAq9r3UXY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 6: Top-p sampling**"],"metadata":{"id":"pfQkS6hA2t-i"}},{"cell_type":"code","source":["# threshold\n","p_thresh = .9\n","\n","# sort the probabilities\n","sorted_sm,sorted_idx = torch.sort(logits_sm,descending=True)\n","\n","cumulative_probs = torch.cumsum(sorted_sm,dim=-1)\n","tokens2keep = torch.where(cumulative_probs>=p_thresh)[0][0]\n","\n","# print the results\n","print(f'{tokens2keep} tokens accounts for {cumulative_probs[tokens2keep]:.1%} of the probability mass.')"],"metadata":{"id":"b7rPGsGw3i0N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokens_idx = sorted_idx[:tokens2keep+1]\n","\n","# renormalize from logits to probability\n","sm_top_p = F.softmax(logits[tokens_idx],dim=-1)"],"metadata":{"id":"Qo4JAY_c3ixR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for t in torch.multinomial(sm_top_p,10):\n","  tidx = tokens_idx[t] # token index into vocab (not top-p)\n","  print(f'{txt}\"{tokenizer.decode(tidx)}\"\\t({sm_top_p[t]:6.2%})')"],"metadata":{"id":"GoFu1n_Y3iuh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"snHozTkL3ir_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 7: Chat with GPT2**"],"metadata":{"id":"-a8AIePW3ipL"}},{"cell_type":"code","source":["n_new_tokens = 15\n","\n","# start from a prompt\n","txt = \"I don't want to grow\"\n","print(f'Start prompt: {txt}')\n","\n","# loop over the new tokens\n","for _ in range(n_new_tokens):\n","\n","  # tokenize, forward pass, get final logits\n","  tokens = tokenizer.encode(txt,return_tensors='pt')\n","  with torch.no_grad():\n","    logits = llm(tokens).logits[0,-1,:].detach()\n","\n","  # softmax\n","  logits_sm = F.softmax(logits,dim=-1)\n","\n","  # pick a new token\n","  newtok = torch.multinomial(logits_sm,1)\n","\n","  # concatenate and print\n","  txt += tokenizer.decode(newtok)\n","  print(f'New token {_+1:2}: {txt}')"],"metadata":{"id":"pufVY4CO3imY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZEjz2zMT3ijv"},"execution_count":null,"outputs":[]}]}