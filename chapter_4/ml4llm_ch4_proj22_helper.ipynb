{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[22] Evaluating models with HellaSwag</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","source":[],"metadata":{"id":"R5SI-Iyy4dtm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from tqdm import tqdm\n","from datasets import load_dataset\n","\n","from statsmodels.stats.contingency_tables import mcnemar\n","\n","# pytorch-related libraries\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# libraries to import models\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from transformers import RobertaTokenizer, RobertaForMaskedLM"],"metadata":{"id":"rn8Fcyf87EXx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300,\n","})"],"metadata":{"id":"dy4A-ah8kzZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OoD9sb-M-RSp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 1: Import and compare GPT2 and RoBERTa**"],"metadata":{"id":"e3J2HE9c-RPK"}},{"cell_type":"code","source":["# use the GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# import the tokenizer and models\n","tokenizerG = AutoTokenizer.from_pretrained('gpt2')\n","gpt2_s = AutoModelForCausalLM.from_pretrained('gpt2').to(device)\n","gpt2_l = AutoModelForCausalLM.from_pretrained('gpt2-large').to(device)\n","\n","# switch to evaluation (inference) mode\n"],"metadata":{"id":"89N_ox9DJeJ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import roberta model and tokenizer\n","tokenizerR = RobertaTokenizer.from_pretrained('roberta-base')\n","roberta_s = RobertaForMaskedLM.from_pretrained('roberta-base').to(device)\n","roberta_l = RobertaForMaskedLM.from_pretrained('roberta-large').to(device)\n","\n","# set to eval mode"],"metadata":{"id":"YZton-MeGmuP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# FYI, useful model information\n","roberta_s.config"],"metadata":{"id":"rhdG04j7LXcD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# table of model sizes\n","\n","print(' Model | Tokens | Emb.dim | Xfmr | Parameters')\n","print('-------+--------+---------+------+-------------')\n","\n","# GPT2-small\n","print(f'GPT2-S | {:,} |   {:4}  |  {}  | {:,}')\n","\n","# GPT2-large\n","print(f'GPT2-L | {:,} |   {:4}  |  {}  | {:,}')\n","\n","# roberta-small\n","print(f'RoBA-S | {:,} |   {:4}  |  {}  | {:,}')\n","\n","# roberta-large\n","print(f'RoBA-L | {:,} |   {:4}  |  {}  | {:,}')\n"],"metadata":{"id":"ccL-1-t9HWtD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QJ9xgBWvGmnL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: Import and explore HellaSwag**"],"metadata":{"id":"Ykf7sFkyHen2"}},{"cell_type":"code","source":["# import the HellaSwag validation set\n","dataset = load_dataset('hellaswag',split='validation')\n","dataset"],"metadata":{"id":"Oo0fl8pMJlj7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# an example\n","dataset[1]"],"metadata":{"id":"cuhJI2xWKEvi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7pBOm9uqlamJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: Visualize the evaluation process**"],"metadata":{"id":"jo1BuhDllajO"}},{"cell_type":"code","source":["# pick a random example\n","exampleNum = 224\n","target =  # the target answer\n","\n","# context tokens and length\n","context = dataset[exampleNum]['ctx']\n","context_len =\n","\n","# prompts and their lengths\n","promptC = f\"\"\n","promptC_tox =\n","promptC_len =\n","\n","promptI = f\"\"\n","promptI_tox =\n","promptI_len =\n","\n","# show the prompts\n","print(f'Context:\\n   \"{context}\"\\n')\n","print(f'Correct ending:\\n   \"{promptC}\"\\n')\n","print(f'Incorrect ending:\\n   \"{promptI}\"')"],"metadata":{"id":"gLDukm_pKEsJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# forward pass through the model\n","with torch.no_grad():\n","  logitsC = gpt2_s\n","  logitsI =\n","\n","# log softmax (more numerically stable than prob values for later calculations)\n","log_sm_C =\n","log_sm_I =\n","\n","\n","# get the sequence of sm logits for the correct prompt\n","lsmSeqC = np.zeros(promptC_len-1)\n","for i in range(0,promptC_len-1):\n","  lsmSeqC[i] = log_sm_C[]\n","\n","# repeat for the incorrect prompt\n","lsmSeqI = np.zeros(promptI_len-1)\n","for i in range(0,promptI_len-1):\n","  lsmSeqI[i] = log_sm_I[]\n","\n","# probabilities of prompts (sum of logs equals product of probabilities)\n","probC = lsmSeqC\n","probI = .mean()\n","\n","print(f'    Target ending log-prob: {probC:.3f}')\n","print(f'Non-target ending log-prob: {probI:.3f}')"],"metadata":{"id":"rNjGstXt58cn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# a demo of a log property\n","p,q = .01,.003\n","np.log(p*q), np.log(p)+np.log(q)"],"metadata":{"id":"QHBWjum7yEME"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualize the logits\n","plt.figure(figsize=(12,4))\n","plt.plot(lsmSeqI,)\n","plt.plot(lsmSeqC,)\n","plt.axvline(,linestyle='--',color='gray')\n","\n","plt.gca().set(xlabel='Token position (index)',ylabel='Log-softmax probs',\n","              title='Token log-probabilities in HellaSwag evaluation')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.savefig('ch4_proj22_part3.png')\n","plt.show()"],"metadata":{"id":"xyHkDo7esU9F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DCNOf86o-RIp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 4: A function to test one HellaSwag sample**"],"metadata":{"id":"bMdq0rfbAjuD"}},{"cell_type":"code","source":["# define a pad token for GPT2 (RoBERTa already has a pad token)\n","tokenizerG.pad_token_id = tokenizerG.\n","tokenizerG.pad_token_id"],"metadata":{"id":"IqZ-oRzQA6-r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# demo of padding during tokenization\n","texts = [ 'hello',\n","          'my name is',\n","          'Mike and I like purple.'\n","]\n","\n","t = tokenizerG(texts,padding=True,return_tensors='pt')\n","for k,v in t.items():\n","  print(f'\"{k}\":\\n{v}\\n')"],"metadata":{"id":"rMgx75TPXVwA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# a function to calculate accuracy on one sample\n","def oneHellaSample(sample,model,tokenizer):\n","\n","  # 1) find context length and target\n","  context = sample['ctx']\n","  context_len =\n","  target =\n","\n","  # 2) loop over candidate endings and create prompts\n","  allprompts = []\n","  for opti in range(len(sample['endings'])):\n","    prompt = f\"\"\n","    allprompts.append(\n","\n","  # 3) batch tokenize with padding token\n","  prompt_tox = tokenizer(\n","\n","  # 4) run all prompts in one batch and bring the logits back to the CPU\n","  output = model\n","  logits = .cpu()\n","\n","  # 5) convert to log probabilities\n","  log_probs =\n","\n","  # 6) initialize and populate the log-likelihood vector\n","  loglikelihoods = np.zeros(len(sample['endings']))\n","\n","  # log-probs for each ending\n","  for opti in range(len(sample['endings'])):\n","\n","    # get the token for this ending\n","    token_seq = prompt_tox\n","\n","    # find the valid (non-pad) tokens\n","    valid_tox = np.where\n","\n","    # extract a log-softmax sequence for the valid tokens\n","    lsmSeq =\n","\n","    # average the valid ending log-probs\n","    loglikelihoods[opti] = np.mean(lsmSeq[\n","\n","  # 7) function outputs\n","  return loglikelihoods,target"],"metadata":{"id":"fKOegSpnAjuD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test it with one sample\n","loglikelihoods,target = oneHellaSample(dataset[42],gpt2_s,tokenizerG)\n","\n","print(f'Log-likelihoods: {loglikelihoods}')\n","print(f'Target ending: {target}')\n","\n","if np.argmax(loglikelihoods)==target:\n","  print('Model was correct!')\n","else:\n","  print('Model needs more training ;)')"],"metadata":{"id":"35xjTebcApRu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nOItVge9GNPM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 5: Test all four models**"],"metadata":{"id":"hzKfnZ1TApLY"}},{"cell_type":"code","source":["# number of data samples to test (set low on CPU!)\n","num_samples = 1000\n","\n","# initialize accuracy matrix\n","accuracies = np.zeros((4,num_samples))\n","\n","\n","# loop over data samples with progress bar\n","for datai in tqdm(range(num_samples),desc='Evaluating on HellaSwag'):\n","\n","  # extract one sample from the data\n","  example = dataset[datai]\n","\n","  # test GPT2 small\n","  loglikelihoods,target = oneHellaSample(example,gpt2_s,tokenizerG)\n","  if np.argmax(loglikelihoods)==target: accuracies[0,datai] = 1\n","\n","  # repeat for GPT2-large\n","\n","\n","  # repeat for roberta-small\n","\n","\n","  # repeat for roberta-large\n",""],"metadata":{"id":"9WGk0LlVAyfR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(8,3))\n","\n","# draw each bar at a time (for color and text labels)\n","for i in range(4):\n","\n","  # accuracy for this model\n","  acy =\n","\n","  # the bar\n","  plt.bar(,,color=plt.cm.plasma(2*acy/100))\n","\n","  # write the actual accuracy value\n","  plt.text(,,f'{acy:.1f}%',\n","           fontweight='bold',ha='center',va='bottom')\n","\n","# adjust the axies\n","plt.axhline(,linestyle='--',color='k',linewidth=.5,zorder=-10)\n","plt.gca().set(xticks=range(4),xticklabels=['GPT2 small','GPT2 large','RoBERTa small','RoBERTa large'],\n","              ylabel='Accuracy (%)')\n","plt.title('HellaSwag accuracy',y=1.1)\n","\n","plt.tight_layout()\n","plt.savefig('ch4_proj22_part5a.png')\n","plt.show()"],"metadata":{"id":"PfQq9RGLAybu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(12,4))\n","plt.plot()\n","plt.gca().set(xlabel='Data sample',ylabel='Average accuracy',yticks=np.linspace(0,1,5),\n","              yticklabels=['All wrong','1 correct','2 correct','3 correct','all correct'])\n","\n","plt.tight_layout()\n","plt.savefig('ch4_proj22_part5b.png')\n","plt.show()"],"metadata":{"id":"72SRh3DJpVGP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZV2gbfO0CY-e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 6: Statistical comparisons**"],"metadata":{"id":"SEq3mKvACY7M"}},{"cell_type":"code","source":["# initialize the results matrix\n","statsMat = np.zeros((4,4,2))\n","\n","# loop over all pairs\n","for i in range(4):\n","  for j in range(i+1,4):\n","\n","    # extract this pair of accuracies\n","    a = accuracies[i]\n","    b = accuracies[j]\n","\n","    # initialize and populate the McNemar table (diagonals are ignored in the test)\n","    table = np.zeros((2,2))\n","    table[0,1] = np.sum( & )\n","    table[1,0] = np.sum( & )\n","\n","    # run the McNemar test\n","    res = mcnemar( ,exact=False,correction=True)\n","\n","    # store the mean differences (% accuracy) and p-value\n","    statsMat[i,j,0] =\n","    statsMat[i,j,1] =\n","\n","statsMat[statsMat==0] = np.nan"],"metadata":{"id":"2StZlJ7RqvbE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# mean differences\n","statsMat[:,:,0]"],"metadata":{"id":"UtHRKcWTufu-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# show the matrix\n","plt.imshow()\n","plt.gca().set(xticks=range(4),yticks=range(4),\n","              xticklabels=['GPT-s','GPT-l','RoB-s','RoB-l'],yticklabels=['GPT-s','GPT-l','RoB-s','RoB-l'])\n","\n","# and add the labels\n","for i in range(4):\n","  for j in range(i+1,4):\n","\n","    # create the base label\n","    label = f'{statsMat[i,j,0]\n","\n","    # add * if its significant\n","    if statsMat[i,j,1]<.05/6:\n","\n","\n","    # draw the text\n","    plt.text(j,i,label,fontsize=16,fontweight='bold',color='k',ha='center',va='center')\n","\n","plt.tight_layout()\n","plt.savefig('ch4_proj22_part6.png')\n","plt.show()"],"metadata":{"id":"rYgDgaeptn3F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"V9gi--O5Z2-6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# btw, there are some linguistic imperfections in the HellaSwag dataset, e.g.,\n","dataset[2]"],"metadata":{"id":"Qkf9PjzO-RFV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4FpUNT_cItAm"},"execution_count":null,"outputs":[]}]}