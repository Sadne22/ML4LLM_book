{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMGXrs1f9xvQMjWIzJfOVCi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[16] Softmax probability distributions</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","source":[],"metadata":{"id":"R5SI-Iyy4dtm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn.functional as F\n","\n","from transformers import AutoModelForCausalLM, GPT2Tokenizer"],"metadata":{"id":"rn8Fcyf87EXx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300,\n","})"],"metadata":{"id":"dy4A-ah8kzZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nxqgPabM0FuN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 1: Softmax in numpy and PyTorch**"],"metadata":{"id":"xg66_Mew0Frx"}},{"cell_type":"code","metadata":{"id":"vmjUxlEqGbDu"},"source":["# the list of numbers\n","z = [1,1.1,2,3,5,6,6.1]\n","\n","# compute the softmax result\n","num = np.exp(z)\n","den = np.sum( np.exp(z) )\n","sm = num / den\n","\n","print(sm)\n","print(np.sum(sm))"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["zTorch = torch.tensor(z,dtype=torch.float32)\n","\n","# using a function\n","zTorch_sm = F.softmax(zTorch,dim=-1)\n","zTorch_sm"],"metadata":{"id":"nMluoPkty8LC"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sOug_tPzHY1y"},"source":["# compare\n","plt.figure(figsize=(10,5))\n","\n","plt.plot(z,sm,'ks-',markerfacecolor=[.9,.7,.7],markersize=10,label='Manual')\n","plt.plot(z,zTorch_sm,'bx:',markersize=8,label='PyTorch')\n","plt.legend()\n","\n","plt.gca().set(xlabel='Original number (z)',ylabel='Softmax probability $\\\\sigma (z)$',\n","              title='$\\\\sum\\\\sigma (z)$ = %g' %np.sum(sm))\n","\n","plt.tight_layout()\n","plt.savefig('ch4_proj16_part1.png')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xQGpJZXR0FmS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: Temperature**"],"metadata":{"id":"5FI0HF4C0FpD"}},{"cell_type":"code","source":["x = torch.linspace(-5,5,55)\n","\n","shapes = 'soh^'\n","\n","plt.figure(figsize=(10,5))\n","for i,temp in enumerate([.3,.6,1,1.4]):\n","  sm = F.softmax(x/temp,dim=-1)\n","  plt.plot(x,sm,shapes[i]+'-',linewidth=2,label='T = %g' %temp)\n","\n","plt.legend()\n","plt.gca().set(xlabel='Original number',ylabel='Softmax probability')\n","# plt.yscale('log') # FYI\n","\n","plt.tight_layout()\n","plt.savefig('ch4_proj16_part2.png')\n","plt.show()"],"metadata":{"id":"DEqcFxC50Fju"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VWOmGVIrGvM4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: Softmax of LLM output logits**"],"metadata":{"id":"PtnG5KKjGvJ9"}},{"cell_type":"code","source":["# load pretrained GPT-2 model and tokenizer\n","gpt2_small = AutoModelForCausalLM.from_pretrained('gpt2')\n","gpt2_large = AutoModelForCausalLM.from_pretrained('gpt2-large')\n","\n","# set to eval mode\n","gpt2_small.eval()\n","gpt2_large.eval()\n","\n","# and the tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"],"metadata":{"id":"4m885jVKGvHB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'GPT-2-small has {gpt2_small.num_parameters():,} parameters.')\n","print(f'GPT-2-large has {gpt2_large.num_parameters():,} parameters.')"],"metadata":{"id":"PiitTpg20Il3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["txt = 'It was a dark and stormy'\n","tokens = tokenizer.encode(txt,return_tensors='pt') # pt = PyTorch\n","tokens"],"metadata":{"id":"tJSJ7xWWgsJI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'The text comprises {tokens.shape[1]} tokens.\\n')\n","\n","for t in tokens[0]:\n","  print(f'{t:5} is \"{tokenizer.decode(t)}\"')"],"metadata":{"id":"UmrTUlsfhC3v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# forward pass through the model\n","outputs = gpt2_small(tokens)\n","outputs"],"metadata":{"id":"jFPKdkvuiB2E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outputs.logits.shape"],"metadata":{"id":"_W-rnqMSiBzP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logits = outputs.logits[0,-1,:].detach()\n","logits_sm = F.softmax(logits,dim=-1)\n","logits.shape"],"metadata":{"id":"4saM--Ikicvb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'The sum of the raw logits is {logits.sum():.3f}')\n","print(f'The sum of the softmax logits is {logits_sm.sum():.3f}')"],"metadata":{"id":"pWyqVnDXjC21"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot the raw and softmax logits\n","_,axs = plt.subplots(1,3,figsize=(12,3))\n","\n","axs[0].plot(logits,'ks',markerfacecolor=[.9,.7,.7,.3])\n","axs[0].set(xlim=[-10,tokenizer.vocab_size+9],xlabel='Token index',\n","           ylabel='Output logits',title='A) All final token logits')\n","\n","axs[1].plot(logits_sm,'o',markerfacecolor=[.7,.9,.7,.3])\n","axs[1].set(xlim=[-10,tokenizer.vocab_size+9],xlabel='Token index',\n","           ylabel='Probabilities',title='B) Softmax probabilities')\n","\n","axs[2].plot(logits,logits_sm,'^',markerfacecolor=[.7,.7,.9,.7])\n","axs[2].set(xlabel='\"Raw\" logits',ylabel='Softmax logits',\n","           title='C) Logits by probabilities')\n","\n","plt.tight_layout()\n","plt.savefig('ch4_proj16_part3.png')\n","plt.show()"],"metadata":{"id":"w6IB8lw0ikvU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find the maximum\n","max_logit = logits_sm.argmax()\n","print(f'The maximum softmax logit is #{max_logit} with a value of {logits_sm[max_logit]:.3f}')\n","print(f'The max word is \"{tokenizer.decode(max_logit)}\"')"],"metadata":{"id":"yqH00PEWiLoj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"o0KdHu6cGvEH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 4: Top 10 probabilities and temperature**"],"metadata":{"id":"h0KKyyHVGvA7"}},{"cell_type":"code","source":["k = 10\n","top_k = torch.topk(logits_sm,k)\n","\n","print(txt,'___\\n')\n","\n","for i in range(k):\n","  val = top_k.values[i]\n","  tok = top_k.indices[i]\n","  print(f'{tok:5} ({100*val:4.1f}%) is \"{tokenizer.decode(tok)}\"')"],"metadata":{"id":"JRqheArIGu9p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["temps = [ .5,1,1.5 ]\n","\n","plt.figure(figsize=(10,5))\n","\n","shapes = 'so^'\n","\n","for i,T in enumerate(temps):\n","\n","  # calculate softmax and find the top 10\n","  sm = F.softmax(logits/T,dim=-1)\n","  top_k = torch.topk(sm,k)\n","\n","  # plot\n","  color = [.7,.7,.7]\n","  color[i] = .9\n","  plt.plot(top_k.values,f'{shapes[i]}-',markerfacecolor=color,\n","           color=color,markeredgecolor='k',markersize=10,label=f'T = {T}')\n","\n","\n","plt.legend()\n","plt.gca().set(xlabel=f'Top-{k} indices',ylabel='Softmax probabilities (log)',yscale='log')\n","\n","plt.tight_layout()\n","plt.savefig('ch4_proj16_part4.png')\n","plt.show()"],"metadata":{"id":"6ImK880lGu6m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UgfuJihDIb0O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 5: Numerical instabilities and normalization**"],"metadata":{"id":"nQWQlIn3IbuX"}},{"cell_type":"code","source":["# get the outputs of the models\n","tokens = tokenizer.encode('A plethora of platypuses.',return_tensors='pt')\n","outputs_small = gpt2_small(tokens)\n","outputs_large = gpt2_large(tokens)"],"metadata":{"id":"vVFv6bJG7PhM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# grab the final token logit outputs\n","logits_small = outputs_small.logits[0,-1,:].detach()\n","logits_large = outputs_large.logits[0,-1,:].detach()\n","\n","_,axs = plt.subplots(1,3,figsize=(12,3.5))\n","\n","# gpt2 small\n","axs[0].plot(logits_small,'k.',alpha=.2)\n","axs[0].set(xlim=[-10,tokenizer.vocab_size+9],xlabel='Token index',ylabel='Output logits',title='A) GPT-2 SMALL')\n","\n","# gpt2 large\n","axs[1].plot(logits_large,'k.',alpha=.2)\n","axs[1].set(xlim=[-10,tokenizer.vocab_size+9],xlabel='Token index',ylabel='Output logits',title='B) GPT-2 LARGE')\n","\n","# against each other\n","axs[2].plot(logits_small,logits_large,'m.',alpha=.2)\n","axs[2].set(xlabel='GPT-2 SMALL',ylabel='GPT-2 LARGE',title='C) Comparison of both models')\n","\n","plt.tight_layout()\n","plt.savefig('ch4_proj16_part5a.png')\n","plt.show()"],"metadata":{"id":"HuxTwOnR7Pem"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# manual softmax\n","sm_manual_small = torch.exp(logits_small) / torch.sum(torch.exp(logits_small))\n","sm_manual_large = torch.exp(logits_large) / torch.sum(torch.exp(logits_large))\n","\n","_,axs = plt.subplots(1,3,figsize=(12,3.5))\n","\n","# gpt2 small\n","axs[0].plot(sm_manual_small,'k.',alpha=.2)\n","axs[0].set(xlim=[-10,tokenizer.vocab_size+9],xlabel='Token index',ylabel='Softmax probabilities',title='A) GPT-2 SMALL')\n","\n","# gpt2 large\n","axs[1].plot(sm_manual_large,'k.',alpha=.2)\n","axs[1].set(xlim=[-10,tokenizer.vocab_size+9],xlabel='Token index',ylabel='Softmax probabilities',title='B) GPT-2 LARGE')\n","\n","# against each other\n","axs[2].plot(sm_manual_small,sm_manual_large,'m.',alpha=.2)\n","axs[2].set(xlabel='GPT-2 SMALL',ylabel='GPT-2 LARGE',title='C) Comparison of both models')\n","\n","plt.tight_layout()\n","plt.savefig('ch4_proj16_part5b.png')\n","plt.show()"],"metadata":{"id":"ENUE4s5B7Pbw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logits_small[3000],sm_manual_small[1000]"],"metadata":{"id":"1pLnVPc0tvKy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# simple normalization (subtract max value)\n","logits_small_norm = logits_small - logits_small.max()\n","logits_large_norm = logits_large - logits_large.max()\n","\n","# visualize\n","_,axs = plt.subplots(1,3,figsize=(12,3.5))\n","\n","# gpt2 small\n","axs[0].plot(logits_small_norm,'k.',alpha=.2)\n","axs[0].set(xlim=[-10,tokenizer.vocab_size+9],xlabel='Token index',ylabel='Raw logits (max-norm)',title='A) GPT-2 SMALL')\n","\n","# gpt2 large\n","axs[1].plot(logits_large_norm,'k.',alpha=.2)\n","axs[1].set(xlim=[-10,tokenizer.vocab_size+9],xlabel='Token index',ylabel='Raw logits (max-norm)',title='B) GPT-2 LARGE')\n","\n","# against each other\n","axs[2].plot(logits_small_norm,logits_large_norm,'m.',alpha=.2)\n","axs[2].set(xlabel='GPT-2 SMALL',ylabel='GPT-2 LARGE',title='C) Comparison of both models')\n","\n","plt.tight_layout()\n","plt.savefig('ch4_proj16_part5c.png')\n","plt.show()"],"metadata":{"id":"5f1s4XI47PY-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# now repeat the manual softmax\n","sm_manual_smallN = torch.exp(logits_small_norm) / torch.sum(torch.exp(logits_small_norm))\n","sm_manual_largeN = torch.exp(logits_large_norm) / torch.sum(torch.exp(logits_large_norm))\n","\n","_,axs = plt.subplots(1,3,figsize=(12,3.5))\n","\n","# gpt2 small\n","axs[0].plot(sm_manual_smallN,'k.',alpha=.2)\n","axs[0].set(xlim=[-10,tokenizer.vocab_size+9],xlabel='Token index',ylabel='Softmax probabilities',title='A) GPT-2 SMALL')\n","\n","# gpt2 large\n","axs[1].plot(sm_manual_largeN,'k.',alpha=.2)\n","axs[1].set(xlim=[-10,tokenizer.vocab_size+9],xlabel='Token index',ylabel='Softmax probabilities',title='B) GPT-2 LARGE')\n","\n","# against each other\n","axs[2].plot(sm_manual_smallN,sm_manual_largeN,'m.',alpha=.2)\n","axs[2].set(xlabel='GPT-2 SMALL',ylabel='GPT-2 LARGE',title='C) Comparison of both models')\n","\n","plt.tight_layout()\n","plt.savefig('ch4_proj16_part5d.png')\n","plt.show()"],"metadata":{"id":"kJIyRzvPAQVP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pytorch softmax\n","sm_torch_small = F.softmax(logits_small,dim=-1)\n","sm_torch_large = F.softmax(logits_large,dim=-1)\n","\n","_,axs = plt.subplots(1,3,figsize=(12,3.5))\n","\n","# gpt2 small\n","axs[0].plot(sm_torch_small,'k.',alpha=.2)\n","axs[0].set(xlim=[-10,tokenizer.vocab_size+9],xlabel='Token index',ylabel='Softmax probabilities',title='GPT-2 SMALL')\n","\n","# gpt2 large\n","axs[1].plot(sm_torch_large,'k.',alpha=.2)\n","axs[1].set(xlim=[-10,tokenizer.vocab_size+9],xlabel='Token index',ylabel='Softmax probabilities',title='GPT-2 LARGE')\n","\n","# against each other\n","axs[2].plot(sm_torch_small,sm_torch_large,'m.',alpha=.2)\n","axs[2].set(xlabel='GPT-2 SMALL',ylabel='GPT-2 LARGE',title='Comparison of both models')\n","\n","plt.tight_layout()\n","plt.savefig('ch4_proj16_part5e.png')\n","plt.show()"],"metadata":{"id":"6puCsem-7PWU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wi_BGQGXAVVW"},"execution_count":null,"outputs":[]}]}