{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[20] Perplexity over time and text</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","source":[],"metadata":{"id":"R5SI-Iyy4dtm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import requests\n","from datasets import load_dataset\n","\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn.functional as F\n","\n","from transformers import AutoModelForCausalLM, AutoTokenizer"],"metadata":{"id":"rn8Fcyf87EXx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300,\n","})"],"metadata":{"id":"dy4A-ah8kzZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5TTsfDZ-J0cE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **What does perplexity mean?**"],"metadata":{"id":"vhRHd-SYyiVs"}},{"cell_type":"code","source":["# each list item is model outputs (logits)\n","situations = [\n","    [ 1,1,1,9 ],\n","    [ 1,1,1,2 ],\n","    [ 1,1,2,2 ],\n","    [ 3,1,1,2 ],\n","    [ 9,1,1,1 ] ]\n","\n","y = len(situations[0])-1 # final logit is the target category index\n","\n","# create a figure\n","plt.figure(figsize=(12,4))\n","xlabls = [] # x-axis tick labels\n","label_code = ['A','B','C','D','E'] # for lettering the x-axis labels\n","\n","for i,sit in enumerate(situations):\n","\n","  # raw model output (logits)\n","  model_output = torch.tensor([sit],dtype=torch.float32)\n","\n","  # log-softmax\n","  log_softmax = F.log_softmax(model_output,dim=-1)\n","\n","  # negative log-likelihood loss\n","  loss = -log_softmax[0,y]\n","\n","  # perplexity\n","  ppl = torch.exp(loss)\n","\n","  # draw the results\n","  plt.bar(np.array([.7,.9,1.1,1.3])+i,model_output[0].detach(),width=.2,edgecolor='k')\n","  plt.text(1.3+i,model_output[0,-1].detach()+.1,'Targ',font={'size':14},ha='center',va='bottom')\n","\n","  # x-axis tick label\n","  xlabls.append(f'\"{label_code[i]}\"\\nppl = {ppl.item():.3f}')\n","\n","\n","\n","plt.gca().set(title='Model outputs (logits) and perplexity',ylabel='Logits',\n","              xticks=range(1,len(situations)+1),xticklabels=xlabls,ylim=[0,10])\n","\n","plt.tight_layout()\n","plt.savefig('ch4_proj20_part0.png')\n","plt.show()"],"metadata":{"id":"xgqDtnspyiS6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oCKrjcexJ0ZA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 1: Perplexity over time**"],"metadata":{"id":"lxwdgLfpJ0WO"}},{"cell_type":"code","source":["# load pretrained GPT-2 model and tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('gpt2')\n","gpt2 = AutoModelForCausalLM.from_pretrained('gpt2')\n","gpt2.eval()"],"metadata":{"id":"D3Na1uKpKLgD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# connect to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","gpt2.to(device);"],"metadata":{"id":"zU40VRO-NHpG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import Frankenstein\n","url = 'https://www.gutenberg.org/cache/epub/84/pg84.txt'\n","text = requests.\n","print()"],"metadata":{"id":"BwMxM8uWLmgM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tokenize the book\n","tokens =\n","num_tokens =\n","print(f'Number of tokens: {num_tokens:,}')"],"metadata":{"id":"LD-B6io-MFKm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# max sequence length\n","seq_len =\n","print(f'Sequence length: {seq_len}')\n","\n","# how many samples fit into the data\n","nSegments =\n","print(f'Number of non-overlapping segments: {nSegments:,}')"],"metadata":{"id":"Pu8XHmADLwGC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["perplexities = np.zeros(nSegments)\n","\n","sum_losses = 0.\n","\n","for segi in tqdm(range(nSegments)):\n","\n","  # 1) start and end indices\n","  start =\n","  end = start +\n","\n","  # 2) extract the data and push to the GPU\n","  X = .to(device)\n","\n","  # 3) forward pass\n","  with torch.no_grad():\n","    outputs = gpt2\n","\n","  # 4) accumulated loss for later perplexity calculations\n","  sum_losses +=\n","\n","  # 5) per-segment perplexity\n","  perplexities[segi] = torch.exp(\n","\n","# 6) calculate perplexity\n","ave_perplexity = torch.exp("],"metadata":{"id":"vJiBkNjILMFu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,4))\n","\n","plt.plot(label='Segment perplexities')\n","plt.axhline(label='Perplexity of average')\n","plt.axhline(label='Average of perplexities')\n","\n","plt.legend()\n","plt.gca().set(xlabel='Segment position',ylabel='Perplexity',xlim=[-2,nSegments+2])\n","\n","plt.tight_layout()\n","plt.savefig('ch4_proj20_part1.png')\n","plt.show()"],"metadata":{"id":"eM0cSJD_LMC9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nSjZuEHDJ0TJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: A perplexity function**"],"metadata":{"id":"rk5sxMJkPbms"}},{"cell_type":"code","source":["def calc_perplex(tokens,model=gpt2,seq_len=gpt2.config.n_positions):\n","\n","  # number of segments in the total token sequence\n","  nSegments =\n","\n","  # initialize losses\n","  sum_losses = 0.\n","\n","  for i in range(nSegments):\n","\n","    # find start and end indices\n","    start =\n","    end =\n","\n","    # get the token sequence (with batch dimension)\n","    X =\n","\n","    # forward pass\n","    with torch.no_grad():\n","      outputs =\n","\n","    # calculate and store this batch's loss\n","    sum_losses +=\n","\n","  # after segments loop, perplexity = exp(average per-token losses over this segment)\n","  perplexity = torch.exp(  /  )\n","\n","  return perplexity"],"metadata":{"id":"STUQPhX3PrZy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["calc_perplex(tokens)"],"metadata":{"id":"fBtKpD7uPbjZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ave_perplexity"],"metadata":{"id":"ClPvXlciTayA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hFZLFB0aP39W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: Impact of sequence length**"],"metadata":{"id":"dxVzJRaPQDW_"}},{"cell_type":"code","source":["seq_lengths =\n","\n","perp_by_len = np.zeros(len(seq_lengths))\n","\n","for i in tqdm(range(len(seq_lengths))):\n","  perp_by_len[i] = calc_perplex(\n","\n","print('\\n')\n","for i in range(len(seq_lengths)):\n","  print(f'Sequence length {} has perplexity {}')"],"metadata":{"id":"xJ9rmXUkUzi-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,4))\n","\n","# min-max scaled\n","perpl_scaled =\n","\n","plt.bar(,,edgecolor='k',linewidth=.5,\n","        color=plt.cm.plasma(perpl_scaled))\n","plt.gca().set(title='Perplexities for different sequence lengths')\n","\n","plt.tight_layout()\n","plt.savefig('ch4_proj20_part3.png')\n","plt.show()"],"metadata":{"id":"LGbEq6BgUzgN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2YJSv_YNQDRP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 4: Perplexities in different texts**"],"metadata":{"id":"rz0sd3fAP2Lt"}},{"cell_type":"code","source":["# all books have the same url format; they are unique by numerical code\n","baseurl='https://www.gutenberg.org/cache/epub/'\n","\n","bookurls = [\n","    # code       title\n","    ['84',    'Frankenstein'    ],\n","    ['64317', 'GreatGatsby'     ],\n","    ['11',    'AliceWonderland' ],\n","    ['1513',  'RomeoJuliet'     ],\n","    ['76',    'HuckFinn'        ],\n","    ['219',   'HeartDarkness'   ],\n","    ['2591',  'GrimmsTales'     ],\n","    ['2148',  'EdgarAllenPoe'   ],\n","    ['36',    'WarOfTheWorlds'  ],\n","    ['829',   'GulliversTravels']\n","]"],"metadata":{"id":"-lIfoG_EUz5T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ppls = np.zeros(len(bookurls))\n","\n","for i,(code,title) in enumerate(bookurls):\n","\n","  # get the text tokens\n","  fullurl = baseurl + code + '/pg' + code + '.txt'\n","  text = requests.get(fullurl).text\n","  tokens = tokenizer.encode(text,return_tensors='pt')\n","\n","  # just the first 50k tokens for speed and direct comparison\n","  tokens =\n","\n","  # calculate perplexity\n","  ppls[i] = calc_perplex\n","\n","  print"],"metadata":{"id":"xq03qbxlUz2q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,4))\n","\n","# min-max scaling for coloring the bars\n","ppls_scaled = (ppls - ppls.min()) / (ppls.max() - ppls.min())\n","\n","# and draw the bars\n","\n","\n","plt.tight_layout()\n","plt.savefig('ch4_proj20_part4.png')\n","plt.show()"],"metadata":{"id":"uEiDKNcMYsrN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"d0-8B3ZSPbge"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 5: Perplexities in different models**"],"metadata":{"id":"pEg5cnmiPbdq"}},{"cell_type":"code","source":["# dictionary of modelname:identifier\n","model_ids = {\n","    'small':  'gpt2',        # 124M\n","    'medium': 'gpt2-medium', # 355M\n","    'large':  'gpt2-large',  # 774M\n","    'xl':     'gpt2-xl'      # 1.6B\n","}\n","\n","# load all models into a dictionary\n","models = {}\n","for name, id in model_ids.items():\n","  models[name] = AutoModelForCausalLM.from_pretrained(id).to(device)\n","  # switch to eval mode"],"metadata":{"id":"wwvDZ1nFx7Zv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["perp_models = np.zeros(len(models))\n","\n","for i,(name, model) in enumerate(models.items()):\n","  perp_models[i] = calc_perplex(,model=\n","  print(f'Perplexity of {} for GPT2-{}')"],"metadata":{"id":"39FZjKT_x7dX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,4))\n","\n","perp_models_scaled = (perp_models - perp_models.min()) / \\\n","                     (perp_models.max() - perp_models.min())\n","\n","\n","plt.tight_layout()\n","plt.savefig('ch4_proj20_part5.png')\n","plt.show()"],"metadata":{"id":"tIsXEASWx7gX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"eRCEeMWKiu7v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 6: Perplexities in different models (Wikitext)**"],"metadata":{"id":"NmRv8F79cDrG"}},{"cell_type":"code","source":["# note: over-writing 'tokens' from earlier\n","text = load_dataset('wikitext','wikitext-2-raw-v1',split='test')\n","\n","# join the text samples\n","tokens =\n","torch.numel(tokens)"],"metadata":{"id":"IEPQplsSUzuH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["perp_models = np.zeros(len(models))\n","\n","for i,(name, model) in enumerate(models.items()):\n","  perp_models[i] =\n","  print(f'Perplexity of"],"metadata":{"id":"zo0YNoWrivCc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,4))\n","\n","perp_models_scaled =\n","\n","plt.bar(range(len(perp_models)),perp_models,edgecolor='k',linewidth=.5,\n","        color=plt.cm.plasma(perp_models_scaled))\n","plt.gca().set(xticks=range(len(perp_models)),xticklabels=list(models.keys()),\n","              ylabel='Perplexity',title='Perplexities of different GPT2 models (Wikitext)')\n","\n","plt.tight_layout()\n","plt.savefig('ch4_proj20_part6.png')\n","plt.show()"],"metadata":{"id":"pGZslnsPiu40"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7bprODNAnZlA"},"execution_count":null,"outputs":[]}]}