{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[35] Raw and softmax attention scores</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BvQj17hzqwM"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","import torch\n","from transformers import AutoModelForCausalLM, GPT2Tokenizer\n","import torch.nn.functional as F"]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300\n","})"],"metadata":{"id":"dy4A-ah8kzZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bvbupe_p4T5L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 1: The reason to scale by sqrt(d_h)**"],"metadata":{"id":"o_cesTFr5AU-"}},{"cell_type":"code","source":["matsizes = np.arange(5,101)\n","\n","plt.figure(figsize=(10,4))\n","\n","for n in matsizes:\n","\n","  # create the matrices\n","  M1 =  @\n","  M2 = M1 /\n","\n","  # plot\n","  plt.plot(n,,'kh',markerfacecolor=[.9,.7,.7,.5],markersize=10)\n","  plt.plot(n,,'ks',markerfacecolor=[.7,.9,.7,.5],markersize=10)\n","\n","\n","plt.plot(matsizes,(),'r')\n","plt.gca().set(xlabel='Matrix size',ylabel='Matrix standard deviation')\n","plt.legend(['No scaling','$\\\\sqrt{N}$ scaling','Theory'])\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj35_part1.png')\n","plt.show()"],"metadata":{"id":"b1IC5Cmw5AST"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DugL7dpykd_h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: Model, tokens, QVK activations**"],"metadata":{"id":"oGIKYsGKEEO-"}},{"cell_type":"code","source":["# load GPT2 model and tokenizer\n","model = AutoModelForCausalLM.from_pretrained('gpt2-medium')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","model.eval()"],"metadata":{"id":"vEwYHoqWz0nB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hook the attention vectors\n","activations = {}\n","\n","def hook(module,input,output):\n","  activations[keyName] = output.detach()\n","\n","# implant the hooks\n","whichlayer = 6\n","keyName = f'attn_{whichlayer}'\n","hookhandle = model.transformer....register_forward_hook(hook)"],"metadata":{"id":"nUpcb0WgR0J5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# https://en.wikipedia.org/wiki/Purple\n","txt = \"Purple is a color similar in appearance to violet light. In the RYB color model historically used in the arts, purple is a secondary color created by combining red and blue pigments. In the CMYK color model used in modern printing, purple is made by combining magenta pigment with either cyan pigment, black pigment, or both. In the RGB color model used in computer and television screens, purple is created by mixing red and blue light in order to create colors that appear similar to violet light. According to color theory, purple is considered a cool color.\"\n","\n","# tokenize\n","tokens = tokenizer.encode(txt,return_tensors='pt')\n","ntokens =\n","\n","# run through the model\n","with torch.no_grad():\n","  model"],"metadata":{"id":"ltyzzBQ6R0HV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# checking sizes\n","print(activations.keys())\n","print(activations[keyName].shape)"],"metadata":{"id":"BD0Wb8YdMBqZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oDDh2AEr9Hss"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: Split into heads**"],"metadata":{"id":"5w4npn-M9Hop"}},{"cell_type":"code","source":["# some helpful variables\n","n_layers = model.config.\n","n_emb = model.config.\n","n_heads = model.config.\n","head_dim = model.config. //\n","sqrtD =\n","\n","print(f'There are {n_heads} heads, each with {head_dim} dimensions.')"],"metadata":{"id":"vt807ZdTR0Mq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# first, separate the Q,K,V matrices\n","Q,K,V = torch.split(\n","Q.shape"],"metadata":{"id":"JGs7C-adPhDN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# now split into heads\n","Q_h = torch.split(Q,,dim=)\n","K_h = torch.split(K,,dim=)\n","\n","print(f'There are {len(Q_h)} heads')\n","print(f'Each head has size {Q_h[2].shape}')"],"metadata":{"id":"Xd7TpP8VCH-O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualize\n","_,axs = plt.subplots(4,4,figsize=(12,6))\n","\n","for i,ax in enumerate(axs.flatten()):\n","  ax.pcolor(,cmap='plasma',vmin=-2,vmax=2)\n","  ax.text(2,head_dim-1,f'Qh{i}',fontsize=12,fontweight='bold',color='k',ha='left',va='top')\n","  ax.text(1,head_dim-2,f'Qh{i}',fontsize=12,fontweight='bold',color='w',ha='left',va='top')\n","  ax.set(xticks=[],yticks=[])\n","\n","# finalize\n","axs[3,0].set(ylabel='Head dim',xlabel='Token position')\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj35_part3.png')\n","plt.show()"],"metadata":{"id":"1TGYXzSIOJg4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"J6xc3RYWR0BI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 4: Raw attention scores in one layer**"],"metadata":{"id":"JIX1EovRCoS0"}},{"cell_type":"code","source":["# initializations\n","withinhead_dp = np.array([])\n","acrosshead_dp = np.array([])\n","\n","# loop over pairs of heads\n","for qi in range(n_heads):\n","  for ki in range(n_heads):\n","\n","    # dot product for last token in Q with all previous tokens in K (excluding first token)\n","    dp = Q_h[][] @ K_h[][].t() /\n","    dp =  # numpy will convert from pytorch, but this is cleaner\n","\n","    # store in the appropriate matrix\n","    if qi==ki:\n","      withinhead_dp = np.concatenate((,))\n","    else:\n","      acrosshead_dp = np.concatenate((,))\n","\n","print(f'There are {len(acrosshead_dp)} values in \"across head\"')\n","print(f'      and {len(withinhead_dp):5} values in \"within head\".')"],"metadata":{"id":"ia6jLKBNCoIK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## visualizations\n","_,axs = plt.subplots(1,2,figsize=(10,4))\n","\n","# plot the raw data\n","axs[0].plot(,,'ko',linewidth=.1,markerfacecolor=[.7,.9,.7,.3],markersize=3)\n","axs[0].plot(,,'ks',linewidth=.1,markerfacecolor=[.9,.7,.7,.3],markersize=3)\n","\n","# and the violin plot\n","v = axs[0].violinplot()\n","\n","# change the colors\n","v['bodies'][0].set_facecolor([.7,.9,.7])\n","v['bodies'][1].set_facecolor([.9,.7,.7])\n","v['bodies'][0].set_alpha([.9])\n","v['bodies'][1].set_alpha([.9])\n","v['cbars'].set_edgecolor('k')\n","v['cmins'].set_edgecolor('k')\n","v['cmaxes'].set_edgecolor('k')\n","\n","axs[0].axhline(0,linestyle='--',color=[.7,.7,.7],zorder=-3)\n","axs[0].set(xticks=[1,2],xticklabels=['Same head','Diff heads'],\n","              ylabel='QK$^\\\\top$ dot products',title='A) Raw attention scores',xlim=[.5,2.5])\n","\n","\n","# distributions\n","y,x = np.histogram()\n","axs[1].plot(x[:-1],y,'g',linewidth=2,label='Same head')\n","\n","y,x = np.histogram()\n","axs[1].plot(x[:-1],y,'r',linewidth=2,label='Diff heads')\n","\n","axs[1].legend()\n","axs[1].set(xlabel='Dot product value',ylabel='Density',title='B) Distributions')\n","axs[1].axvline(0,linestyle='--',color=[.7,.7,.7])\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj35_part4.png')\n","plt.show()"],"metadata":{"id":"sfdE3heyW4M8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-5lJDpm6U99G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 5: Attention score distributions over layers**"],"metadata":{"id":"lrj3TwmWU950"}},{"cell_type":"code","source":["hookhandle.remove()"],"metadata":{"id":"6WQjOGUwWcDj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hook the attention vectors\n","activations = {}\n","\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","    activations[f'attn_L{layer_number}'] =\n","  return hook\n","\n","# implant the hooks\n","handles = []\n","for i in range(n_layers):\n","  h = model.transformer.h[i].attn.c_attn.register_forward_hook(implant_hook(i))\n","  handles.append(h)"],"metadata":{"id":"CLd2HavwWKi-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# run through the model\n","with torch.no_grad():\n","  model(tokens)"],"metadata":{"id":"9lWszak-WKgD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["activations.keys()"],"metadata":{"id":"TAxVpCvpWKdP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["histedges = np.linspace()\n","\n","layerHists = np.zeros((,,))\n","\n","\n","# loop over layers\n","for layeri in range(n_layers):\n","\n","  # get the activations\n","  Q,K,V = torch.split(\n","  Q_h = torch.split(Q,)\n","  K_h = torch.split(K,)\n","\n","\n","  # re-initialize\n","  withinhead_dp = np.array([])\n","  acrosshead_dp = np.array([])\n","\n","  # loop over pairs of heads\n","  for qi in range(n_heads):\n","    for ki in range(n_heads):\n","\n","      # dot product for last token in Q with all previous tokens in K (excluding first token)\n","      dp =\n","      dp =  # pytorch -> numpy\n","\n","      # store in the appropriate matrix\n","      if qi==ki:\n","        withinhead_dp =\n","      else:\n","        acrosshead_dp =\n","\n","\n","  # distributions\n","  y,_ = np.histogram(withinhead_dp,bins=,density=)\n","  layerHists[layeri,0,:] = y\n","\n","  y,_ = np.histogram()\n","  layerHists[layeri,1,:] = y"],"metadata":{"id":"I7jNp4EMW1nv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(10,4))\n","axs[0].imshow(,origin='lower',aspect='auto',cmap='magma',\n","              extent=[histedges[0],histedges[-1],0,n_layers],vmin=0,vmax=.15)\n","axs[0].axvline(0,linestyle='--',color='w',linewidth=.4)\n","\n","axs[1].imshow(,origin='lower',aspect='auto',cmap='magma',\n","              extent=[histedges[0],histedges[-1],0,n_layers],vmin=0,vmax=.15)\n","\n","axs[0].set(xlabel='$\\\\mathbf{QK^\\\\top}$ activation value',ylabel='Transformer layer',title='A) Within heads')\n","axs[1].set(xlabel='$\\\\mathbf{QK^\\\\top}$ activation value',ylabel='Transformer layer',title='B) Across heads')\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj35_part5a.png')\n","plt.show()"],"metadata":{"id":"DBfsxwzqW1kE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get the means and standard deviations\n","y0  = layerHists[:,0,:].mean(axis=\n","y0e = layerHists[:,0,:].std\n","y1  =\n","y1e =\n","\n","plt.figure(figsize=(10,4))\n","\n","plt.fill_between(histedges[:-1],y0-y0e,y0+y0e,color=[.7,.7,.9,.7],label='Within heads')\n","plt.plot(histedges[:-1],y0,'b')\n","plt.fill_between(histedges[:-1],y1-y1e,y1+y1e,color=[.9,.7,.7,.7],label='Across head')\n","plt.plot(histedges[:-1],y1,'r')\n","\n","plt.legend()\n","plt.gca().set(xlabel='$\\\\mathbf{QK^\\\\top}$ activation value',ylabel='Density',xlim=histedges[[0,-1]])\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj35_part5b.png')\n","plt.show()"],"metadata":{"id":"YhIao2qtVWEv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KLUIyhk0W1hF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 6: Laminar distributions of raw and softmax scores**"],"metadata":{"id":"l1HrBc98fjgU"}},{"cell_type":"code","source":["_,axs = plt.subplots(1,3,figsize=(12,3))\n","\n","\n","# normalization for mapping line colors to colorbar\n","cmap = plt.cm.plasma\n","norm = mpl.colors.Normalize(vmin=0,vmax=n_layers)\n","\n","\n","# loop over all the layers\n","for layeri in range(n_layers):\n","\n","  # split the matrices\n","  Q,K,V = torch.split(activations[f'attn_L{layeri}'],n_emb,dim=-1)\n","\n","  # calculate the attention activations\n","  Qh = Q.view().permute(0,2,1,3)\n","  Kh = K.view(\n","  qkt = (Qh @ Kh.\n","\n","  # plot the average QK^T scores\n","  axs[0].errorbar(layeri,,,color=plt.cm.plasma(layeri/n_layers))\n","  axs[0].plot(layeri,,'kh',markersize=10,\n","              markerfacecolor=plt.cm.plasma(layeri/n_layers))\n","\n","  # distribution of \"raw\" values\n","  y,x = torch.histogram(,torch.linspace(-15,15,201),density=True)\n","  axs[1].plot(x[:-1],y,color=cmap(norm(layeri)),label=f'Layer {layeri}')\n","\n","  # distribution of softmax-prob values\n","  y,x = torch.histogram(,201,density=True)\n","  axs[2].plot(x[:-1],y,color=plt.cm.plasma(layeri/n_layers),label=f'Layer {layeri}')\n","\n","\n","# plot adjustments\n","axs[0].set(xlabel='Transformer layer',ylabel='Activation mean',title='A) Means of $\\\\mathbf{QK^\\\\top}$')\n","axs[1].set(xlabel='Activation value',ylabel='Density',title='B) Distributions of $\\\\mathbf{QK^\\\\top}$')\n","axs[2].set(xlabel='Softmax probability',ylabel='Density (log scale)',yscale='log',xlim=[0,1],\n","           title='C) Distributions of $\\\\sigma(\\\\mathbf{QK^\\\\top})$')\n","\n","# create a colorbar\n","sm = mpl.cm.ScalarMappable(cmap=cmap,norm=norm)\n","cbar = plt.colorbar(sm,ax=axs[-1],pad=.02)\n","cbar.set_label('Transformer layer')\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj35_part6.png')\n","plt.show()"],"metadata":{"id":"sWv0bd0z28sd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"tDMCHoIzwfFz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 7: Softmax probabilities for self- vs. cross-attention**"],"metadata":{"id":"-2EluHa3Bn-K"}},{"cell_type":"code","source":["# create a mask with 0/1\n","N = 5\n","M =\n","\n","# open a figure\n","fig,axs = plt.subplots(1,3,figsize=(10,3))\n","\n","# show the mask\n","axs[0].imshow(M,vmin=0,vmax=1)\n","axs[0].set(title='A) Binary mask (0 or 1)')\n","\n","# replace ones with -inf\n","M[M==1] =\n","axs[1].imshow(M,vmin=0,vmax=1)\n","axs[1].set(title='B) Time-causal mask (M)')\n","\n","# impact of softmax on mask values\n","h = axs[2].imshow(,vmin=0,vmax=1)\n","axs[2].set(title='C) Impact of softmax')\n","fig.colorbar(h,ax=axs[2],pad=.01)\n","\n","# adjustments for all axes\n","for a in axs:\n","  a.set(xlabel='Token position',ylabel='Token position')\n","  a.set_xticks(np.arange(.5,N,1),minor=True)\n","  a.set_yticks(np.arange(.5,N,1),minor=True)\n","  a.grid(which='minor')\n","  a.spines['top'].set_visible(True) # switched off by default, but helpful here\n","  a.spines['right'].set_visible(True)\n","\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj35_part7a.png')\n","plt.show()"],"metadata":{"id":"QQEcKQAT5U9B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get the activations\n","layeri = 6\n","Q,K,V = torch.split(,n_emb,dim=1)\n","Q_h =\n","K_h =\n","\n","# empty initializations\n","final2prev = np.array([])\n","selfAttend = np.array([])\n","first2self = np.array([])\n","\n","\n","# loop over heads\n","for qi in range(n_heads):\n","\n","  # raw attention scores with mask\n","  attn_scores =  @ .t()) /\n","  pastmask = torch.triu(torch.ones(,),1)\n","  pastmask[pastmask==1] =\n","  attn_scores +=\n","\n","\n","  # softmax\n","  attn_sm = F.softmax( attn_scores ,\n","\n","  # the final token with all previous tokens (including the first but excluding self-attn)\n","  final_with_prev =\n","\n","  # matching tokens are self-attention\n","  matching_toks =  # exclude the first token in the sequence\n","  first_selfTok =   # isolate the first token\n","\n","  # add to dataset\n","  final2prev = np.concatenate((final2prev,final_with_prev.numpy()))\n","  selfAttend = np.concatenate(())\n","  first2self = np.concatenate(())\n","\n","\n","## visualize\n","plt.figure(figsize=(8,4))\n","\n","plt.plot(np.random.normal(1,.04,len(final2prev)),,'ko',linewidth=.1,markerfacecolor=[.7,.9,.7,.2],markersize=3)\n","plt.plot(np.random.normal(2,.04,len(selfAttend)),,'ks',linewidth=.1,markerfacecolor=[.9,.7,.7,.2],markersize=3)\n","plt.plot(np.random.normal(3,.04,len(first2self)),,'ks',linewidth=.1,markerfacecolor=[.7,.7,.9,.2],markersize=3)\n","\n","v = plt.violinplot([])\n","\n","# change the colors\n","v['bodies'][0].set_facecolor([.7,.9,.7])\n","v['bodies'][1].set_facecolor([.9,.7,.7])\n","v['bodies'][0].set_alpha([.9])\n","v['bodies'][1].set_alpha([.9])\n","v['cmins'].set(linewidth=.5,edgecolor='k')\n","v['cbars'].set(linewidth=.5,edgecolor='k')\n","v['cmaxes'].set(linewidth=.5,edgecolor='k')\n","\n","plt.gca().set(xticks=[1,2,3],ylabel='Softmax attention weight',xlim=[.5,3.5],\n","              xticklabels=['Final to\\nprev','Self-attention\\nother tokens','Self-attention\\nfirst token'])\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj35_part7b.png')\n","plt.show()"],"metadata":{"id":"Fv3k52SW6k2q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"G401BYtVRz-G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 8: Laminar softmax, self- vs. cross-attention**"],"metadata":{"id":"tS3nqigFdeFu"}},{"cell_type":"code","source":["binedges = np.linspace(0,1,91)\n","\n","attn_hists = np.zeros((n_layers,3,len(binedges)-1))\n","\n","\n","for layeri in range(n_layers):\n","\n","\n","  # empty initializations\n","  final2prev = np.array([])\n","  selfAttend = np.array([])\n","  first2self = np.array([])\n","\n","\n","  # get the activations\n","  Q,K,V = torch.split(activations[f'attn_L{layeri}'][0,:,:],n_emb,dim=1)\n","  Q_h = torch.split(Q,head_dim,dim=1)\n","  K_h = torch.split(K,head_dim,dim=1)\n","\n","\n","  # loop over heads\n","  for qi in range(n_heads):\n","\n","    # raw attention scores with mask\n","    attn_scores =\n","    pastmask =\n","    attn_scores[pastmask==0] =\n","\n","    # softmax\n","    attn_sm =\n","\n","    # the final token with all previous tokens (including the first but excluding self-attn)\n","    final_with_prev =\n","\n","    # matching tokens are self-attention\n","    matching_toks =  # exclude the first token in the sequence\n","    first_selfTok =   # isolate the first token\n","\n","    # add to dataset\n","    final2prev =\n","    selfAttend =\n","    first2self =\n","\n","\n","  # get histograms\n","  attn_hists[layeri,0,:],_ = np.histogram(final2prev,bins=binedges,density=True)\n","  attn_hists[layeri,1,:],_ = np.histogram(selfAttend,bins=binedges,density=True)\n","  attn_hists[layeri,2,:],_ = np.histogram(first2self,bins=binedges,density=True)\n"],"metadata":{"id":"T_uhKrCjdeFu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,3,figsize=(12,3.5))\n","\n","titels = [ 'A) Final to previous','B) Self-attention','C) First to self' ]\n","\n","for i in range(3):\n","  axs[i].imshow(attn_hists[:,i,:],aspect='auto',origin='lower',\n","                extent=[binedges[0],binedges[-1],0,n_layers],vmin=0,vmax=.5,cmap='magma')\n","  axs[i].set(xlabel='Softmax attention prob',ylabel='Transformer layer',title=titels[i])\n","\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj35_part8.png')\n","plt.show()"],"metadata":{"id":"TsyTMlMaemgT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jQgTXt3HemdX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 9: Averaging within vs. across heads**"],"metadata":{"id":"HhXgsj2HtkM4"}},{"cell_type":"code","source":["# get the activations\n","Q,K,V = torch.split(activations['attn_L6'][0,:,:],n_emb,dim=1)\n","Q_h = torch.split(Q,head_dim,dim=1)\n","K_h = torch.split(K,head_dim,dim=1)\n","\n","\n","# ignoring heads\n","qkt_allheads =\n","\n","# separate per head\n","qkt_eachhead = torch.zeros((n_heads,ntokens,ntokens))\n","for headi in range(n_heads):\n","  qkt_eachhead[headi,:,:] =\n","\n","print(f'Size of QKt all heads: {list(qkt_allheads.shape)}')\n","print(f'Size of QKt per head: {list(qkt_eachhead.shape)}')\n","\n","# plot their relationship\n","_,axs = plt.subplots(1,2,figsize=(10,4))\n","axs[0].plot(.flatten(),.flatten(),\n","         'ko',markerfacecolor='y',alpha=.2)\n","\n","axs[0].set(xlabel='$QK^\\\\top$ ignoring heads',ylabel='Average of per-head $QK^\\\\top$',\n","           title='A) Scatter plot of the two calculations')\n","axs[0].grid(linestyle='--',color='k',linewidth=.4)\n","\n","# plot the differences\n","axs[1].plot(-,\n","         'ko',markerfacecolor='m',alpha=.2)\n","axs[1].set(xlabel='$QK^\\\\top$ matrix index',ylabel='Difference',\n","           title='B) Differences between calculations')\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj35_part9.png')\n","plt.show()"],"metadata":{"id":"wPJnP4ljtkJ-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# a little arithmetic demo (comment one of the lines)\n","np.mean([1,2,3,4]), (np.mean([1,2])+np.mean([3,4]))/2 # equivalent b/c balanced sample size\n","np.mean([1,2,3,4,5]), (np.mean([1,2])+np.mean([3,4,5]))/2 # unbalanced sample size breaks equivalence"],"metadata":{"id":"PYi1fwWruv75"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PepFq_ea2aRb"},"execution_count":null,"outputs":[]}]}