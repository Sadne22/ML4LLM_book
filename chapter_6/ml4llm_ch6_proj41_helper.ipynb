{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[41] Patching heads in IOI</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OxmEbIoa-yv0"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.gridspec import GridSpec\n","\n","from tqdm import tqdm\n","\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","import torch"]},{"cell_type":"code","source":["### Run this cell only if you're using \"dark mode\"\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300\n","})"],"metadata":{"id":"dy4A-ah8kzZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0JaacnaV4Bu6"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# **Part 1: The IOI task** (from Project 32)"],"metadata":{"id":"ny-nH3gniySB"}},{"cell_type":"code","source":["# Note: I didn't remove any code for Part 1. Just run the cells and enjoy :)"],"metadata":{"id":"2INpRwHYWq36"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\n","model = GPT2LMHeadModel.from_pretrained('gpt2-large').to(device)\n","model.eval()"],"metadata":{"id":"pFyxDs3UaDiw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_ME = 'When Mike and Emma went to the cafe, Mike gave a coffee to'\n","text_EM = 'When Mike and Emma went to the cafe, Emma gave a coffee to'\n","\n","target_M = tokenizer.encode(' Mike')[0]\n","target_E = tokenizer.encode(' Emma')[0]\n","\n","tokensME = tokenizer.encode(text_ME,return_tensors='pt').to(device)\n","tokensEM = tokenizer.encode(text_EM,return_tensors='pt').to(device)\n","\n","nbatches,ntokens = tokensME.shape"],"metadata":{"id":"iKaNCGyPunU1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","  outME = model(tokensME)\n","  outEM = model(tokensEM)"],"metadata":{"id":"xjEr-A4M4i6I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# predicted next words\n","nextword_ME = torch.argmax(outME.logits[0,-1,:])\n","nextword_EM = torch.argmax(outEM.logits[0,-1,:])\n","\n","print(f'{text_ME} \"{tokenizer.decode(nextword_ME)}\"')\n","print(f'{text_EM} \"{tokenizer.decode(nextword_EM)}\"')"],"metadata":{"id":"r03kmOx24l4T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logsm_ME = torch.log_softmax(outME.logits[0,-1,:].detach().cpu(),dim=0)\n","logsm_EM = torch.log_softmax(outEM.logits[0,-1,:].detach().cpu(),dim=0)\n","\n","\n","# setup the figure\n","fig = plt.figure(figsize=(12,3))\n","gs = GridSpec(1,5,figure=fig)\n","ax1 = fig.add_subplot(gs[:2])\n","ax2 = fig.add_subplot(gs[2:4])\n","ax3 = fig.add_subplot(gs[-1])\n","\n","# plot log-sm from \"EM\" sentence\n","ax1.plot(target_M,logsm_EM[target_M],'go',label='\"Mike\"')\n","ax1.plot(target_E,logsm_EM[target_E],'rs',label='\"Emma\"')\n","ax1.plot(logsm_EM,'k.',alpha=.2)\n","ax1.legend(fontsize=8)\n","ax1.set(xlabel='Vocab index',ylabel='Log softmax',\n","           title=text_EM[-21:]+'...',xlim=[-100,tokenizer.vocab_size+100])\n","\n","# plot log-sm from \"ME\" sentence\n","ax2.plot(target_M,logsm_ME[target_M],'go',label='\"Mike\"')\n","ax2.plot(target_E,logsm_ME[target_E],'rs',label='\"Emma\"')\n","ax2.plot(logsm_ME,'k.',alpha=.2)\n","ax2.legend(fontsize=8)\n","ax2.set(xlabel='Vocab index',ylabel='Log softmax',\n","           xlim=[-100,tokenizer.vocab_size+100],title=text_ME[-21:]+'...')\n","\n","# how they relate to each other\n","ax3.plot(logsm_ME,logsm_EM,'k.',alpha=.3)\n","ax3.set(xlabel='ME log-sm logits',ylabel='EM log-sm logits',title='ME vs. EM')\n","\n","# this figure is a replication from project 32 and is not shown in the book\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"hGp6mBtf4l0F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["IOI_score_ME = outME.logits[0,-1,target_M] - outME.logits[0,-1,target_E]\n","IOI_score_EM = outEM.logits[0,-1,target_M] - outEM.logits[0,-1,target_E]\n","\n","print(f'IOI score for text \"ME\": {IOI_score_ME:6.3f}')\n","print(f'IOI score for text \"EM\": {IOI_score_EM:6.3f}')"],"metadata":{"id":"kNJ73slf4lwz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# some useful variables\n","n_heads = model.config.n_head\n","n_layers = model.config.n_layer\n","n_embd = model.config.n_embd\n","head_dim = n_embd // n_heads"],"metadata":{"id":"n87U_QvJCRB_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UfbxMSCJ-MKw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: Impact of single-head permutation on IOI**"],"metadata":{"id":"5eMS9_iAjcje"}},{"cell_type":"code","source":["# 1) create a matrix\n","Ho = torch.arange(12).reshape(3,4)\n","\n","# 2) vectorize\n","H =\n","\n","# 3) random permute\n","H = H[\n","\n","# 4) reshape back\n","H = H.view\n","\n","# print\n","print('Original:\\n',Ho)\n","print('\\nRandomized:\\n',H)"],"metadata":{"id":"B23iDPQ26OaH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# can also do it in one line\n","H = Ho.reshape()[].reshape()\n","H"],"metadata":{"id":"q38Ud4217TCc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initializations\n","IOI_scores = np.zeros((,))\n","headnorms = np.zeros((,))\n","\n","# loop over layers\n","for layeri in tqdm(range\n","  for headi in range\n","\n","\n","    ### --- silence this head --- ###\n","    def hook(module,input):\n","      head_tensor = input[0].view(nbatches,ntokens,n_heads,head_dim) # reshape to index one head\n","      H = head_tensor[,,,].flatten()                         # isolate and vectorize one head\n","      H = H[]                                  # randomly permute the elements\n","      head_tensor[:,:,headi,:] = H.view(,,)   # reshape and replace\n","      head_tensor = head_tensor.view(,,)        # reshape back to tensor\n","      input = (head_tensor,*input[1:])                               # repackage the tuple\n","      headnorms[headi,layeri] = np.linalg.norm()      # norm of head in numpy\n","      return input\n","\n","    # implant the hook\n","    handle = model.transformer.h[layeri].attn.c_proj.register_forward_pre_hook(hook)\n","\n","\n","    ### --- forward pass --- ###\n","    with torch.no_grad():\n","      outEM = model(\n","\n","    # remove the hook\n","\n","\n","\n","    ### --- calculate IOI score --- ###\n","    IOI_scores[headi,layeri] =  -"],"metadata":{"id":"giaEdthGum8d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,axs = plt.subplots(1,3,figsize=(12,3))\n","\n","# image data to show\n","I = IOI_scores -\n","\n","# find color limits based on L1-mean\n","clim =\n","\n","# create the image\n","h = axs[0].pcolor(range(),range(),I,vmin=-clim,vmax=clim,cmap='RdBu_r')\n","axs[0].set(xlabel='Transformer layer',ylabel='Attention head index',yticks=range(0,n_heads,2),\n","           title='A) $\\\\Delta$ IOI (silenced - clean)')\n","c = fig.colorbar(h,ax=axs[0],pad=.02)\n","c.ax.tick_params(labelsize=8)\n","axs[0].spines.top.set_visible(True)\n","axs[0].spines.right.set_visible(True)\n","\n","# create the error bar plot\n","Imean =\n","Istd =\n","axs[1].errorbar()\n","axs[1].plot()\n","axs[1].axhline()\n","axs[1].set(xlabel='Transformer layer',ylabel='Average $\\\\Delta$ IOI',title='B) Head-average results')\n","\n","axs[2].scatter(,,30,c=,edgecolor='k',\n","               linewidth=.4,cmap='RdBu_r',vmin=-clim,vmax=clim,alpha=.7)\n","axs[2].set(xlabel='Head norm',ylabel='$\\\\Delta$IOI',title='C) Head norm vs. IOI impact')\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj41_part2.png')\n","plt.show()"],"metadata":{"id":"zN6ejBv9-MCZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gl3S0nq0-L_m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: Get all ME-related attention activations**"],"metadata":{"id":"Y777iAFdRpYP"}},{"cell_type":"code","source":["# Define a hook function to store QVK vectors\n","MEattn = {}\n","\n","def implant_hook(layer_number):\n","  def hook(module,input):\n","    MEattn[f'L{layer_number}'] = input[0].view(,,,).detach()\n","  return hook\n","\n","\n","# implant the hooks\n","handles = []\n","for i in range(n_layers):\n","  h = model.transformer.h[i].attn.c_proj.register_forward_pre_hook(implant_hook(i))\n","  handles.append(h)\n","\n","# run the clean model to get all the activations\n","with torch.no_grad():\n","  outME = model(tokensME)\n","\n","# remove the hooks to avoid risk of overwriting in the experiment\n"],"metadata":{"id":"xYfe37VPR00y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for v,p in MEattn.items():\n","  print()"],"metadata":{"id":"lDEfTXC4RpVq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# some visualizations\n","_,axs = plt.subplots(1,ntokens,figsize=(14,3))\n","for i in range(ntokens):\n","  I = MEattn['L4'][\n","  cmin,cmax = np.percentile\n","  axs[i].imshow(,origin='lower',aspect='auto',cmap='plasma',vmin=cmin,vmax=cmax)\n","  axs[i].set(title=f'\"{tokenizer.decode(tokensME[0,i])}\"',xticklabels=[],yticklabels=[])\n","\n","axs[0].set(xlabel='Head',ylabel='Dimension')\n","plt.suptitle('Head activation heatmaps from layer 4',fontsize=16,fontweight='bold')\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj41_part3a.png')\n","plt.show()"],"metadata":{"id":"Ghau7F1KV1pG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["headnorms = torch.zeros((n_layers,ntokens))\n","\n","# average of attention head vector norms per token and layer\n","for layeri in range(n_layers):\n","  headnorms[layeri,:] = MEattn[][,,,].norm(dim=).mean(dim=).cpu()\n","\n","# visualize\n","plt.figure(figsize=(11,4))\n","cmin,cmax = np.percentile(\n","plt.imshow(headnorms)\n","plt.colorbar(pad=.01)\n","plt.gca().set(xticks=range(ntokens),xticklabels=\n","              ylabel='Transformer layer',title='Average attention head norms')\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj41_part3b.png')\n","plt.show()"],"metadata":{"id":"mFs0rVp4YIkK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DBkrMjvwtdN8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 4: An interesting interlude**"],"metadata":{"id":"cKapXFs1tdfX"}},{"cell_type":"code","source":["# positions of the Mikes\n","mike_idx = torch.where(\n","\n","layerskip = 3\n","_,axs = plt.subplots(1,n_layers//layerskip,figsize=(12,3))\n","\n","for layeri in range(\n","\n","  # extract the Mikes\n","  Mike1 = MEattn[f'L{layeri}'][,,,].cpu()\n","  Mike2 =\n","  MikeDiff =\n","\n","  # visualize their difference\n","  axs[layeri//layerskip].imshow()\n","  axs[layeri//layerskip].set(title=f'L{layeri}',xticks=[],yticks=[])\n","  axs[layeri//layerskip].spines.top.set_visible(True)\n","  axs[layeri//layerskip].spines.right.set_visible(True)\n","\n","axs[0].set(xlabel='Head',ylabel='Dimension')\n","plt.suptitle('The difference of the Mikes',fontweight='bold',fontsize=14)\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj41_part4a.png')\n","plt.show()"],"metadata":{"id":"5jHOAixQJcEJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize\n","meanratio = np.zeros(n_layers)\n","\n","for layeri in range(n_layers):\n","\n","  # extract the Mikes\n","  Mike1 =\n","  Mike2 =\n","  MikeDiff =\n","\n","  # L1 mean\n","  means =\n","\n","  # ratio of max2 to min2 (averaging 2 to boost SNR)\n","  meanratio[layeri] =  /\n","\n","# and plot\n","plt.figure(figsize=(10,3))\n","plt.plot(meanratio,'kh',markerfacecolor=[.7,.9,.9],markersize=12)\n","plt.gca().set(xlabel='Layer',ylabel='Head selectivity ratio')\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj41_part4b.png')\n","plt.show()"],"metadata":{"id":"5jXQgRcpNnQm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kLHBtv_d-L9A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 5: Head-specific IOI patching**"],"metadata":{"id":"pNeCopDxRR5W"}},{"cell_type":"code","source":["# initializations\n","IOI_scores = np.zeros((n_heads,n_layers))\n","\n","# loop over layers\n","for layeri in tqdm(range(n_layers)):\n","  for headi in range(n_heads):\n","\n","\n","    ### --- patch this head --- ###\n","    def hook(module,input):\n","      # reshape, silence one head, reshape back to tensor\n","      head_tensor = input[0].view(,,,) # reshape to index one head\n","      head_tensor[:,:,headi,:] = MEattn   # patch this head from ME\n","      head_tensor = head_tensor.view(,,)        # reshape back to tensor\n","      input = (head_tensor,*input[1:])                               # repackage the tuple\n","      return input\n","\n","    # implant the hook\n","    handle = model.transformer.h[layeri].attn.c_proj.register_forward_pre_hook(hook)\n","\n","\n","    ### --- forward pass --- ###\n","    with torch.no_grad():\n","      outEM = model(tokensEM)\n","\n","    # remove the hook\n","    handle.remove()\n","\n","\n","    ### --- calculate IOI score --- ###\n","    IOI_scores[headi,layeri] =  -"],"metadata":{"id":"odolX6DBRR5X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,axs = plt.subplots(1,3,figsize=(12,3))\n","\n","# image data to show\n","I =\n","\n","# find color limits based on L1-mean\n","clim =\n","\n","# create the image\n","h = axs[0].pcolor(,cmap='RdBu_r')\n","axs[0].set(xlabel='Transformer layer',ylabel='Attention head index',yticks=range(0,n_heads,2),\n","           title='A) $\\\\Delta$ IOI (patched - clean)')\n","c = fig.colorbar(h,ax=axs[0],pad=.02)\n","c.ax.tick_params(labelsize=8)\n","axs[0].spines.top.set_visible(True)\n","axs[0].spines.right.set_visible(True)\n","\n","Imean =\n","Istd =\n","axs[1].errorbar()\n","axs[1].plot()\n","axs[1].axhline()\n","axs[1].set(xlabel='Transformer layer',ylabel='Average $\\\\Delta$ IOI',title='B) Head-average results')\n","\n","axs[2].plot()\n","axs[2].set(xlabel='Transformer layer',ylabel='Stdev across $\\\\Delta$ IOI',title='C) Head-variability results')\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj41_part5.png')\n","plt.show()"],"metadata":{"id":"ylS1MdrV-MFN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UT3tS_0DeQ7E"},"execution_count":null,"outputs":[]}]}