{"cells":[{"cell_type":"markdown","metadata":{"id":"py_eibYAH3Q-"},"source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[39] Token frequencies, attention adjustments, and QK^T</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R5SI-Iyy4dtm"},"outputs":[],"source":["# note: check RAM via system resources; I used 8GB"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rn8Fcyf87EXx"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","from datasets import load_dataset\n","\n","from tqdm import tqdm\n","\n","from scipy.stats import binned_statistic\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dy4A-ah8kzZQ"},"outputs":[],"source":["### Run this cell only if you're using \"dark mode\"\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300\n","})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_K7FNjnzrTt1"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"y_gQoCvVrTq6"},"source":["# **Part 1: Tokenize Wikipedia data and count token frequencies**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sYCPaXZRqDNn"},"outputs":[],"source":["# BERT tokenizer and model\n","from transformers import BertTokenizer, BertModel\n","\n","# load BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n","model = BertModel.from_pretrained('bert-large-uncased')\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2yvUlWtGrToF"},"outputs":[],"source":["# ~7 mins on Colab...\n","wiki = load_dataset('wikimedia/wikipedia','20231101.en',split='train[:1%]')\n","wiki"]},{"cell_type":"code","source":["wiki[1234]"],"metadata":{"id":"glzoIOmrxdmy"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qFr_Dwm2rTlR"},"outputs":[],"source":["# number of tokens in one text sample\n","len(tokenizer.encode(wiki[0]['text'],add_special_tokens=False))"]},{"cell_type":"code","source":["targetlength =\n","\n","token_props = torch.zeros()\n","\n","nsofar = 0\n","row = 0\n","while nsofar<targetlength:\n","\n","  # tokenize new text\n","  toks = tokenizer.encode(,add_special_tokens=)\n","\n","  # increase counter for these tokens\n","  for i in toks:\n","    token_props[i]\n","\n","  # update loop counters\n","  nsofar +=\n","  row += 1\n","\n","  if (row)%10==0:\n","    print(f'{nsofar:>7,} tokens after {row} rows.')\n","\n","print(f'\\nFinal count: {nsofar:>7,} tokens after {row} rows.')"],"metadata":{"id":"TGDo5TnYP_K3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sidx = torch.argsort(token_props,\n","\n","print('Most frequent tokens:')\n","for i in sidx[:20]:\n","  print(f'  {} occurrences of \"{}\"')"],"metadata":{"id":"ksIeXdX6SMw2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# convert from counts to proportion\n","token_props"],"metadata":{"id":"wJC5-letSbL0"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_bSgi-pfvWAN"},"outputs":[],"source":["plt.figure(figsize=(10,3))\n","plt.plot(,'k.',alpha=.3,markersize=3)\n","plt.gca().set(xlabel='Token index',ylabel='Token log-frequency',ylim=[-14,None],\n","              xlim=[0,tokenizer.vocab_size],title='Empirical proportions in 1M tokens')\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj39_part1.png')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JmzCcovsshlY"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# **Part 2: Create a batch of tokens**"],"metadata":{"id":"3FDyxhNLFJS9"}},{"cell_type":"code","source":["# create a batch\n","batchsize = 64\n","seqlen =\n","\n","# initialize as list\n","batch = []\n","\n","txti = -1\n","while len(batch)<:\n","\n","  # increase the counter and get tokens\n","  txti += 1\n","  tokens = tokenizer.encode(\n","\n","  # add a list item to the batch only if it's long enough\n","  if len(tokens)>=seqlen:\n","    batch.append\n","\n","# convert the list to a tensor\n","batch =\n","batch.shape"],"metadata":{"id":"-Xk1NlX7cs_7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(8,4))\n","plt.pcolor(batch,cmap='magma')\n","plt.gca().set(xlabel='Token index',ylabel='Sequence')\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj39_part2.png')\n","plt.show()"],"metadata":{"id":"6mz7nfSzGe0C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MzhcEoFqFJPu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yhYegAlMshhK"},"source":["# **Part 3: Hook attention QK^T and adjustments**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1aY1rxuyHJqt"},"outputs":[],"source":["# reference\n","model.encoder.layer[3].attention"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Hu-Bk9GHsP6"},"outputs":[],"source":["n_layers = model.config.\n","n_heads  = model.\n","emb_dim  =\n","head_dim =  //\n","\n","print(f'{} transformer layers with dimensionality {},')\n","print(f'{} attention heads with dimensionality {}.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"83zGGWE50p99"},"outputs":[],"source":["model.encoder.layer[4].attention.output # only want 'dense'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5RJMIok0HhJa"},"outputs":[],"source":["qkt_acts = {}\n","\n","def implant_hook_qkt(layer_number):\n","  def hook_qkt(module, input, output):\n","\n","    # calculate Q and K as XW\n","    Q = module.query(input[0].detach())\n","    K = module.\n","\n","    # reshape to get heads dimension\n","    Q = Q.view(batchsize,seqlen,n_heads,head_dim).transpose(1,2)\n","    K = K.\n","\n","    # QK^t raw attention scores\n","    qkt =  @ .transpose(-2,-1)\n","    qkt = # scaling\n","\n","    # average over heads and store\n","    qkt_acts[f'L{layer_number}'] = qkt.mean(dim=\n","\n","  return hook_qkt\n","\n","\n","# hook the attention output adjustments\n","adj_norms = {}\n","def implant_hook_adj(layer_number):\n","  def hook_adj(module,input,output):\n","    O = output.detach()\n","    adj_norms[f'L{layer_number}'] =\n","  return hook_adj\n","\n","\n","\n","# implant the hooks\n","handles = []\n","for i in range(n_layers):\n","  h = model.encoder.layer[i].....register_forward_hook(implant_hook_qkt(i))\n","  handles.append(h)\n","  h = model.encoder.layer[i].....register_forward_hook(implant_hook_adj(i))\n","  handles.append(h)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AS5YpdYVsheG"},"outputs":[],"source":["# push the batch through\n","# this is the only GPU-based improvement in the project (minutes -> seconds)\n","with torch.no_grad():\n","  outs = model(batch)\n","\n","print(f'QK^T keys:\\n',qkt_acts.keys())\n","print(f'\\nAdjustment vectors keys:\\n',adj_norms.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UyN0DGBD2aDu"},"outputs":[],"source":["qkt_acts['L5'].shape, adj_norms['L5'].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Slfk4Wj2k1Q"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"0XEV6WHy2kx9"},"source":["# **Part 4: Adjustment norms by token frequencies**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wnnpulwo2kuz"},"outputs":[],"source":["_,axs = plt.subplots(1,3,figsize=(12,3))\n","\n","for i,l in enumerate([,,]):\n","\n","  # extract and plot the data excluding the first token\n","  x = torch.log().flatten()\n","  y = adj_norms[][].flatten()\n","  axs[i].plot(x,y,'k.',markersize=3,alpha=.3)\n","  axs[i].set(xlabel='Log token prop',ylabel='Vector norm',\n","            title=f'Layer {l}: (r = {})')\n","\n","  # just the first token (not [CLS]!)\n","  x = torch.log(token_props[batch[\n","  y =\n","  axs[i].plot(x,y,'rx',markersize=5,alpha=.7)\n","\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj39_part4a.png')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T8-l20Tm2krx"},"outputs":[],"source":["# re-extract x\n","x = torch.log(token_props[batch[:,1:]]).flatten().numpy()\n","\n","# loop over layers (same 'x' for all layers)\n","plt.figure(figsize=(8,3))\n","for i in range(n_layers):\n","  y =\n","  r =\n","  plt.plot(i,r,)\n","\n","plt.axhline(0,linestyle='--',color='k',linewidth=.4)\n","plt.gca().set(xlabel='Transformer layer',ylabel='Correlation coefficient',\n","              title='Attention adjustment magnitudes vs. token frequency')\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj39_part4b.png')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Lz2jGdjpfkx"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"m_RApw0B56uF"},"source":["# **Part 5: QK^T by proportion product (one layer)**"]},{"cell_type":"code","source":["# extract the token proportions from one sequence as a column vector\n","P = np.log( np.array((token_props[batch[0]],)).T )\n","\n","# outer product (broadcast summing logs) to get a matrix\n","Pmat =  +\n","\n","# and visualize\n","plt.imshow()\n","plt.gca().set(xlabel='Token index',ylabel='Token index',title='Token proportion product pairs')\n","plt.colorbar(pad=.01)\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj39_part5a.png')\n","plt.show()"],"metadata":{"id":"cGU0AOehcFHv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# confirmation of symmetry\n","Pmat-Pmat.T"],"metadata":{"id":"pUI4eYF181Sg"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pw386FQondRI"},"outputs":[],"source":["# done per-sequence\n","for seqi in range(batchsize):\n","\n","  P = np.array((token_props[batch[seqi]],)).T\n","  pmat =  +\n","  pmat = pmat.flatten()\n","  qk_t = qkt_acts[][,,].flatten(\n","\n","  plt.plot(pmat[::500],qk_t[::500],'.',markersize=3,alpha=.3)\n","\n","plt.gca().set(xlabel='Log-probabilty sum',ylabel='QK$^\\\\top$')\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj39_part5b.png')\n","plt.show()"]},{"cell_type":"code","source":[],"metadata":{"id":"Jgjo-2r6_PXQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# FYI, difference between percentile (equal data binning) and linspace (equal x-axis binning)\n","# This figure is mentioned but not shown in the book.\n","edges1 = np.percentile(pmat,np.linspace(0,100,41))\n","edges2 = np.linspace(pmat.min(),pmat.max(),41)\n","\n","# plot the results and the line of unity\n","plt.plot(edges1,edges2,'kh',markersize=10,markerfacecolor=[.9,.7,.9,.7])\n","plt.plot(edges1[[0,-1]],edges2[[0,-1]],'k--',linewidth=.5)\n","plt.gca().set(xlabel='Edges from percentile (equal data)',ylabel='Edges from linspace (equal grid spacing)')\n","\n","plt.show()"],"metadata":{"id":"0AlTGDQPAzxV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"koVqp9JS6lS4"},"outputs":[],"source":["# grid resolution and layer\n","N = 41\n","layeri = 7\n","\n","# initialize correlations matrix\n","corrs = np.zeros((batchsize,2))\n","\n","_,axs = plt.subplots(1,2,figsize=(10,3))\n","for seqi in range(batchsize):\n","\n","  # get flattened co-probability matrix\n","  P =\n","  pmat =\n","  pmat =\n","\n","  # define equal-sized bin boundaries and dataset for this sequence\n","  edges = np.percentile(pmat,np.linspace(0,100,N))\n","\n","  qk_t =\n","  meansByBin,_,_ = binned_statistic(pmat,qk_t,statistic='mean',bins=edges)\n","  stdesByBin,_,_ = binned_statistic\n","\n","  # scatter plots\n","  bincenters =\n","  axs[0].plot(bincenters,meansByBin,'h',markersize=4,alpha=.5)\n","  axs[1].plot(bincenters,stdesByBin,'h',markersize=4,alpha=.5)\n","\n","  # correlations\n","  corrs[seqi,0] = np.corrcoef(\n","  corrs[seqi,1] = np.corrcoef(\n","\n","\n","# plot adjustments\n","axs[0].set(xlabel='Token pair log-probs',ylabel='$\\\\mathbf{QK^\\\\top}$ mean',title=f'A) Average scores by token probs, L{layeri}')\n","axs[1].set(xlabel='Token pair log-probs',ylabel='$\\\\mathbf{QK^\\\\top}$ stdev',title=f'B) Stdev scores by token probs, L{layeri}')\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj39_part5c.png')\n","plt.show()"]},{"cell_type":"code","source":["plt.figure(figsize=(8,3))\n","plt.plot(label='$\\\\mathbf{QK^\\\\top}$ stdev')\n","plt.plot(label='$\\\\mathbf{QK^\\\\top}$ mean')\n","\n","plt.axhline(0,linestyle='--',linewidth=.3,color='w')\n","plt.legend()\n","plt.gca().set(xlabel='Batch sequence number',ylabel='Correlation coefficient',\n","              title='Relations between token frequencies and QK$^\\\\top$ features')\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj39_part5d.png')\n","plt.show()"],"metadata":{"id":"kzN39SuFQ1hz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"67gjAzB6OXnq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"db3R0r0ZOdSB"},"source":["# **Part 6: Laminar profiles of QK^T by proportion**"]},{"cell_type":"code","source":["N = 41\n","\n","descriptivesByProb = np.zeros((,,))\n","\n","# loop over sequencies in the batch\n","for seqi in tqdm(\n","\n","  # get flattened co-probability matrix (same for all layers)\n","  P = np.array((token_props[batch[seqi]],)).T\n","  pmat = np.log(P) + np.log(P.T)\n","  pmat = pmat.flatten()\n","\n","  # now loop over transformer layers\n","  for layeri in range(n_layers):\n","\n","    # get flattened QK' dot products for this layer and this sequence\n","    qk_t =\n","\n","    # define equal-sized bin boundaries and dataset for this sequence\n","    edges =\n","    meansByBin,_,_ =\n","    stdesByBin,_,_ =\n","\n","    # pool and sum\n","    descriptivesByProb[layeri,:,0] +=\n","    descriptivesByProb[layeri,:,1] +=\n","    descriptivesByProb[layeri,:,2] +=\n","\n","\n","# divide by batchsize for average\n","descriptivesByProb"],"metadata":{"id":"4VS5h5SUUlnK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(2,2,figsize=(12,6))\n","\n","# for mapping line color onto the colorbar\n","norm = mpl.colors.Normalize(vmin=0,vmax=n_layers-1)\n","sm = mpl.cm.ScalarMappable(cmap=mpl.cm.plasma,norm=norm)\n","\n","\n","# loop over layers\n","for layeri in range(n_layers):\n","\n","  # extract the descriptive stats\n","  x  = descriptivesByProb[,:,]\n","  yM = descriptivesByProb\n","  yS =\n","\n","  # plot the data\n","  axs[0,0].plot(x,,'.-',markersize=7,color=plt.cm.plasma(norm(layeri)),linewidth=2)\n","  axs[0,1].plot(x,,'.-',markersize=7,color=plt.cm.plasma(norm(layeri)),linewidth=2)\n","\n","  # and the correlation coefficients\n","  axs[1,0].plot(layeri,np.corrcoef(,'ks',markersize=11,markerfacecolor=plt.cm.plasma(layeri/n_layers))\n","  axs[1,1].plot(layeri,,'ko',markersize=11,markerfacecolor=plt.cm.plasma(layeri/n_layers))\n","\n","\n","# thin lines for r=0\n","axs[1,0].axhline(0,color='k',linestyle='--',linewidth=.4)\n","axs[1,1].axhline(0,color='k',linestyle='--',linewidth=.4)\n","\n","# add colorbars\n","cbar = plt.colorbar(sm,ax=axs[0,0],pad=.01)\n","cbar.set_label('Transformer layer',fontweight='normal')\n","cbar = plt.colorbar(sm,ax=axs[0,1],pad=.01)\n","cbar.set_label('Transformer layer',fontweight='normal')\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj39_part6.png')\n","plt.show()"],"metadata":{"id":"UJKNuBKSOL-1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8AhmW-J9shXz"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"private_outputs":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}