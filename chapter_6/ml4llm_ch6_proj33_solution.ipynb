{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[33] QKV weights characteristics</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BvQj17hzqwM"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","from transformers import AutoModelForCausalLM"]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300\n","})"],"metadata":{"id":"dy4A-ah8kzZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Eja6hB4TfIAU"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# **Part 1: A dictionary of GPT2 models**"],"metadata":{"id":"oGIKYsGKEEO-"}},{"cell_type":"code","source":["# a list of lists of model infos\n","model_ids = [\n","    #  name    label\n","    ['small' ,       'gpt2'],\n","    ['medium','gpt2-medium'],\n","    ['large' , 'gpt2-large'],\n","    ['xl'    ,    'gpt2-xl']\n","]\n","\n","\n","# load all models into a dictionary\n","models = {}\n","params = {}\n","\n","for modinfo in model_ids:\n","\n","  # load the model\n","  models[modinfo[0]] = AutoModelForCausalLM.from_pretrained(modinfo[1])\n","\n","  # count key parameters\n","  params[modinfo[0]] = {}\n","  params[modinfo[0]]['n_layers'] = models[modinfo[0]].config.n_layer\n","  params[modinfo[0]]['n_emb']    = models[modinfo[0]].config.n_embd\n","  params[modinfo[0]]['n_heads']  = models[modinfo[0]].config.n_head\n","  params[modinfo[0]]['head_dim'] = models[modinfo[0]].config.n_embd // params[modinfo[0]]['n_heads']"],"metadata":{"id":"rf50SgFE-AAF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('\"models\" keys:\\n',models.keys())\n","print('\\n\"params[''small'']\" keys:\\n',params['small'].keys())"],"metadata":{"id":"sj93On0Gbmhw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('  Model  | Embed.dim | Layers | n heads | head.dim')\n","print('---------+-----------+--------+---------+---------')\n","for name in models.keys():\n","  print(f\" {name:>6}  |    {params[name]['n_emb']:4}   |   {params[name]['n_layers']}   |    {params[name]['n_heads']}   |   {params[name]['head_dim']}\")"],"metadata":{"id":"Zde9zSQ1hUPw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# fyi, accessing a weights matrix\n","models['small'].transformer.h[5].attn.c_attn.weight"],"metadata":{"id":"fzsclB2Y6uiY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# count total number of parameters in attention subblock\n","\n","for modlist in model_ids:\n","\n","  name = modlist[0]\n","\n","  # isolate one layer\n","  block = models[name].transformer.h[5].attn\n","\n","  # count the parameters in this layer\n","  params_per_layer = (\n","    block.c_attn.weight.numel() +\n","    block.c_attn.bias.numel() +\n","    block.c_proj.weight.numel() +\n","    block.c_proj.bias.numel()\n","  )\n","\n","  # total params is weights times layers\n","  totparams = params_per_layer * params[name]['n_layers']\n","\n","  # and print the info\n","  print(f'{totparams/1e6:5,.1f}M attention weights in GPT2-{name}')"],"metadata":{"id":"xJ2zF2QC9nrR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1OaTAHSd9niv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: Distributions of attention weights in GPT2-small**"],"metadata":{"id":"11GBfmdb931y"}},{"cell_type":"code","source":["# in one layer for one model\n","whichmod = 'small'\n","layeri = 6\n","\n","# extract the wide weights matrix for this layer\n","wide_weights = models[whichmod].transformer.h[layeri].attn.c_attn.weight.detach()\n","\n","plt.figure(figsize=(10,3))\n","plt.imshow(wide_weights,vmin=-.1,vmax=.1,cmap='plasma')\n","plt.axvline(params[whichmod]['n_emb'],linestyle='--',color='w')\n","plt.axvline(2*params[whichmod]['n_emb'],linestyle='--',color='w')\n","plt.colorbar(pad=.01)\n","\n","plt.gca().set(xticks=[],ylabel='Embeddings dimensions',\n","              xlabel='Queries dimensions         |           Keys dimensions             |           Values dimensions')\n","\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj33_part2a.png')\n","plt.show()"],"metadata":{"id":"MuXzEYV2Er2z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# split the Q, K, and V matrices\n","q,k,v = torch.split(wide_weights,params[whichmod]['n_emb'],dim=1)\n","\n","# histograms of the three weights values\n","plt.figure(figsize=(8,3))\n","y,x = np.histogram(q.flatten(),bins='fd')\n","plt.plot(x[:-1],y,label='$\\\\mathbf{W_Q}$')\n","\n","y,x = np.histogram(k.flatten(),bins='fd')\n","plt.plot(x[:-1],y,label='$\\\\mathbf{W_K}$')\n","\n","y,x = np.histogram(v.flatten(),bins='fd')\n","plt.plot(x[:-1],y,label='$\\\\mathbf{W_V}$')\n","\n","plt.gca().set(xlabel='Weight value',ylabel='Count',\n","              title=f'Distribution of QKV weights in layer {layeri} in GPT2-{whichmod}')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj33_part2b.png')\n","plt.show()"],"metadata":{"id":"n1cwM8jp6ufz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# common histogram boundaries\n","histedges = np.linspace(-.8,.8,81)\n","\n","# initializations\n","distributions = np.zeros((params[whichmod]['n_layers'],len(histedges)-1,3))\n","distchars = np.zeros((params[whichmod]['n_layers'],2,3))\n","\n","# loop over layers\n","for layeri in range(params[whichmod]['n_layers']):\n","\n","  # split into matrices\n","  wideW = models[whichmod].transformer.h[layeri].attn.c_attn.weight.detach()\n","  q,k,v = torch.split(wideW,params[whichmod]['n_emb'],dim=1)\n","\n","  # histograms\n","  distributions[layeri,:,0] = np.histogram(q.flatten(),bins=histedges,density=True)[0]\n","  distributions[layeri,:,1] = np.histogram(k.flatten(),bins=histedges,density=True)[0]\n","  distributions[layeri,:,2] = np.histogram(v.flatten(),bins=histedges,density=True)[0]\n","\n","  # mean and std\n","  distchars[layeri,:,0] = np.array([q.mean(), q.std()])\n","  distchars[layeri,:,1] = np.array([k.mean(), k.std()])\n","  distchars[layeri,:,2] = np.array([v.mean(), v.std()])\n","\n","# show the heatmaps\n","_,axs = plt.subplots(1,3,figsize=(10,3))\n","for i in range(3):\n","  axs[i].imshow(distributions[:,:,i],origin='lower',extent=[histedges[0],histedges[-1],0,params[whichmod]['n_layers']],\n","                aspect='auto',cmap=plt.cm.plasma,vmin=0,vmax=3.5)\n","  axs[i].set(xlabel='Weight value',ylabel='Layer',title=f\"$\\\\mathbf{{W}}_{'QKV'[i]}$\")\n","\n","plt.suptitle(f'Laminar distributions of attention weights in GPT2-{whichmod}',fontweight='bold')\n","\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj33_part2c.png')\n","plt.show()"],"metadata":{"id":"2Sigiy4d6udH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(10,3))\n","\n","for i in [0,1]:\n","  axs[i].plot(distchars[:,i,0],'rs-',markerfacecolor=[.9,.7,.7],markersize=10)\n","  axs[i].plot(distchars[:,i,1],'go-',markerfacecolor=[.7,.9,.7],markersize=10)\n","  axs[i].plot(distchars[:,i,2],'b^-',markerfacecolor=[.7,.7,.9],markersize=10)\n","  axs[i].legend(['$\\\\mathbf{W_Q}$','$\\\\mathbf{W_K}$','$\\\\mathbf{W_V}$'])\n","  axs[i].set(xlabel='Transformer layer',ylabel=['Mean','Stdev'][i])\n","\n","plt.suptitle(f'Means and standard deviations of attention weights in GPT2-{whichmod}',fontweight='bold')\n","\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj33_part2d.png')\n","plt.show()"],"metadata":{"id":"lQQhQkwlQgak"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"h4zcemQr6uaT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: Comparing distributions across models**"],"metadata":{"id":"JWhNlxWh6uXQ"}},{"cell_type":"code","source":["_,axs = plt.subplots(len(models),2,figsize=(10,8))\n","\n","\n","# loop over models\n","for modeli,whichmod in enumerate(models.keys()):\n","\n","  # initialize matrix of statistics for this layer\n","  attn_stats = np.zeros((params[whichmod]['n_layers'],3,2))\n","\n","  # loop over layers\n","  for layeri in range(params[whichmod]['n_layers']):\n","\n","    # split into Q, K, V\n","    Q,K,V = torch.split(models[whichmod].transformer.h[layeri].attn.c_attn.weight.detach(),\n","                        params[whichmod]['n_emb'],dim=1)\n","\n","    ### Q\n","    attn_stats[layeri,0,0] = Q.mean()\n","    attn_stats[layeri,0,1] = Q.std()\n","\n","    ### K\n","    attn_stats[layeri,1,0] = K.mean()\n","    attn_stats[layeri,1,1] = K.std()\n","\n","    ### V\n","    attn_stats[layeri,2,0] = V.mean()\n","    attn_stats[layeri,2,1] = V.std()\n","\n","  # end of layer loop\n","\n","  for i in [0,1]:\n","    axs[modeli,i].plot(attn_stats[:,0,i],linewidth=2,label='$\\\\mathbf{W_Q}$')\n","    axs[modeli,i].plot(attn_stats[:,1,i],linewidth=2,label='$\\\\mathbf{W_K}$')\n","    axs[modeli,i].plot(attn_stats[:,2,i],linewidth=2,label='$\\\\mathbf{W_V}$')\n","    axs[modeli,i].legend(fontsize=8)\n","    axs[modeli,i].set(xlabel='Transformer layer',ylabel=['Mean','Stdev'][i],\n","                      title=f'GPT2-{whichmod}')\n","\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj33_part3.png')\n","plt.show()"],"metadata":{"id":"jG3PHLXr6uRp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fs8fhpcq_OUT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 4: Cosine similarity within heads (one model)**"],"metadata":{"id":"Dqr-EIh7_OR1"}},{"cell_type":"code","source":["whichmod = 'small'\n","layeri = 6\n","\n","# to split into heads, first split into QKV\n","Q,K,V = torch.split(models[whichmod].transformer.h[layeri].attn.c_attn.weight.detach(),\n","                    params[whichmod]['n_emb'],dim=1)\n","\n","# now split each W matrix into heads\n","WQ_h = torch.split(Q,params[whichmod]['head_dim'],dim=1)\n","WK_h = torch.split(K,params[whichmod]['head_dim'],dim=1)\n","\n","len(WQ_h), WQ_h[3].shape"],"metadata":{"id":"9kSoTHPL_OM6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,axs = plt.subplots(3,4,figsize=(10,5))\n","\n","axs = axs.flatten()\n","for i in range(len(WQ_h)):\n","  axs[i].imshow(WQ_h[i].T,aspect='auto',vmin=-.1,vmax=.1,cmap='plasma')\n","  axs[i].set(xticks=[],yticks=[])\n","  axs[i].text(18,11,f'H{i}',fontweight='bold',color='k',fontsize=16)\n","  axs[i].text(10,10,f'H{i}',fontweight='bold',color='w',fontsize=16)\n","\n","axs[8].set(xlabel='Embeddings dimensions',ylabel='Head dimension')\n","\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj33_part4a.png')\n","plt.show()"],"metadata":{"id":"CnfHw9quScp2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# cosine similarity in one attention head\n","tmp = WQ_h[0] / torch.norm(WQ_h[0],dim=0,keepdim=True)\n","cs_Q = tmp.T @ tmp\n","\n","fig,axs = plt.subplots(1,2,figsize=(10,3))\n","# show the similarity matrix\n","h = axs[0].imshow(cs_Q,cmap='plasma',vmin=-.2,vmax=.2)\n","axs[0].set(xlabel='Head dimension',ylabel='Head dimension',title='A) Cosine similarity matrix')\n","fig.colorbar(h,ax=axs[0],pad=.01)\n","\n","# and the distribution\n","axs[1].hist(cs_Q[np.triu_indices(cs_Q.shape[0],1)],40,color='gray',edgecolor='k')\n","axs[1].set(xlabel='Cosine similarity',ylabel='Count',title='B) Distribution of similarities')\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj33_part4b.png')\n","plt.show()"],"metadata":{"id":"CffkjT7iXmZC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"E3dfrzhNYAiH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 5: Cosine similarity within- vs. across-heads (one model)**"],"metadata":{"id":"EeccT28HXDfx"}},{"cell_type":"code","source":["# initialize as empty arrays\n","withinhead_csQ = np.array([])\n","acrosshead_csQ = np.array([])\n","withinhead_csK = np.array([])\n","acrosshead_csK = np.array([])\n","\n","# loop over pairs of heads\n","for i in range(params[whichmod]['n_heads']):\n","  for j in range(i,params[whichmod]['n_heads']):\n","\n","    ### Q\n","    tmpi = WQ_h[i] / torch.norm(WQ_h[i],dim=0,keepdim=True)\n","    tmpj = WQ_h[j] / torch.norm(WQ_h[j],dim=0,keepdim=True)\n","    cs = (tmpi.T @ tmpj).numpy() # convert to numpy here...\n","\n","    # store in the appropriate matrix\n","    if i==j: # within-head -> symmetric matrix -> keep nonredundant values\n","      cs = cs[np.triu_indices(params[whichmod]['head_dim'],k=1)]\n","      withinhead_csQ = np.concatenate((withinhead_csQ,cs))\n","    else: # across-head -> nonsymmetric matrix -> keep all values\n","      acrosshead_csQ = np.concatenate((acrosshead_csQ,cs.flatten()))\n","\n","\n","    ### K\n","    tmpi = WK_h[i] / torch.norm(WK_h[i],dim=0,keepdim=True)\n","    tmpj = WK_h[j] / torch.norm(WK_h[j],dim=0,keepdim=True)\n","    cs = tmpi.T @ tmpj # leave as pytorch here...\n","\n","    # store in the appropriate matrix\n","    if i==j: # within-head -> symmetric matrix -> keep nonredundant values\n","      cs = cs[np.triu_indices(params[whichmod]['head_dim'],k=1)]\n","      withinhead_csK = np.concatenate((withinhead_csK,cs))\n","    else: # across-head -> nonsymmetric matrix -> keep all values\n","      acrosshead_csK = np.concatenate((acrosshead_csK,cs.flatten()))\n","\n","\n","_,axs = plt.subplots(1,2,figsize=(12,3))\n","y,x = np.histogram(withinhead_csQ,bins='fd',density=True)\n","axs[0].plot(x[:-1],y,linewidth=2,label='Within heads')\n","y,x = np.histogram(acrosshead_csQ,bins='fd',density=True)\n","axs[0].plot(x[:-1],y,linewidth=2,label='Across heads')\n","axs[0].set(xlabel='Cosine similarity',ylabel='Density',title='A) Similarities in $\\\\mathbf{W_Q}$')\n","axs[0].legend()\n","\n","y,x = np.histogram(withinhead_csK,bins='fd',density=True)\n","axs[1].plot(x[:-1],y,linewidth=2,label='Within heads')\n","y,x = np.histogram(acrosshead_csK,bins='fd',density=True)\n","axs[1].plot(x[:-1],y,linewidth=2,label='Across heads')\n","axs[1].set(xlabel='Cosine similarity',ylabel='Density',title='B) Similarities in $\\\\mathbf{W_K}$')\n","axs[1].legend()\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj33_part5a.png')\n","plt.show()"],"metadata":{"id":"411tjzM0XmWI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["meenz = np.zeros((params[whichmod]['n_layers'],2,2))\n","stdez = np.zeros((params[whichmod]['n_layers'],2,2))\n","\n","histbins = np.linspace(-.4,.4,101)\n","hists = np.zeros((params[whichmod]['n_layers'],len(histbins)-1,2,2))\n","\n","\n","# loop over all layers\n","for layeri in range(params[whichmod]['n_layers']):\n","\n","  # split into heads\n","  Q,K,V = torch.split(models[whichmod].transformer.h[layeri].attn.c_attn.weight.detach(),\n","                      params[whichmod]['n_emb'],dim=1)\n","  WQ_h = torch.split(Q,params[whichmod]['head_dim'],dim=1)\n","  WK_h = torch.split(K,params[whichmod]['head_dim'],dim=1)\n","\n","  # re-initialize\n","  withinhead_csQ = np.array([])\n","  acrosshead_csQ = np.array([])\n","  withinhead_csK = np.array([])\n","  acrosshead_csK = np.array([])\n","\n","  # loop over pairs of heads (copy from proj35)\n","  for i in range(params[whichmod]['n_heads']):\n","    for j in range(i,params[whichmod]['n_heads']):\n","\n","      ### Q\n","      tmpi = WQ_h[i] / torch.norm(WQ_h[i],dim=0,keepdim=True)\n","      tmpj = WQ_h[j] / torch.norm(WQ_h[j],dim=0,keepdim=True)\n","      cs = (tmpi.T @ tmpj).numpy()\n","\n","      # store in the appropriate matrix\n","      if i==j: # within-head -> symmetric matrix -> keep nonredundant values\n","        cs = cs[np.triu_indices(params[whichmod]['head_dim'],k=1)]\n","        withinhead_csQ = np.concatenate((withinhead_csQ,cs))\n","      else: # across-head -> nonsymmetric matrix -> keep all values\n","        acrosshead_csQ = np.concatenate((acrosshead_csQ,cs.flatten()))\n","\n","\n","      ### repeat for K\n","      tmpi = WK_h[i] / torch.norm(WK_h[i],dim=0,keepdim=True)\n","      tmpj = WK_h[j] / torch.norm(WK_h[j],dim=0,keepdim=True)\n","      cs = tmpi.T @ tmpj\n","\n","      # store in the appropriate matrix\n","      if i==j: # within-head -> symmetric matrix -> keep nonredundant values\n","        cs = cs[np.triu_indices(params[whichmod]['head_dim'],k=1)]\n","        withinhead_csK = np.concatenate((withinhead_csK,cs))\n","      else: # across-head -> nonsymmetric matrix -> keep all values\n","        acrosshead_csK = np.concatenate((acrosshead_csK,cs.flatten()))\n","  # end of layer-loop\n","\n","  # get the histograms\n","  hists[layeri,:,0,0] = np.histogram(withinhead_csQ,bins=histbins,density=True)[0]\n","  hists[layeri,:,1,0] = np.histogram(acrosshead_csQ,bins=histbins,density=True)[0]\n","  hists[layeri,:,0,1] = np.histogram(withinhead_csK,bins=histbins,density=True)[0]\n","  hists[layeri,:,1,1] = np.histogram(acrosshead_csK,bins=histbins,density=True)[0]\n","\n","  # the means\n","  meenz[layeri,0,0] = withinhead_csQ.mean()\n","  meenz[layeri,0,1] = acrosshead_csQ.mean()\n","  meenz[layeri,1,0] = withinhead_csK.mean()\n","  meenz[layeri,1,1] = acrosshead_csK.mean()\n","\n","  # and the standard deviations\n","  stdez[layeri,0,0] = withinhead_csQ.std()\n","  stdez[layeri,0,1] = acrosshead_csQ.std()\n","  stdez[layeri,1,0] = withinhead_csK.std()\n","  stdez[layeri,1,1] = acrosshead_csK.std()"],"metadata":{"id":"CQzgh7fnO0me"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,axs = plt.subplots(2,2,figsize=(10,5))\n","\n","# Q within-head\n","axs[0,0].imshow(hists[:,:,0,0],aspect='auto',vmin=0,vmax=7,cmap='magma',\n","              origin='lower',extent=[histbins[0],histbins[-1],0,params[whichmod]['n_layers']])\n","axs[0,0].set(xlabel='Cosine similarity',ylabel='Layer',title='A) $\\\\mathbf{W_Q}$: Distributions within head')\n","\n","\n","# Q across-head\n","axs[0,1].imshow(hists[:,:,1,0],aspect='auto',vmin=0,vmax=7,cmap='magma',\n","              origin='lower',extent=[histbins[0],histbins[-1],0,params[whichmod]['n_layers']])\n","axs[0,1].set(xlabel='Cosine similarity',ylabel='Layer',title='B) $\\\\mathbf{W_Q}$: Distributions across head')\n","\n","\n","# K within-head\n","axs[1,0].imshow(hists[:,:,0,1],aspect='auto',vmin=0,vmax=7,cmap='magma',\n","              origin='lower',extent=[histbins[0],histbins[-1],0,params[whichmod]['n_layers']])\n","axs[1,0].set(xlabel='Cosine similarity',ylabel='Layer',title='C) $\\\\mathbf{W_K}$: Distributions within head')\n","\n","\n","# K across-head\n","axs[1,1].imshow(hists[:,:,1,1],aspect='auto',vmin=0,vmax=7,cmap='magma',\n","              origin='lower',extent=[histbins[0],histbins[-1],0,params[whichmod]['n_layers']])\n","axs[1,1].set(xlabel='Cosine similarity',ylabel='Layer',title='D) $\\\\mathbf{W_K}$: Distributions across head')\n","\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj33_part5b.png')\n","plt.show()"],"metadata":{"id":"VcwToOfef9Aa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(12,3))\n","\n","axs[0].plot(meenz[:,0,0],'rs-', markerfacecolor=[.9,.7,.7],markersize=10,label='$\\\\mathbf{W_Q}$ within head')\n","axs[0].plot(meenz[:,0,1],'ro--',markerfacecolor=[.9,.7,.7],markersize=10,label='$\\\\mathbf{W_Q}$ across head')\n","axs[0].plot(meenz[:,1,0],'bs-', markerfacecolor=[.7,.7,.9],markersize=10,label='$\\\\mathbf{W_K}$ within head')\n","axs[0].plot(meenz[:,1,1],'bo--',markerfacecolor=[.7,.7,.9],markersize=10,label='$\\\\mathbf{W_K}$ across head')\n","axs[0].legend(handlelength=4)\n","axs[0].set(xlabel='Transformer layer',ylabel='Mean',title='A) Average of cosine similarity dist.')\n","\n","axs[1].plot(stdez[:,0,0],'rs-', markerfacecolor=[.9,.7,.7],markersize=10,label='$\\\\mathbf{W_Q}$ within head')\n","axs[1].plot(stdez[:,0,1],'ro--',markerfacecolor=[.9,.7,.7],markersize=10,label='$\\\\mathbf{W_Q}$ across head')\n","axs[1].plot(stdez[:,1,0],'bs-', markerfacecolor=[.7,.7,.9],markersize=10,label='$\\\\mathbf{W_K}$ within head')\n","axs[1].plot(stdez[:,1,1],'bo--',markerfacecolor=[.7,.7,.9],markersize=10,label='$\\\\mathbf{W_K}$ across head')\n","axs[1].legend(handlelength=4) # make the legend lines longer to show dashed vs solid\n","axs[1].set(xlabel='Transformer layer',ylabel='Standard deviation',title='B) Stdev of cosine similarity dist.')\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj33_part5c.png')\n","plt.show()"],"metadata":{"id":"nMLB0qtPO0jz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vLbKPTiN_OKP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 6: WQ, WK, WV similarities across models**"],"metadata":{"id":"dG0_Nu_2_OFF"}},{"cell_type":"code","source":["_,axs = plt.subplots(len(models),2,figsize=(10,8))\n","\n","\n","# loop over models\n","for modeli,whichmod in enumerate(models.keys()):\n","\n","  # initialize matrix to store the metaparameters\n","  cossim_stats = np.zeros((params[whichmod]['n_layers'],3,2))\n","\n","  # start of the progress report\n","  print(f\"\\n\\nWorking on GPT-2-{whichmod} ({params[whichmod]['n_layers']} layers):\")\n","\n","  # mask for non-redundant and non-trivial indices\n","  # it is model-specific but not layer-specific\n","  N = params[whichmod]['n_emb']\n","  mask = torch.triu(torch.ones(N,N,dtype=torch.bool),diagonal=1)\n","\n","\n","  # loop over layers\n","  for layeri in range(params[whichmod]['n_layers']):\n","\n","    # update the layer number in the progress report\n","    print(f'{layeri},',end=' ')\n","\n","    # split into Q, K, V\n","    Q,K,V = torch.split(models[whichmod].transformer.h[layeri].attn.c_attn.weight.detach(),\n","                        params[whichmod]['n_emb'],dim=1)\n","\n","    ### Q\n","    Q = Q / torch.norm(Q,dim=0,keepdim=True)\n","    cs = Q.T @ Q\n","    cs = cs[mask]\n","    cossim_stats[layeri,0,0] = cs.mean().item() # .item() to extract float from torch tensor\n","    cossim_stats[layeri,0,1] = cs.std().item()\n","\n","\n","    ### repeat for K\n","    K = K / torch.norm(K,dim=0,keepdim=True)\n","    cs = K.T @ K\n","    cs = cs[mask]\n","    cossim_stats[layeri,1,0] = cs.mean().item()\n","    cossim_stats[layeri,1,1] = cs.std().item()\n","\n","\n","    ### and for V\n","    V = V / torch.norm(V,dim=0,keepdim=True)\n","    cs = V.T @ V\n","    cs = cs[mask]\n","    cossim_stats[layeri,2,0] = cs.mean().item()\n","    cossim_stats[layeri,2,1] = cs.std().item()\n","\n","  # end of layer loop\n","\n","  for i in [0,1]:\n","    axs[modeli,i].plot(cossim_stats[:,0,i],linewidth=2,label='$\\\\mathbf{W_Q}$')\n","    axs[modeli,i].plot(cossim_stats[:,1,i],linewidth=2,label='$\\\\mathbf{W_K}$')\n","    axs[modeli,i].plot(cossim_stats[:,2,i],linewidth=2,label='$\\\\mathbf{W_V}$')\n","    axs[modeli,i].legend(fontsize=8)\n","    axs[modeli,i].set(xlabel='Transformer layer',ylabel=['Mean','Stdev'][i],\n","                      title=f'GPT2-{whichmod}')\n","\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj33_part6.png')\n","plt.show()"],"metadata":{"id":"fu7uqvyuB9Op"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Db3K2ppQRe-N"},"execution_count":null,"outputs":[]}]}