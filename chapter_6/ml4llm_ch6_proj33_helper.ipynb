{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[33] QKV weights characteristics</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BvQj17hzqwM"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","from transformers import AutoModelForCausalLM"]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300\n","})"],"metadata":{"id":"dy4A-ah8kzZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Eja6hB4TfIAU"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# **Part 1: A dictionary of GPT2 models**"],"metadata":{"id":"oGIKYsGKEEO-"}},{"cell_type":"code","source":["# a list of lists of model infos\n","model_ids = [\n","    #  name    label\n","    ['small' ,       'gpt2'],\n","    ['medium','gpt2-medium'],\n","    ['large' , 'gpt2-large'],\n","    ['xl'    ,    'gpt2-xl']\n","]\n","\n","\n","# load all models into a dictionary\n","models = {}\n","params = {}\n","\n","for modinfo in model_ids:\n","\n","  # load the model\n","  models[modinfo[0]] = AutoModelForCausalLM.from_pretrained(modinfo[1])\n","\n","  # count key parameters\n","  params[modinfo[0]] = {}\n","  params[modinfo[0]]['n_layers'] =\n","  params[modinfo[0]]['n_emb']    =\n","  params[modinfo[0]]['n_heads']  =\n","  params[modinfo[0]]['head_dim'] ="],"metadata":{"id":"rf50SgFE-AAF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('\"models\" keys:\\n',models.keys())\n","print('\\n\"params[''small'']\" keys:\\n',params['small'].keys())"],"metadata":{"id":"sj93On0Gbmhw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('  Model  | Embed.dim | Layers | n heads | head.dim')\n","print('---------+-----------+--------+---------+---------')\n","for name in models.keys():\n","  print(f\" {name:>6}  |    {}   |   {}   |    {}   |   {}\")"],"metadata":{"id":"Zde9zSQ1hUPw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# fyi, accessing a weights matrix\n","models['small'].transformer.h[5].attn.c_attn.weight"],"metadata":{"id":"fzsclB2Y6uiY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# count total number of parameters in attention subblock\n","\n","for modlist in model_ids:\n","\n","  name = mod\n","\n","  # isolate one layer\n","  block = models[name].transformer.h[5].attn\n","\n","  # count the parameters in this layer\n","  params_per_layer = (\n","    block.c_attn.weight.numel() +\n","    block. +\n","    block. +\n","    block.\n","  )\n","\n","  # total params is weights times layers\n","  totparams = params_per_layer *\n","\n","  # and print the info\n","  print(f'{}M attention weights in GPT2-{name}')"],"metadata":{"id":"xJ2zF2QC9nrR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1OaTAHSd9niv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: Distributions of attention weights in GPT2-small**"],"metadata":{"id":"11GBfmdb931y"}},{"cell_type":"code","source":["# in one layer for one model\n","whichmod = 'small'\n","layeri = 6\n","\n","# extract the wide weights matrix for this layer\n","wide_weights = models[whichmod]...\n","\n","plt.figure(figsize=(10,3))\n","plt.imshow()\n","plt.axvline(,linestyle='--',color='w')\n","plt.axvline(,linestyle='--',color='w')\n","plt.colorbar(pad=.01)\n","\n","plt.gca().set(xticks=[],ylabel='Embeddings dimensions',\n","              xlabel='Queries dimensions         |           Keys dimensions             |           Values dimensions')\n","\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj33_part2a.png')\n","plt.show()"],"metadata":{"id":"MuXzEYV2Er2z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# split the Q, K, and V matrices\n","q,k,v = torch.split(,params[whichmod][],dim=)\n","\n","# histograms of the three weights values\n","plt.figure(figsize=(8,3))\n","y,x = np.histogram(,bins='fd')\n","plt.plot(x[:-1],y,label='$\\\\mathbf{W_Q}$')\n","\n","y,x = np.histogram(\n","plt.plot(x[:-1],y,label='$\\\\mathbf{W_K}$')\n","\n","y,x = np.histogram(\n","plt.plot(x[:-1],y,label='$\\\\mathbf{W_V}$')\n","\n","plt.gca().set(xlabel='Weight value',ylabel='Count',\n","              title=f'Distribution of QKV weights in layer {layeri} in GPT2-{whichmod}')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj33_part2b.png')\n","plt.show()"],"metadata":{"id":"n1cwM8jp6ufz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# common histogram boundaries\n","histedges = np.linspace(-.8,.8,81)\n","\n","# initializations\n","distributions = np.zeros()\n","distchars = np.zeros(())\n","\n","# loop over layers\n","for layeri in range(params[whichmod]['n_layers']):\n","\n","  # split into matrices\n","  wideW = models[whichmod].transformer.h[layeri].attn.c_attn.weight.detach()\n","  q,k,v = torch.split\n","\n","  # histograms\n","  distributions[layeri,:,0] = np.histogram(\n","  distributions[layeri,:,1] = np.histogram(\n","  distributions[layeri,:,2] = np.histogram(\n","\n","  # mean and std\n","  distchars[layeri,:,0] =\n","  distchars[layeri,:,1] =\n","  distchars[layeri,:,2] =\n","\n","# show the heatmaps\n","_,axs = plt.subplots(1,3,figsize=(10,3))\n","for i in range(3):\n","  axs[i].imshow(distributions[:,:,i],origin='lower',extent=[histedges[0],histedges[-1],0,params[whichmod]['n_layers']],\n","                aspect='auto',cmap=plt.cm.plasma,vmin=0,vmax=3.5)\n","  axs[i].set(xlabel='Weight value',ylabel='Layer',title=f\"$\\\\mathbf{{W}}_{'QKV'[i]}$\")\n","\n","plt.suptitle(f'Laminar distributions of attention weights in GPT2-{whichmod}',fontweight='bold')\n","\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj33_part2c.png')\n","plt.show()"],"metadata":{"id":"2Sigiy4d6udH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(10,3))\n","\n","for i in [0,1]:\n","  axs[i].plot(,'rs-',markerfacecolor=[.9,.7,.7],markersize=10)\n","  axs[i].plot(,'go-',markerfacecolor=[.7,.9,.7],markersize=10)\n","  axs[i].plot(,'b^-',markerfacecolor=[.7,.7,.9],markersize=10)\n","  axs[i].legend(['$\\\\mathbf{W_Q}$','$\\\\mathbf{W_K}$','$\\\\mathbf{W_V}$'])\n","  axs[i].set(xlabel='Transformer layer',ylabel=['Mean','Stdev'][i])\n","\n","plt.suptitle(f'Means and standard deviations of attention weights in GPT2-{whichmod}',fontweight='bold')\n","\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj33_part2d.png')\n","plt.show()"],"metadata":{"id":"lQQhQkwlQgak"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"h4zcemQr6uaT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: Comparing distributions across models**"],"metadata":{"id":"JWhNlxWh6uXQ"}},{"cell_type":"code","source":["_,axs = plt.subplots(len(models),2,figsize=(10,8))\n","\n","\n","# loop over models\n","for modeli,whichmod in enumerate(models.keys()):\n","\n","  # initialize matrix of statistics for this layer\n","  attn_stats = np.zeros((params[],,))\n","\n","  # loop over layers\n","  for layeri in range(params[whichmod]['n_layers']):\n","\n","    # split into Q, K, V\n","    Q,K,V = torch.split()\n","\n","    ### Q\n","    attn_stats[layeri,0,0] =\n","    attn_stats[layeri,0,1] =\n","\n","    ### K\n","    attn_stats[\n","    attn_stats[\n","\n","    ### V\n","    attn_stats[\n","    attn_stats[\n","\n","  # end of layer loop\n","\n","  for i in [0,1]:\n","    axs[modeli,i].plot(label='$\\\\mathbf{W_Q}$')\n","    axs[modeli,i].plot(label='$\\\\mathbf{W_K}$')\n","    axs[modeli,i].plot(label='$\\\\mathbf{W_V}$')\n","    axs[modeli,i].legend(fontsize=8)\n","    axs[modeli,i].set(xlabel='Transformer layer',ylabel=['Mean','Stdev'][i],\n","                      title=f'GPT2-{whichmod}')\n","\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj33_part3.png')\n","plt.show()"],"metadata":{"id":"jG3PHLXr6uRp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fs8fhpcq_OUT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 4: Cosine similarity within heads (one model)**"],"metadata":{"id":"Dqr-EIh7_OR1"}},{"cell_type":"code","source":["whichmod = 'small'\n","layeri = 6\n","\n","# to split into heads, first split into QKV\n","Q,K,V = torch.split\n","\n","# now split each W matrix into heads\n","WQ_h = torch.split(Q,head_dim,dim=)\n","WK_h = torch.split(\n","\n","len(WQ_h), WQ_h[3].shape"],"metadata":{"id":"9kSoTHPL_OM6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,axs = plt.subplots(3,4,figsize=(10,5))\n","\n","axs = axs.flatten()\n","for i in range(len(WQ_h)):\n","  axs[i].imshow(,aspect='auto',vmin=-.1,vmax=.1,cmap='plasma')\n","  axs[i].set(xticks=[],yticks=[])\n","  axs[i].text(18,11,f'H{i}',fontweight='bold',color='k',fontsize=16)\n","  axs[i].text(10,10,f'H{i}',fontweight='bold',color='w',fontsize=16)\n","\n","axs[8].set(xlabel='Embeddings dimensions',ylabel='Head dimension')\n","\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj33_part4a.png')\n","plt.show()"],"metadata":{"id":"CnfHw9quScp2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# cosine similarity in one attention head\n","tmp =  / torch.norm(,dim=,keepdim=)\n","cs_Q =  @\n","\n","fig,axs = plt.subplots(1,2,figsize=(10,3))\n","# show the similarity matrix\n","h = axs[0].imshow()\n","axs[0].set(xlabel='Head dimension',ylabel='Head dimension',title='A) Cosine similarity matrix')\n","fig.colorbar(h,ax=axs[0],pad=.01)\n","\n","# and the distribution\n","axs[1].hist(,40,color='gray',edgecolor='k')\n","axs[1].set(xlabel='Cosine similarity',ylabel='Count',title='B) Distribution of similarities')\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj33_part4b.png')\n","plt.show()"],"metadata":{"id":"CffkjT7iXmZC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"E3dfrzhNYAiH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 5: Cosine similarity within- vs. across-heads (one model)**"],"metadata":{"id":"EeccT28HXDfx"}},{"cell_type":"code","source":["# initialize as empty arrays\n","withinhead_csQ = np.array([])\n","acrosshead_csQ = np.array([])\n","withinhead_csK = np.array([])\n","acrosshead_csK = np.array([])\n","\n","# loop over pairs of heads\n","for i in range(params[whichmod]['n_heads']):\n","  for j in range(\n","\n","    ### Q\n","    tmpi = WQ_h[i] / torch.norm(,dim=0,keepdim=True)\n","    tmpj =  /\n","    cs = (.T @ ).numpy() # convert to numpy here...\n","\n","    # store in the appropriate matrix\n","    if i==j: # within-head -> symmetric matrix -> keep nonredundant values\n","      cs =\n","      withinhead_csQ = np.concatenate((,cs))\n","    else: # across-head -> nonsymmetric matrix -> keep all values\n","      acrosshead_csQ = np.concatenate((,cs.flatten()))\n","\n","\n","    ### K\n","    tmpi =\n","    tmpj =\n","    cs =  # leave as pytorch here...\n","\n","    # store in the appropriate matrix\n","    if i==j: # within-head -> symmetric matrix -> keep nonredundant values\n","      cs =\n","      withinhead_csK =\n","    else: # across-head -> nonsymmetric matrix -> keep all values\n","      acrosshead_csK =\n","\n","\n","_,axs = plt.subplots(1,2,figsize=(12,3))\n","y,x = np.histogram(withinhead_csQ,bins='fd',density=True)\n","axs[0].plot(,linewidth=2,label='Within heads')\n","y,x = np.histogram(acrosshead_csQ,bins='fd',density=True)\n","axs[0].plot(,linewidth=2,label='Across heads')\n","axs[0].set(xlabel='Cosine similarity',ylabel='Density',title='A) Similarities in $\\\\mathbf{W_Q}$')\n","axs[0].legend()\n","\n","y,x = np.histogram(withinhead_csK,bins='fd',density=True)\n","axs[1].plot(,linewidth=2,label='Within heads')\n","y,x = np.histogram(acrosshead_csK,bins='fd',density=True)\n","axs[1].plot(,linewidth=2,label='Across heads')\n","axs[1].set(xlabel='Cosine similarity',ylabel='Density',title='B) Similarities in $\\\\mathbf{W_K}$')\n","axs[1].legend()\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj33_part5a.png')\n","plt.show()"],"metadata":{"id":"411tjzM0XmWI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["meenz = np.zeros((params[whichmod]['n_layers'],2,2))\n","stdez = np.zeros((params[whichmod]['n_layers'],2,2))\n","\n","histbins = np.linspace(-.4,.4,101)\n","hists = np.zeros((params[whichmod]['n_layers'],len(histbins)-1,2,2))\n","\n","\n","# loop over all layers\n","for layeri in range(params[whichmod]['n_layers']):\n","\n","  # split into heads\n","  Q,K,V = torch.split(models[whichmod].transformer.h[layeri]...,\n","                      params[whichmod]['n_emb'],dim=1)\n","  WQ_h = torch.split(\n","  WK_h = torch.split(\n","\n","  # re-initialize\n","  withinhead_csQ = np.array([])\n","  acrosshead_csQ = np.array([])\n","  withinhead_csK = np.array([])\n","  acrosshead_csK = np.array([])\n","\n","  # loop over pairs of heads (copy from proj35)\n","  for i in range(params[whichmod]['n_heads']):\n","    for j in range(i,params[whichmod]['n_heads']):\n","\n","      ### Q\n","      tmpi = WQ_h[i] / torch.norm(WQ_h[i],dim=0,keepdim=True)\n","      tmpj = WQ_h[j] / torch.norm(WQ_h[j],dim=0,keepdim=True)\n","      cs =\n","\n","      # store in the appropriate matrix\n","      if i==j: # within-head -> symmetric matrix -> keep nonredundant values\n","        cs =\n","        withinhead_csQ =\n","      else: # across-head -> nonsymmetric matrix -> keep all values\n","        acrosshead_csQ =\n","\n","\n","      ### repeat for K\n","      tmpi = WK_h[i] /\n","      tmpj =\n","      cs = tmpi.T @ tmpj\n","\n","      # store in the appropriate matrix\n","      if i==j: # within-head -> symmetric matrix -> keep nonredundant values\n","        cs = cs[np.triu_indices(params[whichmod]['head_dim'],k=1)]\n","        withinhead_csK =\n","      else: # across-head -> nonsymmetric matrix -> keep all values\n","        acrosshead_csK = np.concatenate(())\n","  # end of layer-loop\n","\n","  # get the histograms\n","  hists[layeri,:,0,0] = np.histogram(withinhead_csQ,bins=histbins,density=True)[0]\n","  hists[layeri,:,1,0] = np.histogram(acrosshead_csQ,bins=histbins,density=True)[0]\n","  hists[layeri,:,0,1] = np.histogram(withinhead_csK,bins=histbins,density=True)[0]\n","  hists[layeri,:,1,1] = np.histogram(acrosshead_csK,bins=histbins,density=True)[0]\n","\n","  # the means\n","  meenz[layeri,0,0] =\n","  meenz[layeri,0,1] =\n","  meenz[layeri,1,0] =\n","  meenz[layeri,1,1] =\n","\n","  # and the standard deviations\n","  stdez[layeri,0,0] =\n","  stdez[layeri,0,1] =\n","  stdez[layeri,1,0] =\n","  stdez[layeri,1,1] ="],"metadata":{"id":"CQzgh7fnO0me"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,axs = plt.subplots(2,2,figsize=(10,5))\n","\n","# Q within-head\n","axs[0,0].imshow(,aspect='auto',vmin=0,vmax=7,cmap='magma',\n","              origin='lower',extent=[histbins[0],histbins[-1],0,params[whichmod]['n_layers']])\n","axs[0,0].set(xlabel='Cosine similarity',ylabel='Layer',title='A) $\\\\mathbf{W_Q}$: Distributions within head')\n","\n","\n","# Q across-head\n","axs[0,1].imshow()\n","axs[0,1].set(xlabel='Cosine similarity',ylabel='Layer',title='B) $\\\\mathbf{W_Q}$: Distributions across head')\n","\n","\n","# K within-head\n","axs[1,0].imshow()\n","axs[1,0].set(xlabel='Cosine similarity',ylabel='Layer',title='C) $\\\\mathbf{W_K}$: Distributions within head')\n","\n","\n","# K across-head\n","axs[1,1].imshow)\n","axs[1,1].set(xlabel='Cosine similarity',ylabel='Layer',title='D) $\\\\mathbf{W_K}$: Distributions across head')\n","\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj33_part5b.png')\n","plt.show()"],"metadata":{"id":"VcwToOfef9Aa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(12,3))\n","\n","axs[0].plot(,label='$\\\\mathbf{W_Q}$ within head')\n","axs[0].plot(,label='$\\\\mathbf{W_Q}$ across head')\n","axs[0].plot(,label='$\\\\mathbf{W_K}$ within head')\n","axs[0].plot(,label='$\\\\mathbf{W_K}$ across head')\n","axs[0].legend(handlelength=4)\n","axs[0].set(xlabel='Transformer layer',ylabel='Mean',title='A) Average of cosine similarity dist.')\n","\n","axs[1].plot(,label='$\\\\mathbf{W_Q}$ within head')\n","axs[1].plot(,label='$\\\\mathbf{W_Q}$ across head')\n","axs[1].plot(,label='$\\\\mathbf{W_K}$ within head')\n","axs[1].plot(,label='$\\\\mathbf{W_K}$ across head')\n","axs[1].legend(handlelength=4) # make the legend lines longer to show dashed vs solid\n","axs[1].set(xlabel='Transformer layer',ylabel='Standard deviation',title='B) Stdev of cosine similarity dist.')\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj33_part5c.png')\n","plt.show()"],"metadata":{"id":"nMLB0qtPO0jz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vLbKPTiN_OKP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 6: WQ, WK, WV similarities across models**"],"metadata":{"id":"dG0_Nu_2_OFF"}},{"cell_type":"code","source":["_,axs = plt.subplots(len(models),2,figsize=(10,8))\n","\n","\n","# loop over models\n","for modeli,whichmod in enumerate(models.keys()):\n","\n","  # initialize matrix to store the metaparameters\n","  cossim_stats = np.zeros((params[whichmod]['n_layers'],3,2))\n","\n","  # start of the progress report\n","  print(f\"\\n\\nWorking on ...\n","\n","  # mask for non-redundant and non-trivial indices\n","  # it is model-specific but not layer-specific\n","  N = params[whichmod]['n_emb']\n","  mask =\n","\n","\n","  # loop over layers\n","  for layeri in range(params[whichmod]['n_layers']):\n","\n","    # update the layer number in the progress report\n","    print(f'{\n","\n","    # split into Q, K, V\n","    Q,K,V = torch.split()\n","\n","    ### Q\n","    Q = # normalize\n","    cs = # full similarity matrix\n","    cs = # extract mask elements\n","    cossim_stats[layeri,0,0] = # the mean\n","    cossim_stats[layeri,0,1] = # stdev\n","\n","\n","    ### repeat for K\n","    cossim_stats[layeri,1,0] =\n","    cossim_stats[layeri,1,1] =\n","\n","\n","    ### and for V\n","\n","\n","  # end of layer loop\n","\n","  for i in [0,1]:\n","    axs[modeli,i].plot(cossim_stats[:,0,i],linewidth=2,label='$\\\\mathbf{W_Q}$')\n","    axs[modeli,i].plot(cossim_stats[:,1,i],linewidth=2,label='$\\\\mathbf{W_K}$')\n","    axs[modeli,i].plot(cossim_stats[:,2,i],linewidth=2,label='$\\\\mathbf{W_V}$')\n","    axs[modeli,i].legend(fontsize=8)\n","    axs[modeli,i].set(xlabel='Transformer layer',ylabel=['Mean','Stdev'][i],\n","                      title=f'GPT2-{whichmod}')\n","\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj33_part6.png')\n","plt.show()"],"metadata":{"id":"fu7uqvyuB9Op"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Db3K2ppQRe-N"},"execution_count":null,"outputs":[]}]}