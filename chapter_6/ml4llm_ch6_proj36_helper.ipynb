{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[36] Characteristics of attention adjustment magnitudes</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BvQj17hzqwM"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.gridspec import GridSpec\n","\n","from datasets import load_dataset\n","\n","import torch\n","from transformers import AutoModelForCausalLM, GPT2Tokenizer\n","import torch.nn.functional as F"]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300\n","})"],"metadata":{"id":"dy4A-ah8kzZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Eja6hB4TfIAU"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# **Part 1: Model, tokens, projection vectors**"],"metadata":{"id":"oGIKYsGKEEO-"}},{"cell_type":"code","source":["# load GPT2 model and tokenizer\n","model = AutoModelForCausalLM.from_pretrained('gpt2-large')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","model.eval()"],"metadata":{"id":"DugL7dpykd_h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_layers = model.config.n_layer"],"metadata":{"id":"iV5ChX9th-Pf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hook the attention vectors\n","activations = {}\n","\n","def implant_hook(layer_number):\n","  def hook(module,input,output):\n","\n","  return hook\n","\n","# implant the hooks\n","handles = []\n","for i in range(n_layers):\n","  h = model.transformer.h[i].\n","  handles.append(h)"],"metadata":{"id":"jw3-10N9fbGZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load a dataset\n","dataset = load_dataset('cornell-movie-review-data/rotten_tomatoes', split='test')\n","dataset"],"metadata":{"id":"Go6KXi9ofbDd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset[30]"],"metadata":{"id":"UBm2MHaYfa_f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batchsize = 64\n","\n","txt = []\n","\n","tokenizer.pad_token =\n","tokens = tokenizer(txt,)\n","n_tokens =  # will be helpful later\n","\n","# create the boolean attention mask\n","token_mask =\n","\n","# visualize\n","fig = plt.figure(figsize=(12,3.5))\n","gs = GridSpec(1,3,figure=fig)\n","ax0 = fig.add_subplot(gs[:-1])\n","ax1 = fig.add_subplot(gs[-1])\n","\n","ax0.pcolor\n","ax0.set(xlabel='Token position',ylabel='Sequence in batch',title='A) Heatmap of valid token positions')\n","ax0.spines.top.set_visible(True) # switched off by default, but I want the top spine here\n","\n","ax1.hist(,bins='fd',color='gray',edgecolor='k')\n","ax1.set(xlabel='Sequence length',ylabel='Token count',title='B) Distribution of sequence lengths')\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj36_part1.png')\n","plt.show()"],"metadata":{"id":"eulojD5gfa8y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","  outputs =\n","\n","for k,v in activations.items():\n","  print(f'{k:>8} has shape')"],"metadata":{"id":"9FxwBGrji3vx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-QNWN0DAo0G1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: Laminar profile of attention adjustment magnitudes**"],"metadata":{"id":"A96T9QV3o0Ds"}},{"cell_type":"code","source":["# on broadcast-masking\n","print(activations['attn_L4'][tokens['attention_mask']].shape)\n","print(activations['attn_L4'][token_mask].shape)"],"metadata":{"id":"-EEnZLcli3su"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["activations['attn_L4'][tokens['attention_mask']][:,1,1,:]"],"metadata":{"id":"O4fNGQ3FKDlz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(12,3.5))\n","\n","for layeri in range(n_layers):\n","\n","  # get the adjustments (ignoring the first token)\n","  att_projs = activations[f'attn_L{}'][][]\n","  att_projs =  # using numpy here\n","\n","  # histogram of log-norms\n","  norms = np.( np.(att_projs,axis=) )\n","  y,x = np.histogram(norms,bins=,density=)\n","\n","  # and visualize\n","  axs[0].plot(,color=plt.cm.plasma(layeri/n_layers),linewidth=2,label=f'L{layeri}')\n","  axs[1].errorbar(,color=plt.cm.plasma(layeri/n_layers))\n","  axs[1].plot(,'kh',markeredgewidth=.5,markersize=12,markerfacecolor=plt.cm.plasma(layeri/n_layers))\n","\n","# plot adjustments\n","axs[0].set(xlabel='Log vector norms',ylabel='Density',title='A) Distribution of attention adjustment magnitudes')\n","axs[1].set(xlabel='Transformer layer',ylabel='Log vector norm (ave)',title='B) Average adjustment magnitude per layer')\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj36_part2.png')\n","plt.show()"],"metadata":{"id":"PUNr0MermM97"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YA0lNlJqmM7I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: Adjustment norm by token position**"],"metadata":{"id":"pvvpheArqQHq"}},{"cell_type":"code","source":["# reminder for reference\n","activations['attn_L4'].shape"],"metadata":{"id":"b67jC6FfqQEf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# example of averaging only the valid tokens\n","all_norms = torch.norm()\n","ave_norms = np.zeros()\n","\n","# solved with a for-loop (list comprehension solution in the next code block)\n","for t in range():\n","  this_pos_mask =\n","  valid_norms =\n","  ave_norms[t] =\n","\n","all_norms.shape, ave_norms.shape"],"metadata":{"id":"fALx97pBqQBc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,4))\n","\n","X = np.zeros((n_layers,n_tokens))\n","\n","# loop over layers\n","for layeri in range():\n","\n","  # get the norms\n","  norms =\n","\n","  # average all the norms for each position, only for rows with mask=True\n","  norm_by_pos =\n","  X[layeri,:] = norm_by_pos\n","\n","# percentile-based color values\n","cmin,cmax = np.percentile(,[,])\n","\n","# and visualize\n","plt.imshow(X,origin='lower',vmin=cmin,vmax=cmax,aspect='auto',cmap='magma')\n","\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj36_part3a.png')\n","plt.show()"],"metadata":{"id":"tLWVUNwwfu4M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,3))\n","plt.plot(,'kh',markersize=12,markerfacecolor=[.9,.7,.7,.7])\n","\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj36_part3b.png')\n","plt.show()"],"metadata":{"id":"kVSnP-i_qLsB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QIRPaRHlAjiU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 4: Predicting adjustment norm from token position**"],"metadata":{"id":"2pBQSZDdAjfZ"}},{"cell_type":"code","source":["tokens2use = 30\n","\n","# the design matrix with two IVs\n","designMatrix = np.vstack((\n","    np.ones(),  # intercept\n","    np.arange()  # token position\n","\n","\n","_,axs = plt.subplots(1,3,figsize=(12,3))\n","\n","for layeri in range(n_layers):\n","\n","  # same code as in previous part\n","  norms =\n","  norm_by_pos =\n","  norm_by_pos = np.log(norm_by_pos)\n","\n","  # fit a least-squares model\n","  betas = np.linalg.lstsq(,norm_by_pos[])\n","  axs[0].plot(range(1,tokens2use+1),,'.-',markersize=9,color=plt.cm.plasma(layeri/n_layers))\n","  axs[1].plot(,'ks',markerfacecolor=plt.cm.plasma(layeri/n_layers),markersize=10)\n","  axs[2].plot(,'ko',markerfacecolor=plt.cm.plasma(layeri/n_layers),markersize=10)\n","\n","\n","axs[0].set(xlabel='Token index',ylabel='Log vector norm',title='A) Attention projection norm')\n","axs[1].set(xlabel='Transformer layer',ylabel='Intercept ($\\\\beta_0$)',title='B) Regression intercept')\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj36_part4.png')\n","plt.show()"],"metadata":{"id":"5X-Z6IoS5LQT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1Wxe-Qu95K4o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 5: Adjustment norm by previous norms**"],"metadata":{"id":"Ht_McjEdAcCz"}},{"cell_type":"code","source":["# initialize matrix of beta values\n","betas = np.zeros((n_layers,3))\n","\n","# loop over layers\n","for layeri in range(n_layers):\n","\n","  # initialize an empty list to hold the data\n","  data =\n","\n","  # and get all the norms from this layer\n","  norms = torch.norm(activations[f'attn_L{layeri}'],dim=-1)\n","\n","  # loop over all sequences in the batch\n","  for seqi in range(batchsize):\n","\n","    # just the norms for valid tokens (excluding the first)\n","    validtokens =\n","    normseq = norms[seqi,validtokens]\n","\n","    # create each row in the dataset\n","    for i in range(2,len(normseq)):\n","      data.append\n","\n","  # and stack them into an array\n","  data = np.log(\n","\n","  designMat = np.hstack((\n","  y = data[:,-1]\n","\n","  betas[layeri,:] = np.linalg.lstsq(,)[0]"],"metadata":{"id":"RPDSDFvDB3lQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.shape"],"metadata":{"id":"XB9uJegM2ADl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,4))\n","\n","plt.axhline(0,linestyle='--',color='k',linewidth=.5)\n","plt.plot(,'gs-',linewidth=.5,markerfacecolor=[.7,.9,.7,.7],markersize=12,label='t-2')\n","plt.plot(,'bo-',linewidth=.5,markerfacecolor=[.7,.7,.9,.7],markersize=12,label='t-1')\n","\n","plt.gca().set(xlabel='Transformer layer',ylabel='$\\\\beta$ coefficient',title='Impact of previous attention adjustment')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj36_part5.png')\n","plt.show()"],"metadata":{"id":"FYrx8LVSB3iY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sIC-S0mlB3fk"},"execution_count":null,"outputs":[]}]}