{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[37] Token prediction and attention KL divergences</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BvQj17hzqwM"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import requests\n","\n","from tqdm import tqdm\n","\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch.nn.functional as F"]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300\n","})"],"metadata":{"id":"dy4A-ah8kzZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Eja6hB4TfIAU"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# **Part 1: Model, tokens, attention projections**"],"metadata":{"id":"oGIKYsGKEEO-"}},{"cell_type":"code","source":["# load GPT2 model and tokenizer\n","model = AutoModelForCausalLM.from_pretrained('gpt2-large')\n","tokenizer = AutoTokenizer.from_pretrained('gpt2-large')\n","\n","model.eval()"],"metadata":{"id":"DugL7dpykd_h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_layers = model.config.n_layer"],"metadata":{"id":"iV5ChX9th-Pf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hook the attention projection vectors\n","att_projs = {}\n","\n","def implant_hook(layer_number):\n","  def hook(module, input, output):\n","    att_projs[f'attn_L{layer_number}'] = output.detach()\n","  return hook\n","\n","# implant the hooks\n","handles = []\n","for i in range(n_layers):\n","  h = model.transformer.h[i].attn.c_proj.register_forward_hook(implant_hook(i))\n","  handles.append(h)"],"metadata":{"id":"jw3-10N9fbGZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Through the Looking Glass (Alice in Wonderland)\n","text = requests.get('https://www.gutenberg.org/cache/epub/11/pg11.txt').text\n","\n","allTokens = tokenizer.encode(text,return_tensors='pt')\n","max_seq_len = model.config.max_position_embeddings\n","\n","# get context-length from middle of the book\n","start_idx = len(allTokens[0])//2\n","end_idx = start_idx + max_seq_len\n","\n","tokens = allTokens[:,start_idx:end_idx]\n","tokens.shape"],"metadata":{"id":"4si33wfp7DLO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# vector of accurate token predictions\n","with torch.no_grad():\n","  outs = model(tokens)\n","\n","# print attention activation sizes\n","for k,v in att_projs.items():\n","  print(f'{k:>8} has shape {list(v.shape)}')"],"metadata":{"id":"zS_9MuGE7DH7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7me269FvmM39"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: Token prediction accuracy**"],"metadata":{"id":"cBwfjFvRmMw0"}},{"cell_type":"code","source":["prediction_acc = np.zeros(max_seq_len,dtype=bool)\n","toklens = np.zeros(max_seq_len)\n","\n","for toki in range(max_seq_len):\n","\n","  # target token (taken from 'allTokens', not 'tokens'!)\n","  targettok = allTokens[0,start_idx+toki+1]\n","\n","  # test whether the max-logit on this token matches the next token\n","  prediction_acc[toki] = np.argmax(outs.logits[0,toki,:]) == targettok\n","\n","  toklens[toki] = len(tokenizer.decode(targettok))\n","\n","print(f'Model correctly predicted {prediction_acc.mean():.2%} ({prediction_acc.sum()}/{len(prediction_acc)})')"],"metadata":{"id":"-HXwKLAY7DFA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,3))\n","plt.plot(prediction_acc+np.random.normal(0,.05,max_seq_len),'ws',markerfacecolor='k',alpha=.7)\n","plt.gca().set(yticks=[0,1],yticklabels=['Incorrect','Correct'],ylim=[-.5,1.5],\n","              xlabel='Token position',title='Accuracy by token position')\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj37_part2a.png')\n","plt.show()"],"metadata":{"id":"Di1l_zzyJMwD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,3))\n","y,x = np.histogram(toklens[prediction_acc==False],bins='fd',density=True)\n","plt.plot(x[:-1],y,'rs-',markersize=10,markerfacecolor=[.9,.5,.5],\n","         label=f'Incorrect (ave. {toklens[prediction_acc==False].mean():.2f} chars)')\n","\n","y,x = np.histogram(toklens[prediction_acc==True],bins='fd',density=True)\n","plt.plot(x[:-1],y,'go-',markersize=10,markerfacecolor=[.5,.9,.5],\n","         label=f'Correct (ave. {toklens[prediction_acc==True].mean():.2f} chars)')\n","\n","plt.legend()\n","plt.gca().set(xlabel='Token length (characters)',ylabel='Density',\n","              title='Token length distributions by prediction accuracy')\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj37_part2b.png')\n","plt.show()"],"metadata":{"id":"wUWSJNB9A2pn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1WOI6CwxJMtB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: Attention projection norms by accuracy (one layer)**"],"metadata":{"id":"Qw-TI2-lJMp-"}},{"cell_type":"code","source":["layeri = 4\n","\n","# adjustment vectors and their norms\n","# C = correct prediction; I = incorrect prediction\n","C = att_projs[f'attn_L{layeri}'][0,prediction_acc,:]\n","C_norms = torch.norm(C,dim=-1).log() # using method here instead of function\n","\n","I = att_projs[f'attn_L{layeri}'][0,~prediction_acc,:]\n","I_norms = torch.norm(I,dim=-1).log()\n","\n","# histogram bins\n","minmax = [ min(min(C_norms),min(I_norms)),\n","           max(max(C_norms),max(I_norms)) ]\n","histbins = torch.linspace(minmax[0],minmax[1],31)\n","\n","# histogram data with counts\n","yC,_ = torch.histogram(C_norms,bins=histbins,density=False)\n","yI,_ = torch.histogram(I_norms,bins=histbins,density=False)\n","\n","# then convert to probability (not densities)\n","yC = yC/yC.sum()\n","yI = yI/yI.sum()\n","\n","\n","plt.figure(figsize=(10,4))\n","plt.plot(histbins[:-1],yC,linewidth=2,label='Correct')\n","plt.plot(histbins[:-1],yI,linewidth=2,label='Incorrect')\n","\n","plt.gca().set(xlabel='Log-norm',ylabel='Probability')\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj37_part3.png')\n","plt.show()"],"metadata":{"id":"BcXZEmC_7C_f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_count,_ = torch.histogram(C_norms,bins=histbins,density=False)\n","y_density,_ = torch.histogram(C_norms,bins=histbins,density=True)\n","\n","print(f'Sum of counts: {y_count.sum()}')\n","print(f'Sum of density: {y_density.sum():.2f}')\n","print(f'Sum of bin-normalized density: {y_density.sum() * (histbins[1]-histbins[0]):.2f}')\n","print(f'Sum of count-scaled probability: {sum(y_count/y_count.sum()):.2f}')"],"metadata":{"id":"yxsxKZe17NwS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KX-y0q0XDmuk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 4: Asymmetric and symmetric KL divergence**"],"metadata":{"id":"IZsWoTiGSdov"}},{"cell_type":"code","source":["F.kl_div(torch.log(yC),yI)#,reduction='batchmean')\n","# torch.__version__ # to check version"],"metadata":{"id":"sEo97edmS1vT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# first argument is the \"input\" (q); second argument is \"target\" (p)\n","c2i = F.kl_div(torch.log(yC+1e-15),yI,reduction='batchmean')\n","i2c = F.kl_div(torch.log(yI+1e-15),yC,reduction='batchmean')\n","\n","print(f'Target is \"incorrect\": {c2i:.4f}')\n","print(f'Target is \"correct\"  : {i2c:.4f}')"],"metadata":{"id":"7W5kNe34Sjtl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# symmetric approach (Jensen-Shannon divergence)\n","pAve = (yC+yI)/2\n","\n","symKL = F.kl_div(torch.log(yC + 1e-15), pAve, reduction='batchmean') + \\\n","        F.kl_div(torch.log(yI + 1e-15), pAve, reduction='batchmean')\n","symKL /= 2\n","\n","print(f'Average KLs : {(c2i+i2c)/2:.4f}')\n","print(f'Symmetric KL: {symKL:.4f}')"],"metadata":{"id":"pwVxJhPRTE5T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# JSD function\n","def symmetric_KL(d1,d2,nbins=31):\n","\n","  # histogram bins\n","  minmax = [ min(min(d1),min(d2)),\n","             max(max(d1),max(d2)) ]\n","  histbins = torch.linspace(minmax[0],minmax[1],nbins)\n","\n","\n","  # histogram data in probabilities\n","  yd1,_ = torch.histogram(d1,bins=histbins)\n","  yd2,_ = torch.histogram(d2,bins=histbins)\n","  yd1 = yd1/yd1.sum()\n","  yd2 = yd2/yd2.sum()\n","\n","  # get the average probability distribution\n","  pAve = (yd1+yd2)/2\n","\n","  # and calculate symmetric KL\n","  symKL = F.kl_div(torch.log(yd1 + 1e-15), pAve, reduction='batchmean') + \\\n","          F.kl_div(torch.log(yd2 + 1e-15), pAve, reduction='batchmean')\n","  return symKL/2"],"metadata":{"id":"U-lrUvHDNB35"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check for symmetry\n","symmetric_KL(C_norms,I_norms), symmetric_KL(I_norms,C_norms)"],"metadata":{"id":"LyunVt4oLI6J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initializations\n","bincounts = np.arange(10,51)\n","kl_divs = np.zeros(len(bincounts))\n","\n","# run the experiment!\n","for bini in range(len(bincounts)):\n","  kl_divs[bini] = symmetric_KL(C_norms,I_norms,bincounts[bini])\n","\n","\n","# visualize\n","plt.figure(figsize=(8,4))\n","plt.plot(bincounts,kl_divs,'kh',markersize=10,markerfacecolor=[.7,.7,.9])\n","plt.gca().set(xlabel='Number of histogram bins',ylabel='Divergence value',title='Impact of discretization on symmetric KL divergence')\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj37_part4.png')\n","plt.show()"],"metadata":{"id":"HQboE7DxKoTQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"e6pdTkUkSjqp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 5: Statistical significance of KL**"],"metadata":{"id":"HZcl8QWWSdlY"}},{"cell_type":"code","source":["# observed KL distance\n","kldiv = symmetric_KL(C_norms,I_norms)\n","\n","# number of permutes\n","n_permutes = 1000\n","\n","# H0 distribution\n","kl_perms = torch.zeros(n_permutes)\n","for permi in range(n_permutes):\n","\n","  # permuted predictions\n","  fake_predictions = np.random.permutation(prediction_acc)\n","  C = att_projs[f'attn_L{layeri}'][0,fake_predictions,:]\n","  C_norms_p = torch.norm(C,dim=-1).log()\n","\n","  I = att_projs[f'attn_L{layeri}'][0,~fake_predictions,:]\n","  I_norms_p = torch.norm(I,dim=-1).log()\n","\n","  # calculate and store KL\n","  kl_perms[permi] = symmetric_KL(C_norms_p,I_norms_p)\n","\n","# p-value is number of H0 values greater than observed value\n","pval = (kl_perms>kldiv).sum() / n_permutes"],"metadata":{"id":"bsQ5BA33V44z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,3))\n","plt.hist(kl_perms,bins='fd',edgecolor='k',linewidth=.3,color=[.5,.3,.3],label='$H_0$ dist.')\n","plt.axvline(kldiv,linewidth=4,color='k',label='Observed KL')\n","\n","plt.gca().set(xlabel='KL divergence',ylabel='Count',title=f'Statistical evaluation of KL (p = {pval:.3f})')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj37_part5.png')\n","plt.show()"],"metadata":{"id":"chntyV27VPnw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qSLQT4-CV40k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 6: Laminar distribution of KL divergences**"],"metadata":{"id":"PI8c1I1uV4uw"}},{"cell_type":"code","source":["n_permutes = 1000\n","\n","_,axs = plt.subplots(1,3,figsize=(12,3))\n","\n","\n","for layeri in tqdm(range(n_layers)):\n","\n","  # adjustment vectors and their norms\n","  C = att_projs[f'attn_L{layeri}'][0,prediction_acc,:]\n","  C_norms = torch.norm(C,dim=-1).log()\n","\n","  I = att_projs[f'attn_L{layeri}'][0,~prediction_acc,:]\n","  I_norms = torch.norm(I,dim=-1).log()\n","\n","  # panel A: means of the norms\n","  axs[0].plot(layeri,I_norms.mean(),'rh',markerfacecolor=[.9,.5,.5,.7],markersize=7)\n","  axs[0].plot(layeri,C_norms.mean(),'bs',markerfacecolor=[.5,.5,.9,.7],markersize=7)\n","\n","  diff = C_norms.mean() - I_norms.mean()\n","\n","\n","  # observed KL distance\n","  kldiv = symmetric_KL(C_norms,I_norms).item()\n","\n","  # H0 distribution via permutation testing\n","  kl_perms = np.zeros(n_permutes)\n","  for permi in range(n_permutes):\n","\n","    # permuted predictions\n","    fake_predictions = np.random.permutation(prediction_acc)\n","    C = att_projs[f'attn_L{layeri}'][0,fake_predictions,:]\n","    C_norms = torch.norm(C,dim=-1).log()\n","\n","    I = att_projs[f'attn_L{layeri}'][0,~fake_predictions,:]\n","    I_norms = torch.norm(I,dim=-1).log()\n","\n","    # calculate and store KL\n","    kl_perms[permi] = symmetric_KL(C_norms,I_norms)\n","\n","  # p-value is number of H0 values greater than observed value\n","  pval = (kl_perms>kldiv).sum() / n_permutes\n","\n","  axs[1].plot([layeri,layeri],[kl_perms.min(),kl_perms.max()],'k',linewidth=.4)\n","  axs[1].plot(layeri,kl_perms.mean(),'ks',markersize=4)\n","\n","  # draw the observed KL according to significance\n","  if pval>.05:\n","\n","    # for panel B\n","    msize = 6\n","    axs[1].plot(layeri,kldiv,'kh',markerfacecolor=[.9,.5,.5,.7],markersize=msize)\n","\n","    # for panel C\n","    if diff>0:\n","      axs[2].plot(layeri,diff,'bs',markerfacecolor=[.7,.7,.9,.7],markersize=msize)\n","    else:\n","      axs[2].plot(layeri,diff,'rh',markerfacecolor=[.9,.7,.7,.7],markersize=msize)\n","\n","  else:\n","\n","    # panel B\n","    msize = 10\n","    axs[1].plot(layeri,kldiv,'kh',markerfacecolor=[.5,.9,.5,.7],markersize=msize)\n","\n","    # panel C\n","    if diff>0:\n","      axs[2].plot(layeri,diff,'bs',markerfacecolor=[.3,.3,.9,.7],markersize=msize)\n","    else:\n","      axs[2].plot(layeri,diff,'rh',markerfacecolor=[.9,.3,.3,.7],markersize=msize)\n","\n","\n","# finalize the figure\n","axs[0].legend(['Incorrect','Correct'])\n","axs[0].set(xlabel='Transformer layer',ylabel='Mean of norms',title='A) Distribution means')\n","axs[1].set(xlabel='Transformer layer',ylabel='KL divergence',title='B) Observed KL with $H_0$ distribution')\n","axs[2].axhline(0,linestyle='--',color='k',linewidth=.4)\n","axs[2].set(xlabel='Transformer layer',ylabel='Difference',title='C) Difference of means')\n","\n","\n","plt.tight_layout()\n","plt.savefig('ch6_proj37_part6.png')\n","plt.show()"],"metadata":{"id":"DSNZ9gWpDmr4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Zk7wJKQxfa5p"},"execution_count":null,"outputs":[]}]}