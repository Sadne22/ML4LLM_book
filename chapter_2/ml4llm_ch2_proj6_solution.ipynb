{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[6] Tokenization and compression in different languages</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","source":[],"metadata":{"id":"R5SI-Iyy4dtm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt"],"metadata":{"id":"rn8Fcyf87EXx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300,\n","})"],"metadata":{"id":"dy4A-ah8kzZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UGHCakBb7EUl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# GPT2's tokenizer\n","from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained('gpt2')"],"metadata":{"id":"-LMjpL-0kdmm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dPOPtbvUkdjp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 1: Compression in different languages**"],"metadata":{"id":"LFnWFXMGzsCm"}},{"cell_type":"code","source":["# Google-translated ;)\n","sentences = [\n","    ['English',   \"In Penny Lane, there is a barber showing photographs of every head he's had the pleasure to have known. And all the people that come and go stop and say hello.\"],\n","    ['Spanish',   \"En Penny Lane hay un barbero que muestra fotografías de todas las personas que ha tenido el placer de conocer. Y todas las personas que van y vienen se detienen y dicen hola.\"],\n","    ['Arabic',    \"في شارع بيني لين، يوجد حلاق يعرض صورًا لكل شخص كان له شرف معرفته. وكل الناس الذين يأتون ويذهبون يتوقفون ويقولون مرحبًا.\"],\n","    ['Persian',   \"در خیابان پنی لین، یک آرایشگر وجود دارد که عکس‌های تمام افرادی را که افتخار آشنایی با آن‌ها را داشته به نمایش می‌گذارد. و همهٔ کسانی که می‌آیند و می‌روند می‌ایستند و سلام می‌کنند.\"],\n","    ['Lithuanian',\"Penny Lane gatvėje yra kirpėjas, rodantis nuotraukas visų žmonių, kuriuos jam teko su malonumu pažinti. Ir visi žmonės, kurie ateina ir išeina, sustoja ir pasisveikina.\"],\n","    ['Chinese',   \"在彭尼巷，有一位理发师正在展示他曾有幸认识的每一个人的照片。所有来来往往的人都会停下来打招呼。\"],\n","    ['Tamil',     \"பென்னி லேனில், தான் அறிந்த ஒவ்வொருவரையும் அறிந்ததற்கான மகிழ்ச்சியுடன் அவர்களின் புகைப்படங்களை காட்டும் ஒரு முடி திருத்துபவர் இருக்கிறார். வரவும் போகவும் 하는 அனைத்து மக்களும் நின்று வணக்கம் சொல்கிறார்கள்.\"],\n","    ['Hebrew',    \"בפני ליין יש ספר שמציג תמונות של כל אדם שהיה לו העונג להכיר. וכל האנשים שבאים והולכים עוצרים ואומרים שלום.\"],\n","            ]"],"metadata":{"id":"Vll_zH5t_gHC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initializations\n","tokenCount = []\n","charsCount = []\n","bytesCount = []\n","table = ''\n","\n","for lang,text in sentences:\n","\n","  # tokenize the text\n","  tokens = tokenizer.encode(text)\n","  print(f'{lang:>10}: {tokenizer.decode(tokens)}')\n","\n","  # count\n","  tokenCount.append(len(tokens))\n","  charsCount.append(len(text))\n","  bytesCount.append(len(text.encode('utf-8')))\n","\n","  # format a table row and store\n","  table += f'{lang:>10} |   {charsCount[-1]:3}   |   {bytesCount[-1]:3}   |  {tokenCount[-1]:3}\\n'"],"metadata":{"id":"f34QAbA-_gD-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print the table\n","print(' Language  |  Chars  |  Bytes  |  Tokens ')\n","print('-----------+---------+---------+---------')\n","print(table)"],"metadata":{"id":"74--GjBJSnna"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"i9WWW3WCoWev"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: Visualizations**"],"metadata":{"id":"Na8T4E4zSnhc"}},{"cell_type":"code","source":["fig,axs = plt.subplots(1,3,figsize=(12,3.5))\n","\n","# for axis limits\n","maxCount = 1.1 * np.max([np.max(charsCount),np.max(bytesCount),np.max(tokenCount)])\n","\n","for i in range(len(sentences)):\n","\n","  # extract the first letter of the language\n","  langMark = rf'${sentences[i][0][0]}$'\n","\n","  # plot characters by bytes\n","  axs[0].plot(charsCount[i],bytesCount[i],marker=langMark,\n","           markersize=13,color=plt.cm.Dark2(i/len(sentences)))\n","\n","  # characters by tokens\n","  axs[1].plot(charsCount[i],tokenCount[i],marker=langMark,\n","           markersize=13,color=plt.cm.Dark2(i/len(sentences)))\n","\n","  # bytes by tokens\n","  axs[2].plot(bytesCount[i],tokenCount[i],marker=langMark,\n","           markersize=13,color=plt.cm.Dark2(i/len(sentences)))\n","\n","\n","for a in axs:\n","  a.plot([0,maxCount],[0,maxCount],\n","         '--',color=[.7,.7,.7],zorder=-23)\n","\n","axs[0].set(xlabel='Character count',ylabel='Byte count',xlim=[0,maxCount],ylim=[0,maxCount],\n","           title='A) Character vs. byte counts')\n","axs[1].set(xlabel='Character count',ylabel='Token count',xlim=[0,maxCount],ylim=[0,maxCount],\n","           title='B) Character vs. token counts')\n","axs[2].set(xlabel='Bytes count',ylabel='Token count',xlim=[0,maxCount],ylim=[0,maxCount],\n","           title='C) Byte vs. token counts')\n","\n","plt.tight_layout()\n","plt.savefig('ch2_proj6_part2.png')\n","plt.show()"],"metadata":{"id":"Y6LxEvjZr09E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xka_woYEzcgR"},"execution_count":null,"outputs":[]}]}