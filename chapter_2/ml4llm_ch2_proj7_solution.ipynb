{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[2] Book lengths in characters, words, and tokens</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","source":[],"metadata":{"id":"R5SI-Iyy4dtm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import requests\n","import matplotlib.pyplot as plt"],"metadata":{"id":"rn8Fcyf87EXx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300,\n","})"],"metadata":{"id":"dy4A-ah8kzZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fmMz1q8qubD-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 1: Number of tokens**"],"metadata":{"id":"5poSQBOjHD6r"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QnBjS3LAujoq"},"outputs":[],"source":["# OpenAI cl100k_base tokenizer (used by GPT-4)\n","import tiktoken\n","tokenizer_4 = tiktoken.get_encoding('cl100k_base')"]},{"cell_type":"code","source":["# GPT2's tokenizer\n","from transformers import AutoTokenizer\n","tokenizer_2 = AutoTokenizer.from_pretrained('gpt2')"],"metadata":{"id":"xEFoa9xSGZpe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check all attributes\n","dir(tokenizer_4)"],"metadata":{"id":"OrAq0iZIIAJF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'GPT-4 tokenizer has {tokenizer_4.n_vocab:7,} tokens.')\n","print(f'GPT-2 tokenizer has {tokenizer_2.vocab_size:7,} tokens.')"],"metadata":{"id":"kixCk7xWGZyR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"V5OcJNbIMOXr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: Token byte length distributions**"],"metadata":{"id":"jHta_Z4OMOO6"}},{"cell_type":"code","source":["# using initialization and a for-loop\n","lengths_2 = np.zeros(tokenizer_2.vocab_size,dtype=int)\n","for t in range(tokenizer_2.vocab_size):\n","  lengths_2[t] = len(tokenizer_2.decode(t).encode('utf-8'))\n","\n","# using list-comprehension\n","#  (note: this is a list whereas the loop version is a numpy array, though that doesn't matter for this project)\n","lengths_2 = [ len(tokenizer_2.decode(t).encode('utf-8')) for t in range(tokenizer_2.vocab_size) ]"],"metadata":{"id":"Uwg7r8HVIOuu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize\n","lengths_4 = np.zeros(tokenizer_4.n_vocab,dtype=int)\n","\n","# get the lengths\n","for t in range(tokenizer_4.n_vocab):\n","\n","  # some token IDs cannot be decoded in isolation\n","  try:\n","    lengths_4[t] = len(tokenizer_4.decode([t]).encode('utf-8'))\n","  except:\n","    lengths_4[t] = -1"],"metadata":{"id":"vdVnesnNIJzv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# bin counts for lengths\n","bincounts_2 = np.bincount(lengths_2)\n","bincounts_4 = np.bincount(lengths_4[lengths_4>-1])\n","\n","# note: first element in np.bincount output is for count=0, which we can ignore\n","\n","# plot\n","plt.figure(figsize=(10,4))\n","plt.plot(range(1,max(lengths_4)+1),bincounts_4[1:]/bincounts_4.max(),'s-',color=[.3,.9,.3],markerfacecolor=[.7,.9,.7],label='GPT-4')\n","plt.plot(range(1,max(lengths_2)+1),bincounts_2[1:]/bincounts_2.max(),'o-',color=[.3,.3,.9],markerfacecolor=[.7,.7,.9],label='GPT-2')\n","\n","plt.legend(fontsize=14)\n","plt.gca().set(xlabel='Token length (byte)',ylabel='Density',\n","              title='Distributions of token lengths',yscale='log',xscale='log')\n","\n","plt.tight_layout()\n","plt.savefig('ch2_proj7_part2.png')\n","plt.show()"],"metadata":{"id":"I7lizgRak_KG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ztwT7rLDKTaO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: Text token lengths**"],"metadata":{"id":"59R9rK83Ms6E"}},{"cell_type":"code","source":["# all books have the same url format;\n","# they are unique by numerical code\n","baseurl = 'https://www.gutenberg.org/cache/epub/'\n","\n","bookurls = [\n","    # code       title\n","    ['84',    'Frankenstein'    ],\n","    ['64317', 'GreatGatsby'     ],\n","    ['11',    'AliceWonderland' ],\n","    ['1513',  'RomeoJuliet'     ],\n","    ['76',    'HuckFinn'        ],\n","    ['219',   'HeartDarkness'   ],\n","    ['2591',  'GrimmsTales'     ],\n","    ['2148',  'EdgarAllenPoe'   ],\n","    ['36',    'WarOfTheWorlds'  ],\n","    ['829',   'GulliversTravels']\n","]"],"metadata":{"id":"Ote3clXlMzCW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize\n","tokens_2 = np.zeros(len(bookurls))\n","tokens_4 = np.zeros(len(bookurls))\n","token_lens_2 = np.zeros(len(bookurls))\n","token_lens_4 = np.zeros(len(bookurls))\n","\n","\n","# loop over books\n","for i,(code,title) in enumerate(bookurls):\n","\n","  # get the text\n","  fullurl = baseurl + code + '/pg' + code + '.txt'\n","  text = requests.get(fullurl).text\n","\n","  # tokenize the text\n","  gpt2_toks = tokenizer_2.encode(text)\n","  gpt4_toks = tokenizer_4.encode(text)\n","\n","  # count the numbers of tokens\n","  tokens_2[i] = len( gpt2_toks )\n","  tokens_4[i] = len( gpt4_toks )\n","\n","  # count the average lengths of the tokens\n","  token_lens_2[i] = np.mean([len(tokenizer_2.decode(t).encode('utf-8'))   for t in gpt2_toks])\n","  token_lens_4[i] = np.mean([len(tokenizer_4.decode([t]).encode('utf-8')) for t in gpt4_toks])\n"],"metadata":{"id":"eZJ3IgclM6Hd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# setup the figure\n","_,axs = plt.subplots(1,2,figsize=(10,4))\n","\n","### left panel: token counts\n","\n","# the scatter plot\n","axs[0].plot(tokens_2,tokens_4,'kh',markerfacecolor=[.9,.7,.7,.7],markersize=12)\n","\n","# line of unity\n","minlength = np.min([np.min(tokens_2),np.min(tokens_4)])\n","maxlength = np.max([np.max(tokens_2),np.max(tokens_4)])\n","axs[0].plot([minlength,maxlength],[minlength,maxlength],'--',color=[.5,.5,.5])\n","\n","# stylize\n","axs[0].set(xlabel='GPT-2 tokens',ylabel='GPT-4 tokens',title='A) Book lengths in tokens')\n","axs[0].ticklabel_format(style='scientific',axis='both',scilimits=(0,0))\n","\n","\n","### right panel: token lengths\n","axs[1].plot(token_lens_2,token_lens_4,'ks',markerfacecolor=[.7,.9,.7,.7],markersize=12)\n","\n","minlength = np.min([np.min(token_lens_2),np.min(token_lens_4)])\n","maxlength = np.max([np.max(token_lens_2),np.max(token_lens_4)])\n","axs[1].plot([minlength,maxlength],[minlength,maxlength],'--',color=[.5,.5,.5])\n","\n","# stylize\n","axs[1].set(xlabel='GPT-2 token lengths',ylabel='GPT-4 token lengths',title='B) Average token lengths (bytes)')\n","\n","\n","plt.tight_layout()\n","plt.savefig('ch2_proj7_part3.png')\n","plt.show()"],"metadata":{"id":"8Ojrfn5nNE7D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"85u_BsGRJh3g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 4: Translator functions**"],"metadata":{"id":"i5e1Du85GZg0"}},{"cell_type":"code","source":["# translation functions\n","def gpt2_to_4(toks):\n","  newtxt  = tokenizer_2.decode(toks)\n","  newtoks = tokenizer_4.encode(newtxt)\n","  return newtoks\n","\n","# this function has more compact code but is less human-readable\n","def gpt4_to_2(toks):\n","  return tokenizer_2.encode( tokenizer_4.decode(toks) )"],"metadata":{"id":"bD7vOFUBGZdd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = 'Canadian winters are kept WArM bY the friendliness of THE PEOPLE.'\n","print('Original text:\\n ',text,'\\n')\n","\n","# tokenize\n","toks_2 = tokenizer_2.encode(text)\n","toks_4 = tokenizer_4.encode(text)\n","\n","# print the original tokens\n","print('GPT-2 tokens:\\n ',toks_2)\n","print('GPT-4 tokens:\\n ',toks_4,'\\n')\n","\n","# show that the decoding reconstructs the original text\n","print('GPT-2 reconstruction:\\n ',tokenizer_2.decode(toks_2))\n","print('GPT-4 reconstruction:\\n ',tokenizer_4.decode(toks_4),'\\n')\n","\n","# and finally, translate between tokenizers\n","print('GPT-2 to GPT-4 translation:\\n ',tokenizer_4.decode(gpt2_to_4(toks_2)))\n","print('GPT-4 to GPT-2 translation:\\n ',tokenizer_2.decode(gpt4_to_2(toks_4)))"],"metadata":{"id":"433r4GcgPpTq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ffy5AB8uGZWh"},"execution_count":null,"outputs":[]}]}