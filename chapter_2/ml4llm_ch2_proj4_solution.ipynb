{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[4] Token lengths in characters and bytes</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","source":[],"metadata":{"id":"R5SI-Iyy4dtm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl"],"metadata":{"id":"rn8Fcyf87EXx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300,\n","})"],"metadata":{"id":"dy4A-ah8kzZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ProWlq4pAi2p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Demo of counting bytes**"],"metadata":{"id":"pXe5nf6ULKHR"}},{"cell_type":"code","source":["things = [ 'x','!','Ã¼','ðŸ¥°','æ´»' ]\n","\n","for t in things:\n","  print(f\"{len(t.encode('utf-8'))} bytes in {t}\")"],"metadata":{"id":"MwRh2XI-LZMM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UGHCakBb7EUl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 1: Character vs. byte lengths in all tokens**"],"metadata":{"id":"M-giUeU1E63n"}},{"cell_type":"code","source":["# GPT2 tokenizer\n","from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained('gpt2')"],"metadata":{"id":"wcRoGJQ5-Ytp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize count-by-count matrix\n","token_lengths = np.zeros((200,200),dtype=int)\n","token_lens_chars = np.zeros(tokenizer.vocab_size,dtype=int)\n","token_lens_bytes = np.zeros(tokenizer.vocab_size,dtype=int)\n","\n","\n","for i in range(tokenizer.vocab_size):\n","\n","  # token for this index\n","  thistoken = tokenizer.decode([i])\n","\n","  # lengths in vectors\n","  token_lens_chars[i] = len(thistoken)\n","  token_lens_bytes[i] = len(thistoken.encode('utf-8'))\n","\n","  # increment the matrix counter\n","  token_lengths[token_lens_chars[i],token_lens_bytes[i]] += 1\n","\n","\n","\n","### draw a figure\n","fig = plt.figure(figsize=(8,5))\n","\n","# normalization function for mapping frequency onto color\n","norm = mpl.colors.Normalize(vmin=0,vmax=np.log(token_lengths.max()))\n","\n","# draw the individual points\n","x,y = np.nonzero(token_lengths)\n","for xi,yi in zip(x,y):\n","  plt.plot(xi,yi,'kh',alpha=.7,markersize=9,\n","           markerfacecolor=plt.cm.magma(norm(np.log(token_lengths[xi,yi]))))\n","\n","sm = mpl.cm.ScalarMappable(cmap=mpl.cm.magma,norm=norm)\n","cbar = plt.colorbar(sm,ax=fig.gca(),pad=.01)\n","cbar.set_label('Log frequency')\n","\n","\n","# draw the line of equality\n","plt.plot([0,xi],[0,xi],'--',color=[.8,.8,.8],zorder=-100)\n","\n","# prettify the plot\n","plt.gca().set(xlabel='Token length in characters',\n","              ylabel='Token length in bytes')\n","\n","plt.tight_layout()\n","plt.savefig('ch2_proj4_part1.png')\n","plt.show()"],"metadata":{"id":"LMI3nZQTy7Jw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["noteq = np.where(token_lens_chars != token_lens_bytes)[0]\n","print(f'There are {len(noteq)} tokens with unequal lengths.\\nHere are 30:\\n')\n","\n","for idx in noteq[np.random.randint(0,len(noteq),30)]:\n","  print(f'{tokenizer.decode([idx])} | ',end='')"],"metadata":{"id":"7atO3bPQbhDC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Lev0BAcJbhA0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: Visualizing token lengths**"],"metadata":{"id":"9y_p7GYS8m9M"}},{"cell_type":"code","source":["plt.figure(figsize=(12,4))\n","\n","# use for plotting only the first N tokens\n","max2plot = tokenizer.vocab_size #600\n","\n","# note: I added a small y-axis offset to help visualize discrepancies\n","plt.plot(token_lens_chars[:max2plot]+.1,'rs',label='Characters',markerfacecolor=[.9,.7,.7,.7],markersize=5)\n","plt.plot(token_lens_bytes[:max2plot],'bo',label='Bytes',markerfacecolor=[.7,.7,.9,.7],markersize=5)\n","\n","plt.legend()\n","plt.gca().set(xlabel='Token index',ylabel='Token length',xlim=[-10,max2plot+10])\n","\n","plt.tight_layout()\n","plt.savefig('ch2_proj4_part2.png')\n","plt.show()"],"metadata":{"id":"egYqYLxaOkLG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VcDrWV6kBl8u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: Finding the \"mismatch-length\" tokens**"],"metadata":{"id":"XA0LCavCBl1d"}},{"cell_type":"code","source":["plt.figure(figsize=(12,4))\n","\n","# y-values are 0 or 1, depending on match\n","yVals = (token_lens_chars!=token_lens_bytes).astype(float)\n","yVals += np.random.normal(0,.02,len(yVals))\n","\n","# Not in the instructions, but also interesting to plot the discrepancy counts\n","#yVals = token_lens_bytes - token_lens_chars\n","\n","# plot them\n","plt.plot(yVals,'kh',markerfacecolor=[.9,.7,.7,.3],markersize=5)\n","\n","# and make the axis look nicer\n","plt.gca().set(xlabel='Token index',ylim=[-.5,1.5],xlim=[-30,tokenizer.vocab_size+30],\n","              yticks=[0,1],yticklabels=['Match','Mismatch'])\n","\n","plt.tight_layout()\n","plt.savefig('ch2_proj4_part3.png')\n","plt.show()"],"metadata":{"id":"79IGUtTPAQPC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0nFIlX93OkIR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 4: Length distributions**"],"metadata":{"id":"_Gt3Bj6ZOlGw"}},{"cell_type":"code","source":["# convert to percent of total\n","xB,yB = np.unique(token_lens_bytes,return_counts=True)\n","yB = 100*yB / yB.sum()\n","\n","xC,yC = np.unique(token_lens_chars,return_counts=True)\n","yC = 100*yC / yC.sum()\n","\n","# draw\n","plt.figure(figsize=(10,4))\n","plt.scatter(xB,yB,100+yB*10,c=xB,alpha=.7,cmap='rainbow',edgecolor='k',marker='s',label='Bytes')\n","plt.scatter(xC,yC,100+yC*10,c=xC,alpha=.7,cmap='rainbow',edgecolor='k',marker='o',label='Characters')\n","\n","plt.gca().set(xlabel='Token length',xticks=xB[::2],ylabel='Frequency (percent)')\n","plt.grid(color=[.4,.4,.4],linestyle='--')\n","plt.gca().set_axisbelow(True)\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.savefig('ch2_proj4_part4.png')\n","plt.show()"],"metadata":{"id":"Cn0IWwlzv4uP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the longest token :P\n","tokenizer.decode(np.argmax(token_lens_bytes))"],"metadata":{"id":"a0nbt3bX7mjf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-VYWVtY6AisW"},"execution_count":null,"outputs":[]}]}