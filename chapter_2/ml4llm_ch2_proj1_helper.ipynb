{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[1] Three tokenization schemes</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","source":[],"metadata":{"id":"R5SI-Iyy4dtm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt"],"metadata":{"id":"rn8Fcyf87EXx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","    'savefig.dpi':300,\n","})"],"metadata":{"id":"dy4A-ah8kzZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UGHCakBb7EUl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 1: Character-based tokenization**"],"metadata":{"id":"JeafpB7yEK9N"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"kA3jUiyu7DWw"},"outputs":[],"source":["# start with a text as one string variable\n","txt = 'The way you do anything is the way you do everything.'\n","\n","# use list-comprehension to create a list of characters\n","characters =\n","\n","# print the two versions\n","print('The full text:\\n',\n","print('As a list of characters:\\n',\n","\n","# the vocab (sorted set of unique elements, then make it a list)\n","char_vocab =\n","print('The vocabulary is:\\n',\n","\n","# print some numerical info\n","print(f'There are {} characters, {} of which are unique.')"]},{"cell_type":"code","source":["# the tokens and their indices\n","\n","# vertical format\n","# for index,tok in enumerate(char_vocab):\n","#   print(f'Index {index:2} is \"{tok}\"')\n","\n","# horizontal format is better for the book figure ;)\n","print('Token :',end='')\n","for v in char_vocab:\n","  print(f'', end='')\n","print('\\n','-'*131)\n"],"metadata":{"id":"lac5UsYty032"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize a list\n","tokens =\n","\n","# loop through text, find the indices into the vocab\n","# this is called encoding\n","for i,c in enumerate(txt):\n","  tokens[i] =\n","\n","print(tokens)"],"metadata":{"id":"b8fVSh18ru8Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a figure\n","_,ax = plt.subplots(1,figsize=(12,4))\n","\n","# plot the tokens\n","ax.plot(tokens,'ks',markersize=12,markerfacecolor=[.7,.7,.9])\n","ax.set(xlabel='Character index',yticks=range(len(char_vocab)))\n","ax.grid(linestyle='--',axis='y')\n","\n","# invisible axis for right-hand-side labels\n","\n","\n","plt.tight_layout()\n","plt.savefig('ch2_proj1_part1-tokenscatter.png')\n","plt.show()"],"metadata":{"id":"tq8FL58kr5GT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SzrAnJW77mpW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: Word-based tokenization**"],"metadata":{"id":"EULuiKyAErVG"}},{"cell_type":"code","source":["# split into words\n","words =\n","\n","# print the words\n","print('The list of words is:\\n',\n","\n","# find the unique words, and print numerical info\n","word_vocab =\n","print(f'There are {} words, {} of which are unique.\\n')\n","\n","print('The vocab is:\\n',word_vocab)"],"metadata":{"id":"HmxoWdgY7mme"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize a list\n","tokens = [0]*len(words)\n","\n","# loop through text, find the indices into the vocab\n","for i,c in enumerate(words):\n","  tokens[i] =\n","\n","print(tokens)"],"metadata":{"id":"m3fB8UAeuB-R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a figure\n","_,ax = plt.subplots(1,figsize=(10,3))\n","\n","# plot the tokens\n","ax.plot(tokens,'ks',markersize=15,markerfacecolor=[.7,.7,.9])\n","\n","# invisible axis for right-hand-side labels\n","\n","plt.tight_layout()\n","plt.savefig('ch2_proj1_part2.png')\n","plt.show()"],"metadata":{"id":"FsyqyGpSuGMn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VZXaj61N-Y41"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: OpenAI's GPT2 tokenizer**"],"metadata":{"id":"0Lri_do3EtqL"}},{"cell_type":"code","source":["# GPT2 tokenizer\n","from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained('gpt2')"],"metadata":{"id":"wcRoGJQ5-Ytp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer."],"metadata":{"id":"bDsQUdG-EA22"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gpt2_tokens = tokenizer.encode(txt)\n","\n","for token in gpt2_tokens:\n","  print(f'Token {token:4} is \"{}\"')"],"metadata":{"id":"n1HxyNZntsbY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# leading spaces\n","word1 = ' Mike'\n","word2 = 'Mike'\n","\n","for w in [word1,word2]:\n","  print(f'Token for \"{w}\" is {}')"],"metadata":{"id":"vF4tjHdRJaWJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'There are {} tokens, {} of which are unique.')"],"metadata":{"id":"tDrLo_Vzt1lE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_,axs = plt.subplots(1,2,figsize=(10,3))\n","\n","# bars of total counts\n","axs[0].bar(0,len(characters))\n","axs[0].bar(1,len(words))\n","axs[0].bar(2,len(gpt2_tokens))\n","axs[0].set(xticks=[0,1,2],xticklabels=['Characters','Words','GPT-2'],\n","           ylabel='Count',title='Total')\n","\n","# bars of unique counts\n","\n","axs[1].set(xticks=[0,1,2],xticklabels=['Characters','Words','GPT-2'],\n","           ylabel='Count',title='Unique')\n","\n","plt.tight_layout()\n","plt.savefig('ch2_proj1_part3.png')\n","plt.show()"],"metadata":{"id":"ts7aGasiubQn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fmMz1q8qubD-"},"execution_count":null,"outputs":[]}]}