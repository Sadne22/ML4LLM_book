{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["|<h2>Book:</h2>|<h1><a href=\"https://open.substack.com/pub/mikexcohen/p/llm-breakdown-16-tokenization-words\" target=\"_blank\">50 ML projects to understand LLMs</a></h1>|\n","|-|:-:|\n","|<h2>Project:</h2>|<h1><b>[2] Book lengths in characters, words, and tokens</b></h1>|\n","|<h2>Author:<h2>|<h1>Mike X Cohen, <a href=\"https://sincxpress.com\" target=\"_blank\">sincxpress.com</a></h1>|\n","\n","<br>\n","\n","<i>Using the code without reading the book may lead to confusion or errors.</i>"],"metadata":{"id":"py_eibYAH3Q-"}},{"cell_type":"code","source":[],"metadata":{"id":"R5SI-Iyy4dtm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import requests\n","import matplotlib.pyplot as plt"],"metadata":{"id":"rn8Fcyf87EXx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### matplotlib adjustments (commented lines are for dark mode)\n","\n","# svg plots (higher-res)\n","import matplotlib_inline.backend_inline\n","matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n","\n","plt.rcParams.update({\n","    # 'figure.facecolor': '#282a2c',\n","    # 'figure.edgecolor': '#282a2c',\n","    # 'axes.facecolor':   '#282a2c',\n","    # 'axes.edgecolor':   '#DDE2F4',\n","    # 'axes.labelcolor':  '#DDE2F4',\n","    # 'xtick.color':      '#DDE2F4',\n","    # 'ytick.color':      '#DDE2F4',\n","    # 'text.color':       '#DDE2F4',\n","    'axes.spines.right': False,\n","    'axes.spines.top':   False,\n","    'axes.titleweight': 'bold',\n","    'axes.labelweight': 'bold',\n","})"],"metadata":{"id":"dy4A-ah8kzZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fmMz1q8qubD-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Import the two tokenizers"],"metadata":{"id":"JzjJCjfHQ6WS"}},{"cell_type":"code","source":["# GPT2's tokenizer\n","from transformers import AutoTokenizer\n","tokenizer_G = AutoTokenizer.from_pretrained('gpt2')"],"metadata":{"id":"xEFoa9xSGZpe"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QnBjS3LAujoq"},"outputs":[],"source":["# load BERT tokenizer and model\n","from transformers import BertTokenizer\n","tokenizer_B = BertTokenizer.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","source":["# note about additional tokens:\n","text = 'Peanut and strawberry'\n","tokens = tokenizer_B.encode(text)\n","\n","\n","print(f'Original text:\\n {text}\\n')\n","print(f'Token sequence:\\n {tokens}\\n')\n","\n","for t in tokens:\n","  print(f'Token index {t:6} is \"{tokenizer_B.decode(t)}\"')"],"metadata":{"id":"a5RxJhPy8FlT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ignoring by slicing\n","print(tokenizer_B.decode(tokens[1:-1]))\n","\n","# ignoring by input argument\n","tokens = tokenizer_B.encode(text,add_special_tokens=False)\n","print(tokenizer_B.decode(tokens))"],"metadata":{"id":"Ihx6iYPe8PJw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HNM4VcvAHD9l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 1: Number of tokens**"],"metadata":{"id":"5poSQBOjHD6r"}},{"cell_type":"code","source":["dir(tokenizer_B)"],"metadata":{"id":"OrAq0iZIIAJF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'BERT tokenizer has {tokenizer_B.vocab_size:,} tokens.')\n","print(f'GPT2 tokenizer has {tokenizer_G.vocab_size:,} tokens.')"],"metadata":{"id":"kixCk7xWGZyR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"V5OcJNbIMOXr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 2: Token byte length distributions**"],"metadata":{"id":"jHta_Z4OMOO6"}},{"cell_type":"code","source":["# initialize\n","lengths_B = np.zeros(tokenizer_B.vocab_size,dtype=int)\n","\n","# get all the lengths\n","for t in range(tokenizer_B.vocab_size):\n","  lengths_B[t] = len(tokenizer_B.decode(t).encode('utf-8'))"],"metadata":{"id":"Uwg7r8HVIOuu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize\n","lengths_G = np.zeros(tokenizer_G.vocab_size,dtype=int)\n","\n","# get all the lengths\n","for t in range(tokenizer_G.vocab_size):\n","  lengths_G[t] = len(tokenizer_G.decode(t).encode('utf-8'))"],"metadata":{"id":"vdVnesnNIJzv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# bin counts for lengths\n","bincounts_B = np.bincount(lengths_B)\n","bincounts_G = np.bincount(lengths_G)\n","\n","# note: first element in np.bincount output is for count=0, which we can ignore\n","\n","# plot\n","plt.figure(figsize=(10,4))\n","plt.plot(range(1,max(lengths_B)+1),bincounts_B[1:]/bincounts_B.max(),'s-',color=[.3,.9,.3],markerfacecolor=[.7,.9,.7],label='BERT')\n","plt.plot(range(1,max(lengths_G)+1),bincounts_G[1:]/bincounts_G.max(),'o-',color=[.3,.3,.9],markerfacecolor=[.7,.7,.9],label='GPT2')\n","\n","plt.legend(fontsize=14)\n","plt.gca().set(xlabel='Token length (byte)',ylabel='Density',\n","              title='Distributions of token lengths',yscale='log',xscale='log')\n","\n","plt.show()"],"metadata":{"id":"1BhLX--W6MgI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ztwT7rLDKTaO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 3: Text token length**"],"metadata":{"id":"59R9rK83Ms6E"}},{"cell_type":"code","source":["# all books have the same url format;\n","# they are unique by numerical code\n","baseurl = 'https://www.gutenberg.org/cache/epub/'\n","\n","bookurls = [\n","    # code       title\n","    ['84',    'Frankenstein'    ],\n","    ['64317', 'GreatGatsby'     ],\n","    ['11',    'AliceWonderland' ],\n","    ['1513',  'RomeoJuliet'     ],\n","    ['76',    'HuckFinn'        ],\n","    ['219',   'HeartDarkness'   ],\n","    ['2591',  'GrimmsTales'     ],\n","    ['2148',  'EdgarAllenPoe'   ],\n","    ['36',    'WarOfTheWorlds'  ],\n","    ['829',   'GulliversTravels']\n","]"],"metadata":{"id":"Ote3clXlMzCW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize\n","tokens_B = np.zeros(len(bookurls))\n","tokens_G = np.zeros(len(bookurls))\n","token_lens_B = np.zeros(len(bookurls))\n","token_lens_G = np.zeros(len(bookurls))\n","\n","\n","# loop over books\n","for i,(code,title) in enumerate(bookurls):\n","\n","  # get the text\n","  fullurl = baseurl + code + '/pg' + code + '.txt'\n","  text = requests.get(fullurl).text\n","\n","  # tokenize the text\n","  brt_toks = tokenizer_B.encode(text,add_special_tokens=False)\n","  gpt_toks = tokenizer_G.encode(text)\n","\n","  # count the numbers of tokens\n","  tokens_B[i] = len( brt_toks )\n","  tokens_G[i] = len( gpt_toks )\n","\n","  # count the average lengths of the tokens\n","  token_lens_B[i] = np.mean([len(tokenizer_B.decode(t).encode('utf-8')) for t in brt_toks])\n","  token_lens_G[i] = np.mean([len(tokenizer_G.decode(t).encode('utf-8')) for t in gpt_toks])\n"],"metadata":{"id":"eZJ3IgclM6Hd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# setup the figure\n","_,axs = plt.subplots(1,2,figsize=(10,4))\n","\n","### left panel: token counts\n","\n","# the scatter plot\n","axs[0].plot(tokens_B,tokens_G,'kh',markerfacecolor=[.9,.7,.7,.7],markersize=12)\n","\n","# line of unity\n","minlength = np.min([np.min(tokens_B),np.min(tokens_G)])\n","maxlength = np.max([np.max(tokens_B),np.max(tokens_G)])\n","axs[0].plot([minlength,maxlength],[minlength,maxlength],'--',color=[.5,.5,.5])\n","\n","# stylize\n","axs[0].set(xlabel='BERT tokens',ylabel='GPT2 tokens',title='Book lengths in tokens')\n","axs[0].ticklabel_format(style='scientific',axis='both',scilimits=(0,0))\n","\n","\n","### right panel: token lengths\n","axs[1].plot(token_lens_B,token_lens_G,'ks',markerfacecolor=[.7,.9,.7,.7],markersize=12)\n","\n","minlength = np.min([np.min(token_lens_B),np.min(token_lens_G)])\n","maxlength = np.max([np.max(token_lens_B),np.max(token_lens_G)])\n","axs[1].plot([minlength,maxlength],[minlength,maxlength],\n","            '--',zorder=-10,color=[.5,.5,.5])\n","\n","# stylize\n","axs[1].set(xlabel='BERT tokens',ylabel='GPT2 tokens',title='Average token lengths (bytes)')\n","\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"8Ojrfn5nNE7D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"85u_BsGRJh3g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Part 4: Translator functions**"],"metadata":{"id":"i5e1Du85GZg0"}},{"cell_type":"code","source":["# translation functions\n","def GPT_to_BERT(toks):\n","  newtxt  = tokenizer_G.decode(toks)\n","  newtoks = tokenizer_B.encode(newtxt,add_special_tokens=False)\n","  return newtoks\n","\n","def BERT_to_GPT(t):\n","  return tokenizer_G.encode( tokenizer_B.decode(t),add_special_tokens=False )"],"metadata":{"id":"bD7vOFUBGZdd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = 'Canadian winters are kept WArM bY the friendliness of THE PEOPLE.'\n","print('Original text:\\n ',text,'\\n')\n","\n","# tokenize\n","toks_B = tokenizer_B.encode(text,add_special_tokens=False)\n","toks_G = tokenizer_G.encode(text)\n","\n","# print the original tokens\n","print('BERT tokens:\\n ',toks_B)\n","print('GPT2 tokens:\\n ',toks_G,'\\n')\n","\n","# is the decoding perfect?\n","print('BERT reconstruction:\\n ',tokenizer_B.decode(toks_B))\n","print('GPT2 reconstruction:\\n ',tokenizer_G.decode(toks_G),'\\n')\n","\n","# and finally, translate between tokenizers\n","print('BERT to GPT2 translation:\\n ',tokenizer_G.decode(BERT_to_GPT(toks_B)))\n","print('GPT2 to BERT translation:\\n ',tokenizer_B.decode(GPT_to_BERT(toks_G)))"],"metadata":{"id":"UZz_JTyzPEQt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ffy5AB8uGZWh"},"execution_count":null,"outputs":[]}]}